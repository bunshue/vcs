"""
æ©Ÿå™¨å­¸ç¿’å…¥é–€


"""

import sys
import numpy as np
import matplotlib.pyplot as plt

print('------------------------------------------------------------')	#60å€‹

'''

"""
01.è®€å…¥åŸºæœ¬å¥—ä»¶

æ©Ÿå™¨å­¸ç¿’å…¶å¯¦åŸºæœ¬ä¸Šå’Œæˆ‘å€‘ä¸€ç›´ä»¥ä¾†èªªçš„ä¸€æ¨£, å°±æ˜¯æˆ‘å€‘è¦å­¸ä¸€å€‹æœªçŸ¥çš„å‡½æ•¸
f(x)=y

å¦‚æœæ˜¯åˆ†é¡, åŸºæœ¬ä¸Šå°±æ˜¯æœ‰ä¸€ç­†è³‡æ–™ x=(x1,x2,â€¦,xk), æˆ‘å€‘æƒ³çŸ¥é“é€™
f(x)=y,å…¶ä¸­çš„ y å°±æ˜¯æŸä¸€å€‹é¡åˆ¥ã€‚

é€™ç¨®å­¸å‡½æ•¸çš„æ–¹æ³•, åˆå¯ä»¥åˆ†ç‚º:

    supervised learning
    unsupervised learning

å…¶ä¸­çš„ supervised learning å°±æ˜¯æˆ‘å€‘æœ‰ä¸€çµ„çŸ¥é“ç­”æ¡ˆçš„è¨“ç·´è³‡æ–™, ç„¶å¾Œæ‰¾åˆ°æˆ‘å€‘è¦çš„å‡½æ•¸ã€‚è€Œ unsupervised learning å°±ç¥äº†, æˆ‘å€‘ä¸çŸ¥é“ç­”æ¡ˆ, å»è¦é›»è…¦è‡ªå·±å»å­¸!

åšæ•¸æ“šåˆ†æ, å¹¾ä¹æ¯ä¸€æ¬¡éƒ½è¦è®€å…¥é€™äº›å¥—ä»¶!
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# ç”¨ Seaborn ç•«åœ–, ä¸¦è¨­å¥½åœ–çš„å¤§å°

import seaborn as sns
sns.set(rc={'figure.figsize':(11.7,8.27)})


#02. é—œæ–¼ overfitting

#æˆ‘å€‘åœ¨æ•¸æ“šåˆ†æ, å°±æ˜¯æ”¶é›†äº†æ­·å²è³‡æ–™, æ¯”å¦‚èªªé€™äº›æ•¸æ“šã€‚

X = np.random.randn(6)
Y = np.random.randn(6)
plt.scatter(X, Y, c='r', s=100)
plt.grid()
plt.title('aaaa')
plt.show()

x = np.linspace(0, 1, 200)
y = -(x-1)**2+1
plt.plot(x,y)
plt.grid()
plt.title('aaaa')
plt.show()


#çœŸå¯¦ä¸–ç•Œçœ‹ä¾†æ¯”è¼ƒåƒæ˜¯é€™æ¨£...

X = np.linspace(0, 1, 20)
Y = -(X-1)**2+1 + 0.08*np.random.randn(20)
plt.scatter(X,Y, c='r',s=50)
plt.plot(x,y)
plt.grid()
plt.title('aaaa')
plt.show()

z = np.polyfit(X, Y, 19)
p = np.poly1d(z)
plt.plot(x, p(x),'r')
plt.scatter(X,Y, c='r',s=50)
plt.plot(x,y)
plt.ylim(0, 2)
plt.grid()
plt.title('é€™å«å¾ˆä½çš„ bias, å¾ˆé«˜çš„ variance')
plt.show()

"""
03. è¿´æ­¸æ³•é æ¸¬å‡½æ•¸
03-1. å‡çš„æ•¸æ“šçœŸçš„è¿´æ­¸
åšä¸€æ¢ç›´ç·š

æˆ‘å€‘ä¾†ä¸€æ¢ç·š, æ¯”å¦‚èªª f(x)=1.2x+0.8

æº–å‚™å¥½å€‹ 1000 å€‹é» (ç¾å ´å»ºè­°, é›–ç„¶å¤šäº†ä¸€é»...)

åŠ å…¥ noise é …, çœ‹ä¾†æ›´çœŸå¯¦ å¤§æ¦‚çš„æƒ³æ³•å°±æ˜¯, æˆ‘å€‘çœŸå¯¦ä¸–ç•Œçš„å•é¡Œ, åŒ–æˆå‡½æ•¸, æˆ‘å€‘å‡è¨­èƒŒå¾Œæœ‰å€‹ç¾å¥½çš„å‡½æ•¸ã€‚ä½†ç›¸ä¿¡æˆ‘å€‘å¾ˆå°‘çœ‹åˆ°çœŸå¯¦ä¸–ç•Œçš„è³‡æ–™é‚£éº¼æ¼‚äº®ã€‚åœ¨çµ±è¨ˆä¸Š, æˆ‘å€‘å°±æ˜¯å‡è¨­

ğ‘“(ğ‘¥)+ğœ€(ğ‘¥)

ä¹Ÿå°±æ˜¯éƒ½æœ‰å€‹ noise é …ã€‚
"""

x = np.linspace(0, 1, 300)

y = 1.2*x + 0.8 + 0.2*np.random.randn(300)

#ç•«å‡ºåœ–å½¢ä¾†ã€‚

plt.scatter(x,y)

plt.grid()
plt.title('aaaa')
plt.show()

"""
åˆ†è¨“ç·´è³‡æ–™ã€æ¸¬è©¦è³‡æ–™

ä¸€èˆ¬æˆ‘å€‘æƒ³è¦çœ‹ç®—å‡ºä¾†çš„é€¼è¿‘å‡½æ•¸åœ¨é æ¸¬ä¸Šæ˜¯ä¸æ˜¯å¯é , æœƒæŠŠä¸€äº›è³‡æ–™ç•™çµ¦ã€Œæ¸¬è©¦ã€, å°±æ˜¯ä¸è®“é›»è…¦åœ¨è¨ˆç®—æ™‚ã€Œçœ‹åˆ°ã€é€™äº›æ¸¬è©¦è³‡æ–™ã€‚ç­‰å‡½æ•¸å­¸æˆäº†ä»¥å¾Œ, å†ä¾†æ¸¬è©¦æº–ä¸æº–ç¢ºã€‚é€™æ˜¯æˆ‘å€‘å¯ä»¥ç”¨

sklearn.model_selection è£¡çš„ train_test_split

ä¾†äº‚æ•¸é¸ä¸€å®šç™¾åˆ†æ¯”çš„è³‡æ–™ä¾†ç”¨ã€‚
"""
from sklearn.model_selection import train_test_split

#æŠŠåŸä¾†çš„ x, y ä¸­çš„ 80% çµ¦ training data, 20% çµ¦ testing dataã€‚

x_train, x_test, y_train, y_test = train_test_split(x, y,
                                                   test_size=0.2,
                                                   random_state=9487)

#len(x_train)    #80%
#len(x_test)     #20%

"""
ã€é‡é»ã€‘æ³¨æ„è¼¸å…¥æ ¼å¼

åªæœ‰ä¸€å€‹ feature æ™‚, æˆ‘å€‘è¦å°å¿ƒçš„æ˜¯, å¾ˆå¤šæ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’çš„å¥—ä»¶, éƒ½ä¸å¸Œæœ›æˆ‘å€‘ç”¨

x=[x1,x2,â€¦,xn]

é€™æ¨£å­å»åš, è€Œæ˜¯å¸Œæœ›è®Šæˆ

x=[[x1],[x2],â€¦,[xn]]

é€™ç¨®å½¢å¼!
"""

xx = np.array([3, 9, 8, 1, 2])
yy = np.array([1, 3, 9, 2, 4])

"""
xx.shape
xx.reshape(5,1)
xx = xx.reshape(len(xx),1)
"""

#æ­£å¼è½‰æˆ‘å€‘çš„è¨“ç·´è³‡æ–™

x_train = x_train.reshape(len(x_train),1)
x_test = x_test.reshape(len(x_test), 1)

#step 1. é–‹ä¸€å°ã€Œç·šæ€§è¿´æ­¸æ©Ÿã€

from sklearn.linear_model import LinearRegression

regr = LinearRegression()

#step 2. fit å­¸ç¿’ã€è¨“ç·´

regr.fit(x_train, y_train)

#step 3. predict é æ¸¬

Ypred = regr.predict(x_test)

# x: x_test
# y: Ypred
# x_test.ravel()

plt.plot(x_test.ravel(), Ypred, 'r')

plt.scatter(x_test.ravel(), y_test)

plt.grid()
plt.title('aaaa')
plt.show()


#è¨ˆç®—åˆ†æ•¸
from sklearn.metrics import mean_squared_error, r2_score
mse_t = mean_squared_error(y_train, regr.predict(x_train))
r2_t = r2_score(y_train, regr.predict(x_train))
print('è¨“ç·´è³‡æ–™')
print('MSE =', mse_t)
print("R2 =", r2_t)

mse = mean_squared_error(y_test, Ypred)
r2 = r2_score(y_test, Ypred)
print("æ¸¬è©¦è³‡æ–™")
print(f"MSE = {mse:.4f}")
print(f"R2 = {r2:.4f}")


""" ç„¡è³‡æ–™
03-2 æˆ¿åƒ¹é æ¸¬
è®€å…¥è³‡æ–™

SciKit-Learn æœ‰è¨±å¤š "Toy Datasets" å¯ä»¥è®“æˆ‘å€‘ç©ç©ã€‚

ä»Šå¤©æˆ‘å€‘è¦ä½¿ç”¨çš„æ˜¯ã€Œæ³¢å£«é “æˆ¿åƒ¹è³‡æ–™ã€ã€‚

from sklearn.datasets import load_boston

boston_dataset = load_boston()

print(boston_dataset.DESCR)

.. _boston_dataset:



Boston house prices dataset

---------------------------



**Data Set Characteristics:**  



    :Number of Instances: 506 



    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.



    :Attribute Information (in order):

        - CRIM     per capita crime rate by town

        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.

        - INDUS    proportion of non-retail business acres per town

        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)

        - NOX      nitric oxides concentration (parts per 10 million)

        - RM       average number of rooms per dwelling

        - AGE      proportion of owner-occupied units built prior to 1940

        - DIS      weighted distances to five Boston employment centres

        - RAD      index of accessibility to radial highways

        - TAX      full-value property-tax rate per $10,000

        - PTRATIO  pupil-teacher ratio by town

        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town

        - LSTAT    % lower status of the population

        - MEDV     Median value of owner-occupied homes in $1000's



    :Missing Attribute Values: None



    :Creator: Harrison, D. and Rubinfeld, D.L.



This is a copy of UCI ML housing dataset.

https://archive.ics.uci.edu/ml/machine-learning-databases/housing/





This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.



The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic

prices and the demand for clean air', J. Environ. Economics & Management,

vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics

...', Wiley, 1980.   N.B. Various transformations are used in the table on

pages 244-261 of the latter.



The Boston house-price data has been used in many machine learning papers that address regression

problems.   

     

.. topic:: References



   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.

   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.


boston_dataset.feature_names

array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
,       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')

boston_dataset.data[:5]

array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,
,        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,
,        1.5300e+01, 3.9690e+02, 4.9800e+00],
,       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,
,        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,
,        1.7800e+01, 3.9690e+02, 9.1400e+00],
,       [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,
,        7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,
,        1.7800e+01, 3.9283e+02, 4.0300e+00],
,       [3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,
,        6.9980e+00, 4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,
,        1.8700e+01, 3.9463e+02, 2.9400e+00],
,       [6.9050e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,
,        7.1470e+00, 5.4200e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,
,        1.8700e+01, 3.9690e+02, 5.3300e+00]])

boston = pd.DataFrame(boston_dataset.data,

                     columns=boston_dataset.feature_names)

boston.head()

, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,
	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT
0	0.00632	18.0	2.31	0.0	0.538	6.575	65.2	4.0900	1.0	296.0	15.3	396.90	4.98
1	0.02731	0.0	7.07	0.0	0.469	6.421	78.9	4.9671	2.0	242.0	17.8	396.90	9.14
2	0.02729	0.0	7.07	0.0	0.469	7.185	61.1	4.9671	2.0	242.0	17.8	392.83	4.03
3	0.03237	0.0	2.18	0.0	0.458	6.998	45.8	6.0622	3.0	222.0	18.7	394.63	2.94
4	0.06905	0.0	2.18	0.0	0.458	7.147	54.2	6.0622	3.0	222.0	18.7	396.90	5.33
,

boston['MEDV'] = boston_dataset.target

boston.head()

, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,
	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT	MEDV
0	0.00632	18.0	2.31	0.0	0.538	6.575	65.2	4.0900	1.0	296.0	15.3	396.90	4.98	24.0
1	0.02731	0.0	7.07	0.0	0.469	6.421	78.9	4.9671	2.0	242.0	17.8	396.90	9.14	21.6
2	0.02729	0.0	7.07	0.0	0.469	7.185	61.1	4.9671	2.0	242.0	17.8	392.83	4.03	34.7
3	0.03237	0.0	2.18	0.0	0.458	6.998	45.8	6.0622	3.0	222.0	18.7	394.63	2.94	33.4
4	0.06905	0.0	2.18	0.0	0.458	7.147	54.2	6.0622	3.0	222.0	18.7	396.90	5.33	36.2
,

sns.distplot(boston.MEDV, bins=30)

<matplotlib.axes._subplots.AxesSubplot at 0x7ffae2eabac8>

correlation_matrix = boston.corr().round(2)

sns.heatmap(correlation_matrix, annot=True)

<matplotlib.axes._subplots.AxesSubplot at 0x7ffae2f0a940>

boston.head()

, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,
	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT	MEDV
0	0.00632	18.0	2.31	0.0	0.538	6.575	65.2	4.0900	1.0	296.0	15.3	396.90	4.98	24.0
1	0.02731	0.0	7.07	0.0	0.469	6.421	78.9	4.9671	2.0	242.0	17.8	396.90	9.14	21.6
2	0.02729	0.0	7.07	0.0	0.469	7.185	61.1	4.9671	2.0	242.0	17.8	392.83	4.03	34.7
3	0.03237	0.0	2.18	0.0	0.458	6.998	45.8	6.0622	3.0	222.0	18.7	394.63	2.94	33.4
4	0.06905	0.0	2.18	0.0	0.458	7.147	54.2	6.0622	3.0	222.0	18.7	396.90	5.33	36.2
,

X = boston.loc[:,"CRIM":"LSTAT"].values

Y = boston.MEDV

x_train, x_test, y_train, y_test = train_test_split(X, Y,

                                                   test_size=0.2,

                                                   random_state=9487)

regr = LinearRegression()

regr.fit(x_train, y_train)

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
,         normalize=False)

Ypred = regr.predict(x_test)

mse = mean_squared_error(y_test, Ypred)

r2 = r2_score(y_test, Ypred)

â€‹

print("MSE =", mse)

print("R2 =", r2)

MSE = 23.62871172175499

R2 = 0.7432212177482068

plt.scatter(y_test, Ypred, s=100)

plt.xlim(0, 55)

plt.ylim(0, 55)

plt.plot([0,55],[0,55],'r')

plt.grid()
plt.title('aaaa')
plt.show()

X = boston[['NOX', 'AGE', 'DIS']].values

boston.columns

Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
,       'PTRATIO', 'B', 'LSTAT', 'MEDV'],
,      dtype='object')

X = boston[['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',

       'PTRATIO', 'B', 'LSTAT']].values

X

array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,
,        4.9800e+00],
,       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,
,        9.1400e+00],
,       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,
,        4.0300e+00],
,       ...,
,       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,
,        5.6400e+00],
,       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,
,        6.4800e+00],
,       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,
,        7.8800e+00]])

x_train, x_test, y_train, y_test = train_test_split(X, Y,

                                                    test_size=0.2,

                                                    random_state=9487)

regr = LinearRegression()

regr.fit(x_train, y_train)

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
,         normalize=False)

Ypred = regr.predict(x_test)

mse = mean_squared_error(y_test, Ypred)

r2 = r2_score(y_test, Ypred)

â€‹

print('MSE =', mse)

print('r2 =', r2)

MSE = 23.620687554217824

r2 = 0.743308418269041
"""

'''

#07. æ€éº¼é¸æœ€å¥½åƒæ•¸ã€modelï¼Ÿ
#7-1. è£½é€ åƒçœŸçš„ä¸€æ§˜çš„æ•¸æ“š

#from sklearn.datasets.samples_generator import make_blobs
from sklearn.datasets import make_blobs

x, y = make_blobs(n_samples = 500,
                  centers = 3,
                  n_features = 2,
                  random_state = 0)

plt.scatter(x[:, 0], x[:, 1], c = y, cmap = 'Paired', s = 80)

plt.grid()
plt.title('aaaa')
plt.show()

#07-2. Cross Validation

from sklearn.model_selection import cross_val_score

#è©¦ç”¨ SVC

from sklearn.svm import SVC

clf_svc = SVC(gamma = 'scale')

scores = cross_val_score(clf_svc, x, y, cv = 5)

"""
çœ‹ä¸€ä¸‹äº”æ¬¡çš„æˆç¸¾ã€‚
scores
array([0.94117647, 0.95049505, 0.97979798, 0.87878788, 0.91919192])
å¾ˆå¿«çš„ç®—ä¸€ä¸‹å¹³å‡ã€‚
scores.mean()
0.9338898595741927
"""

#è©¦ç”¨ Decision Tree

from sklearn.tree import DecisionTreeClassifier

clf_dt = DecisionTreeClassifier()
scores = cross_val_score(clf_dt, x, y, cv=5)

"""
scores
array([0.92156863, 0.89108911, 0.94949495, 0.90909091, 0.88888889])
scores.mean()
0.9120264967673238
"""

#è©¦ç”¨ Random Forest

from sklearn.ensemble import RandomForestClassifier

clf_rf = RandomForestClassifier(n_estimators=100)
scores = cross_val_score(clf_rf, x, y, cv=5)

"""
scores

array([0.92156863, 0.92079208, 0.96969697, 0.88888889, 0.88888889])

scores.mean()

0.9179670908267298
"""


print('------------------------------------------------------------')	#60å€‹


print('------------------------------------------------------------')	#60å€‹




print('------------------------------------------------------------')	#60å€‹


print('------------------------------------------------------------')	#60å€‹




print('------------------------------------------------------------')	#60å€‹


print('------------------------------------------------------------')	#60å€‹




print('------------------------------------------------------------')	#60å€‹


print('------------------------------------------------------------')	#60å€‹

print('------------------------------------------------------------')	#60å€‹


print('ä½œæ¥­å®Œæˆ')

