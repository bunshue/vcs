{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate statistics\n",
    "\n",
    "Basics univariate statistics are required to explore dataset:\n",
    "\n",
    "- Discover associations between a variable of interest and potential predictors. It is strongly recommended to start with simple univariate methods before moving to complex multivariate predictors. \n",
    "\n",
    "- Assess the prediction performances of machine learning predictors.\n",
    "\n",
    "- Most of the univariate statistics are based on the linear model which is one of the main model in machine learning.\n",
    "\n",
    "\n",
    "## Estimators of the main statistical measures\n",
    "\n",
    "\n",
    "### Mean\n",
    "\n",
    "Properties of the expected value operator $\\operatorname{E}(\\cdot)$ of a random variable $X$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    E(X + c) &= E(X) + c \\\\ \n",
    "    E(X + Y) &= E(X) + E(Y) \\\\ \n",
    "    E(aX)    &= a E(X)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The estimator $\\bar{x}$ on a sample of size $n$: $x = x_1, ..., x_n$ is given by\n",
    "$$\n",
    "    \\bar{x} = \\frac{1}{n} \\sum_i x_i\n",
    "$$\n",
    "\n",
    "$\\bar{x}$ is itself a random variable with properties:\n",
    "\n",
    "- $E(\\bar{x}) = \\bar{x}$,\n",
    "\n",
    "- $Var(\\bar{x}) = \\frac{Var(X)}{n}$.\n",
    "\n",
    "### Variance\n",
    "\n",
    "$$\n",
    "    Var(X) = E((X - E(X))^2) =  E(X^2) - (E(X))^2\n",
    "$$\n",
    "\n",
    "The estimator is\n",
    "$$\n",
    "    \\sigma_x^2 = \\frac{1}{n-1} \\sum_i (x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "Note here the subtracted 1 degree of freedom (df) in the divisor. In standard statistical practice, $df=1$ provides an unbiased estimator of the variance of a hypothetical infinite population. With $df=0$ it instead provides a maximum likelihood estimate of the variance for normally distributed variables.\n",
    "\n",
    "### Standard deviation\n",
    "\n",
    "$$\n",
    "    Std(X) = \\sqrt{Var(X)}\n",
    "$$\n",
    "\n",
    "The estimator is simply $\\sigma_x = \\sqrt{\\sigma_x^2}$.\n",
    "\n",
    "### Covariance\n",
    "\n",
    "$$\n",
    "    Cov(X, Y) = E((X - E(X))(Y - E(Y))) =  E(XY) - E(X)E(Y).\n",
    "$$\n",
    "\n",
    "Properties: \n",
    "$$\n",
    "    \\operatorname{Cov}(X, X) = \\operatorname{Var}(X)\\\\\n",
    "    \\operatorname{Cov}(X, Y) = \\operatorname{Cov}(Y, X)\\\\\n",
    "    \\operatorname{Cov}(cX, Y) = c \\operatorname{Cov}(X, Y)\\\\\n",
    "    \\operatorname{Cov}(X+c, Y) = \\operatorname{Cov}(X, Y)\\\\\n",
    "$$\n",
    "\n",
    "The estimator with $df=1$ is\n",
    "$$\n",
    "    \\sigma_{xy} = \\frac{1}{n-1} \\sum_i (x_i - \\bar{x}) (y_i - \\bar{y}).\n",
    "$$\n",
    "\n",
    "### Correlation\n",
    "\n",
    "$$\n",
    "    Cor(X, Y) = \\frac{Cov(X, Y)}{Std(X)Std(Y)}\n",
    "$$\n",
    "\n",
    "The estimator is\n",
    "$$\n",
    "    \\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_{x} \\sigma_{y}}.\n",
    "$$\n",
    "\n",
    "### Standard Error (SE) \n",
    "\n",
    "The standard error (SE) is the standard deviation (of the sampling distribution) of a statistic: \n",
    "$$\n",
    "    SE(X) = \\frac{Std(X)}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "It is most commonly considered for the mean with the estimator\n",
    "\n",
    "\\begin{align}\n",
    "SE(x) &= Std(X) = \\sigma_{\\bar{x}}\\\\\n",
    "      &= \\frac{\\sigma_x}{\\sqrt{n}}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### Descriptives statistics with numpy\n",
    "\n",
    "- Generate 2 random samples: $x \\sim N(1.78, 0.1)$ and $y \\sim N(1.66, 0.1)$, both of size 10.\n",
    "\n",
    "- Compute $\\bar{x}, \\sigma_x, \\sigma_{xy}$ (`xbar, xvar, xycov`) using only the `np.sum()` operation. \n",
    "Explore the `np.` module to find out which numpy functions performs the same computations and compare them (using `assert`) with your previous results.\n",
    "\n",
    "Warning! by default `np.var()` used the biased estimator ddof=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "x = np.random.normal(loc=1.78, scale=.1, size=n)\n",
    "y = np.random.normal(loc=1.66, scale=.1, size=n)\n",
    "\n",
    "xbar = np.mean(x)\n",
    "assert xbar == np.sum(x) / x.shape[0]\n",
    "\n",
    "xvar = np.var(x, ddof=1)\n",
    "assert xvar == np.sum((x - xbar) ** 2) / (n - 1)\n",
    "\n",
    "xycov = np.cov(x, y)\n",
    "print(xycov)\n",
    "\n",
    "ybar = np.sum(y) / n\n",
    "assert np.allclose(xycov[0, 1], np.sum((x - xbar) * (y - ybar)) / (n - 1))\n",
    "assert np.allclose(xycov[0, 0], xvar)\n",
    "assert np.allclose(xycov[1, 1], np.var(y, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main distributions\n",
    "\n",
    "### Normal distribution\n",
    "\n",
    "The normal distribution, noted $\\mathcal{N}(\\mu, \\sigma)$ with parameters: $\\mu$ mean (location) and $\\sigma>0$ std-dev. Estimators: $\\bar{x}$ and $\\sigma_{x}$.\n",
    "\n",
    "The normal distribution, noted $\\mathcal{N}$, is useful because of the central limit theorem (CLT) which states that: given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed, regardless of the underlying distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline\n",
    "\n",
    "mu = 0 # mean\n",
    "variance = 2 #variance\n",
    "sigma = np.sqrt(variance) #standard deviation\",\n",
    "x = np.linspace(mu-3*variance,mu+3*variance, 100)\n",
    "plt.plot(x, norm.pdf(x, mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chi-Square distribution\n",
    "\n",
    "The chi-square or $\\chi_n^2$ distribution with $n$ degrees of freedom (df) is the distribution of a sum of the squares of $n$ independent standard normal random variables $\\mathcal{N}(0, 1)$. Let $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then, $Z=(X - \\mu)/\\sigma \\sim \\mathcal{N}(0, 1)$, then:\n",
    "\n",
    "- The squared standard $Z^2 \\sim \\chi_1^2$ (one df).\n",
    "\n",
    "- **The distribution of sum of squares** of $n$ normal random variables: $\\sum_i^n Z_i^2 \\sim \\chi_n^2$\n",
    "\n",
    "The sum of two $\\chi^2$ RV with $p$ and $q$ df is a $\\chi^2$ RV with $p+q$ df. This is useful when summing/subtracting sum of squares.\n",
    "\n",
    "The $\\chi^2$-distribution is used to model **errors** measured as **sum of squares** or the distribution of the sample **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fisher's F-distribution\n",
    "\n",
    "The $F$-distribution, $F_{n, p}$, with $n$ and $p$ degrees of freedom is the ratio of two independent $\\chi^2$ variables. Let $X \\sim \\chi_n^2$ and $Y \\sim \\chi_p^2$ then: \n",
    "$$\n",
    "    F_{n, p} = \\frac{X/n}{Y/p}\n",
    "$$\n",
    "\n",
    "The $F$-distribution plays a central role in hypothesis testing answering the question: **Are two variances equals?, is the ratio or two errors significantly large ?**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fvalues = np.linspace(.1, 5, 100)\n",
    "\n",
    "# pdf(x, df1, df2): Probability density function at x of F.\n",
    "plt.plot(fvalues, f.pdf(fvalues, 1, 30), 'b-', label=\"F(1, 30)\")\n",
    "plt.plot(fvalues, f.pdf(fvalues, 5, 30), 'r-', label=\"F(5, 30)\")\n",
    "plt.legend()\n",
    "\n",
    "# cdf(x, df1, df2): Cumulative distribution function of F.\n",
    "# ie. \n",
    "proba_at_f_inf_3 = f.cdf(3, 1, 30) # P(F(1,30) < 3)\n",
    "\n",
    "# ppf(q, df1, df2): Percent point function (inverse of cdf) at q of F.\n",
    "f_at_proba_inf_95 = f.ppf(.95, 1, 30) # q such P(F(1,30) < .95)\n",
    "assert f.cdf(f_at_proba_inf_95, 1, 30) == .95\n",
    "\n",
    "# sf(x, df1, df2): Survival function (1 - cdf) at x of F.\n",
    "proba_at_f_sup_3 = f.sf(3, 1, 30) # P(F(1,30) > 3)\n",
    "assert  proba_at_f_inf_3 + proba_at_f_sup_3 == 1\n",
    "\n",
    "# p-value: P(F(1, 30)) < 0.05\n",
    "low_proba_fvalues = fvalues[fvalues > f_at_proba_inf_95]\n",
    "plt.fill_between(low_proba_fvalues, 0, f.pdf(low_proba_fvalues, 1, 30),\n",
    "                 alpha=.8, label=\"P < 0.05\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The  Student's $t$-distribution\n",
    "\n",
    "Let $M \\sim \\mathcal{N}(0, 1)$ and $V \\sim \\chi_n^2$. The $t$-distribution, $T_n$, with $n$ degrees of freedom is the ratio:\n",
    "$$\n",
    "    T_n = \\frac{M}{\\sqrt{V/n}}\n",
    "$$\n",
    "\n",
    "The distribution of the difference between an estimated parameter and its true (or assumed) value divided by the standard deviation of the estimated parameter (standard error) follow a $t$-distribution. **Is this parameters different from a given value?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- Test a proportion: Biased coin ? 200 heads have been found over 300 flips, is it coins biased ?\n",
    "\n",
    "- Test the association between two variables.\n",
    "    * Exemple height and sex: In a sample of 25 individuals (15 females, 10 males), is female height is different from male height ? \n",
    "    * Exemple age and arterial hypertension: In a sample of 25 individuals is age height correlated with arterial hypertension ? \n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Model the data\n",
    "\n",
    "2. Fit: estimate the model parameters (frequency, mean, correlation, regression coeficient)\n",
    "\n",
    "3. Compute a test statistic from model the parameters.\n",
    "\n",
    "4. Formulate the null hypothesis: What would be the (distribution of the) test statistic if the observations are the result of pure chance.\n",
    "\n",
    "5. Compute the probability ($p$-value) to obtain a larger value for the test statistic by chance (under the null hypothesis).\n",
    "\n",
    "\n",
    "### Flip coin: Simplified example\n",
    "\n",
    "Biased coin ? 2 heads have been found over 3 flips, is it coins biased ?\n",
    "\n",
    "1. Model the data: number of heads follow a Binomial disctribution.\n",
    "\n",
    "2. Compute model parameters: N=3, P = the frequency of number of heads over the number of flip: 2/3.\n",
    "\n",
    "3. Compute a test statistic, same as frequency.\n",
    "\n",
    "2. Under the null hypothesis the distribution of the number of tail is:\n",
    "\n",
    "| 1 | 2 | 3 | count #heads |\n",
    "|---|---|---|--------------|\n",
    "|   |   |   | 0            |\n",
    "| H |   |   | 1            |\n",
    "|   | H |   | 1            |\n",
    "|   |   | H | 1            |\n",
    "| H | H |   | 2            |\n",
    "| H |   | H | 2            |\n",
    "|   | H | H | 2            |\n",
    "| H | H | H | 3            |\n",
    "\n",
    "\n",
    "8 possibles configurations, probabilities of differents values for $p$ are:\n",
    "$x$ measure the number of success.\n",
    "\n",
    "- $P(x=0) = 1/8$\n",
    "- $P(x=1) = 3/8$\n",
    "- $P(x=2) = 3/8$\n",
    "- $P(x=3) = 1/8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar([0, 1, 2, 3], [1/8, 3/8, 3/8, 1/8], width=0.9)\n",
    "_ = plt.xticks([0, 1, 2, 3], [0, 1, 2, 3])\n",
    "plt.xlabel(\"Distribution of the number of head over 3 flip under the null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the probability ($p$-value) to observe a value larger or equal that 2 under the null hypothesis ?\n",
    "This probability is the $p$-value:\n",
    "$$\n",
    "P(x\\geq 2| H_0) = P(x=2) + P(x=3) = 3/8 + 1/8 = 4/8 = 1/2\n",
    "$$\n",
    "\n",
    "### Flip coin: Real Example\n",
    "\n",
    "Biased coin ? 60 heads have been found over 100 flips, is it coins biased ?\n",
    "\n",
    "1. Model the data: number of heads follow a Binomial disctribution.\n",
    "\n",
    "2. Compute model parameters: N=100, P=60/100.\n",
    "\n",
    "3. Compute a test statistic, same as frequency.\n",
    "\n",
    "4. Compute a test statistic: 60/100.\n",
    "\n",
    "5. Under the null hypothesis the distribution of the number of tail ($k$) follow the **binomial distribution** of parameters N=100, **P=0.5**:\n",
    "$$\n",
    "Pr(X=k|H_0) = Pr(X=k|n=100, p=0.5) = {100 \\choose k}0.5^k (1-0.5)^{(100-k)}.\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "P(X=k\\geq 60|H_0) &=  \\sum_{k=60}^{100}{100 \\choose k}0.5^k (1-0.5)^{(100-k)}\\\\\n",
    " &= 1 - \\sum_{k=1}^{60}{100 \\choose k}0.5^k (1-0.5)^{(100-k)}, \\text{the cumulative distribution function.}\n",
    "\\end{align*} \n",
    "\n",
    "**Use tabulated binomial distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tobs = 2.39687663116 # assume the t-value\n",
    "succes = np.linspace(30, 70, 41)\n",
    "plt.plot(succes, scipy.stats.binom.pmf(succes, 100, 0.5), 'b-', label=\"Binomial(100, 0.5)\")\n",
    "upper_succes_tvalues = succes[succes > 60]\n",
    "plt.fill_between(upper_succes_tvalues, 0, scipy.stats.binom.pmf(upper_succes_tvalues, 100, 0.5), alpha=.8, label=\"p-value\")\n",
    "_ = plt.legend()\n",
    "\n",
    "\n",
    "pval = 1 - scipy.stats.binom.cdf(60, 100, 0.5)\n",
    "print(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random sampling of the Binomial distribution under the null hypothesis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sccess_h0 = scipy.stats.binom.rvs(100, 0.5, size=10000, random_state=4)\n",
    "print(sccess_h0)\n",
    "\n",
    "pval_rnd = np.sum(sccess_h0 >= 60) / (len(sccess_h0) + 1)\n",
    "print(\"P-value using monte-carlo sampling of the Binomial distribution under H0=\", pval_rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  One sample $t$-test\n",
    "\n",
    "The one-sample $t$-test is used to determine whether a sample comes from a population with a specific mean. For example you want to test if the average height of a population is $1.75~m$.\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "1. Independence of **residuals** ($\\varepsilon_i$). This assumptions **must** be satisfied.\n",
    "2. Normality of residuals. Approximately normally distributed can be accepted.\n",
    "\n",
    "Remarks: Although the parent population does not need to be normally distributed, the distribution of the population of sample means, $\\overline{x}$, is assumed to be normal. By the central limit theorem, if the sampling of the parent population is independent then the sample means will be approximately normal.\n",
    "\n",
    "#### 1 Model the data\n",
    "\n",
    "Assume that height is normally distributed: $X \\sim \\mathcal{N}(\\mu, \\sigma)$, ie:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{height}_i &= \\text{average height over the population} + \\text{error}_i\\\\\n",
    "x_i &= \\bar{x} + \\varepsilon_i\n",
    "\\end{align}\n",
    "\n",
    "The $\\varepsilon_i$ are called the residuals\n",
    "\n",
    "#### 2 Fit: estimate the model parameters\n",
    "\n",
    "  $\\bar{x}, s_x$ are the estimators of $\\mu, \\sigma$.\n",
    "\n",
    "#### 3 Compute a test statistic\n",
    "\n",
    "In testing the null hypothesis that the population mean is equal to a specified value $\\mu_0=1.75$, one uses the statistic:\n",
    "\n",
    "\\begin{align}\n",
    " t &= \\frac{\\text{difference of means}}{\\text{std-dev of noise}} \\sqrt{n}\\\\\n",
    " t &= \\text{effect size} \\sqrt{n}\\\\ \n",
    " t &= \\frac{\\bar{x} - \\mu_0}{s_x} \\sqrt{n} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### 4 Compute the probability of the test statistic under the null hypotheis. This require to have the distribution of the t statistic under $H_0$.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Given the following samples, we will test whether its true mean is 1.75.\n",
    "\n",
    "Warning, when computing the std or the variance, set `ddof=1`. The default\n",
    "value, `ddof=0`, leads to the biased estimator of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1.83,  1.83,  1.73,  1.82,  1.83,  1.73,  1.99,  1.85,  1.68,  1.87]\n",
    "\n",
    "xbar = np.mean(x) # sample mean\n",
    "mu0 = 1.75 # hypothesized value\n",
    "s = np.std(x, ddof=1) # sample standard deviation\n",
    "n = len(x) # sample size\n",
    "\n",
    "print(xbar)\n",
    "\n",
    "tobs = (xbar - mu0) / (s / np.sqrt(n)) \n",
    "print(tobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **$p$-value** is the probability to observe a value $t$ more extreme than the observed one $t_{obs}$ under the null hypothesis $H_0$: $P(t > t_{obs} | H_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tobs = 2.39687663116 # assume the t-value\n",
    "tvalues = np.linspace(-10, 10, 100)\n",
    "plt.plot(tvalues, stats.t.pdf(tvalues, n-1), 'b-', label=\"T(n-1)\")\n",
    "upper_tval_tvalues = tvalues[tvalues > tobs]\n",
    "plt.fill_between(upper_tval_tvalues, 0, stats.t.pdf(upper_tval_tvalues, n-1), alpha=.8, label=\"p-value\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing pairwise associations\n",
    "\n",
    "Univariate statistical analysis: explore association betweens pairs of variables.\n",
    "\n",
    "\n",
    "- In statistics, a **categorical variable** or **factor** is a variable that can take on one of a limited, and usually fixed, number of possible values, thus assigning each individual to a particular group or \"category\". The levels are the possibles values of the variable. Number of levels = 2: binomial; Number of levels > 2: multinomial. There is no intrinsic ordering to the categories.  For example, gender is a categorical variable having two categories (male and female) and there is no intrinsic ordering to the categories. For example, Sex (Female, Male), Hair color (blonde, brown, etc.).\n",
    "\n",
    "- An **ordinal variable** is a categorical variable with a clear ordering of the levels. For example: drinks per day (none, small, medium and high).\n",
    "\n",
    "- A **continuous** or **quantitative variable** $x \\in \\mathbb{R}$ is one that can take any value in a range of possible values, possibly infinite.  E.g.: salary, experience in years, weight.\n",
    "\n",
    "**What statistical test should I use?**\n",
    "\n",
    "See: http://www.ats.ucla.edu/stat/mult_pkg/whatstat/\n",
    "\n",
    "![Statistical tests](images/stat_tests_flowchart.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson correlation test: test association between two quantitative variables\n",
    "\n",
    "Test the correlation coefficient of two quantitative variables. The test calculates a Pearson correlation coefficient and the $p$-value for testing non-correlation.\n",
    "\n",
    "Let $x$ and $y$ two quantitative variables, where $n$ samples were obeserved. The linear correlation coeficient is defined as :\n",
    "\n",
    "$$r=\\frac{\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar x)^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar y)^2}}.$$\n",
    "\n",
    "Under $H_0$, the test statistic $t=\\sqrt{n-2}\\frac{r}{\\sqrt{1-r^2}}$ follow Student distribution with $n-2$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "n = 50\n",
    "x = np.random.normal(size=n)\n",
    "y = 2 * x + np.random.normal(size=n)\n",
    "\n",
    "# Compute with scipy\n",
    "cor, pval = stats.pearsonr(x, y)\n",
    "print(cor, pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two sample (Student) $t$-test: compare two means\n",
    "\n",
    "![Two-sample model](images/model_two-sample.png){width=7cm}\n",
    "\n",
    "The two-sample $t$-test (Snedecor and Cochran, 1989) is used to determine if two population means are equal. There are several variations on this test.\n",
    "If data are paired (e.g. 2 measures, before and after treatment for each individual) use the one-sample $t$-test of the difference. The variances of the two samples may be assumed to be equal (a.k.a. homoscedasticity) or unequal (a.k.a. heteroscedasticity).\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "1. Independence of **residuals** ($\\varepsilon_i$). This assumptions **must** be satisfied.\n",
    "2. Normality of residuals. Approximately normally distributed can be accepted.\n",
    "3. Homosedasticity use T-test, Heterosedasticity use Welch t-test.\n",
    "\n",
    "\n",
    "### 1. Model the data\n",
    "\n",
    "Assume that the two random variables are normally distributed: $y_1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}), y_2 \\sim \\mathcal{N}(\\mu_{2}, \\sigma_2)$.\n",
    "\n",
    "### 2. Fit: estimate the model parameters\n",
    "\n",
    "Estimate means and variances: $\\bar{y_1}, s^2_{y_1}, \\bar{y_2}, s^2_{y_2}$.\n",
    "\n",
    "### 3. $t$-test\n",
    "\n",
    "The general principle is \n",
    "\n",
    "\\begin{align}\n",
    "t &= \\frac{\\text{difference of means}}{\\text{standard dev of error}}\\\\\n",
    "  &= \\frac{\\text{difference of means}}{\\text{its standard error}}\\\\\n",
    "  &= \\frac{\\bar{y_1}-\\bar{y_2}}{\\sqrt{\\sum\\varepsilon^2}}\\sqrt{n-2}\\\\\n",
    "&= \\frac{\\bar{y_1}-\\bar{y_2}}{s_{\\bar{y_1}-\\bar{y_2}}}\n",
    "\\end{align}\n",
    "\n",
    "Since $y_1$ and $y_2$ are independant:\n",
    "\n",
    "\\begin{align}\n",
    "s^2_{\\bar{y_1}-\\bar{y_2}} &= s^2_{\\bar{y_1}} + s^2_{\\bar{y_2}} = \\frac{s^2_{y_1}}{n_1} + \\frac{s^2_{y_2}}{n_2}\\\\\n",
    "\\text{thus}\\\\\n",
    "s_{\\bar{y_1}-\\bar{y_2}} &= \\sqrt{\\frac{s^2_{y_1}}{n_1} + \\frac{s^2_{y_2}}{n_2}}\n",
    "\\end{align}\n",
    "\n",
    "#### Equal or unequal sample sizes, unequal variances (Welch's $t$-test)\n",
    "\n",
    "Welch's $t$-test defines the $t$ statistic as\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{y_1} - \\bar{y_2}}{\\sqrt{\\frac{s^2_{y_1}}{n_1} + \\frac{s^2_{y_2}}{n_2}}}.\n",
    "$$\n",
    "\n",
    "To compute the $p$-value one needs the degrees of freedom associated with this variance estimate. It is approximated using the Welch–Satterthwaite equation:\n",
    "\n",
    "$$\n",
    "\\nu \\approx \\frac{\\left(\\frac{s^2_{y_1}}{n_1} + \\frac{s^2_{y_2}}{n_2}\\right)^2}{\\frac{s^4_{y_1}}{n_1^2(n_1-1)} + \\frac{s^4_{y_2}}{n_2^2(n_2-1)}}.\n",
    "$$\n",
    "\n",
    "#### Equal or unequal sample sizes, equal variances\n",
    "\n",
    "If we assume equal variance (ie, $s^2_{y_1} = s^2_{y_1} = s^2$), where $s^2$ is an estimator of the common variance of the two samples:\n",
    "\n",
    "\\begin{align}\n",
    "s^2 &= \\frac{s_{y_1}^2(n_1-1)+s_{y_2}^2(n_2-1)}{n_1+n_2-2}\\\\\n",
    "    &= \\frac{\\sum_i^{n_1} (y_{1i} -\\bar{y_1})^2 + \\sum_j^{n_2} (y_{2j} -\\bar{y_2})^2}{(n_1 - 1) + (n_2 - 1)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "s_{\\bar{y_1}-\\bar{y_2}} = \\sqrt{\\frac{s^2}{n_1} + \\frac{s^2}{n_2}} = s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n",
    "$$\n",
    "\n",
    "Therefore, the $t$ statistic, that is used to test whether the means are different is:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{y_1} - \\bar{y_2}}{s \\cdot \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}},\n",
    "$$\n",
    "\n",
    "#### Equal sample sizes, equal variances\n",
    "\n",
    "If we simplify the problem assuming equal samples of size $n_1 = n_2 = n$ we get\n",
    "\n",
    "\\begin{align}\n",
    "t &= \\frac{\\bar{y_1} - \\bar{y_2}}{s \\sqrt{2}} \\cdot \\sqrt{n}\\\\\n",
    "&\\approx \\text{effect size} \\cdot \\sqrt{n}\\\\\n",
    "&\\approx \\frac{\\text{difference of means}}{\\text{standard deviation of the noise}} \\cdot \\sqrt{n}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "Given the following two samples, test whether their means are equal using the **standard t-test, assuming equal variance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "height = np.array([ 1.83,  1.83,  1.73,  1.82,  1.83,  1.73,  1.99,  1.85,  1.68,  1.87,\n",
    "                    1.66,  1.71,  1.73,  1.64,  1.70,  1.60,  1.79,  1.73,  1.62,  1.77])\n",
    "\n",
    "grp = np.array([\"M\"] * 10 + [\"F\"] * 10)\n",
    "\n",
    "# Compute with scipy\n",
    "print(stats.ttest_ind(height[grp == \"M\"], height[grp == \"F\"], equal_var=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA $F$-test (quantitative ~ categorial (>=2 levels))\n",
    "\n",
    "Analysis of variance (ANOVA) provides a statistical test of whether or not the means of several (k) groups are equal, and therefore generalizes the $t$-test to more than two groups. ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance. It is conceptually similar to multiple two-sample $t$-tests, but is less conservative.\n",
    "\n",
    "Here we will consider one-way ANOVA with one independent variable, ie one-way anova.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/F-test):\n",
    "\n",
    "- Test if any group is on average superior, or inferior, to the others versus the null hypothesis that all four strategies yield the same mean response\n",
    "\n",
    "- Detect any of several possible differences.\n",
    "\n",
    "- The advantage of the ANOVA $F$-test is that we do not need to pre-specify which strategies are to be compared, and we do not need to adjust for making multiple comparisons.\n",
    "\n",
    "- The disadvantage of the ANOVA $F$-test is that if we reject the null hypothesis, we do not know which strategies can be said to be significantly different from the others.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "1. The samples are randomly selected in an independent manner from the k populations.\n",
    "2. All k populations have distributions that are approximately normal. Check by plotting groups distribution.\n",
    "3. The k population variances are equal. Check by plotting groups distribution.\n",
    "\n",
    "### 1. Model the data\n",
    "\n",
    "Is there a difference in Petal Width in species from iris dataset.\n",
    "Let $y_1, y_2$ and $y_3$ be Petal Width in three species.\n",
    "\n",
    "Here we assume (see assumptions) that the three populations were sampled from three random variables that are normally distributed. I.e., $Y_1 \\sim N(\\mu_1, \\sigma_1), Y_2 \\sim N(\\mu_2, \\sigma_2)$ and $Y_3 \\sim N(\\mu_3, \\sigma_3)$.\n",
    "\n",
    "### 2. Fit: estimate the model parameters\n",
    "\n",
    "Estimate means and variances: $\\bar{y}_i, \\sigma_i,\\;\\; \\forall i \\in \\{1, 2, 3\\}$.\n",
    "\n",
    "### 3. $F$-test\n",
    "\n",
    "The formula for the one-way ANOVA F-test statistic is\n",
    "\n",
    "\\begin{align}\n",
    "F &= \\frac{\\text{Explained variance}}{\\text{Unexplained variance}}\\\\\n",
    "&=\\frac{\\text{Between-group variability}}{\\text{Within-group variability}} = \\frac{s^2_B}{s^2_W}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The \"explained variance\", or \"between-group variability\" is\n",
    "\n",
    "$$\n",
    "s^2_B = \\sum_i n_i(\\bar{y}_{i\\cdot} - \\bar{y})^2/(K-1),\n",
    "$$\n",
    "\n",
    "where $\\bar{y}_{i\\cdot}$ denotes the sample mean in the $i$th group, $n_i$ is the number of observations in the $i$th group, $\\bar{y}$ denotes the overall mean of the data, and $K$ denotes the number of groups.\n",
    "\n",
    "The \"unexplained variance\", or \"within-group variability\" is\n",
    "\n",
    "$$\n",
    "s^2_W = \\sum_{ij} (y_{ij}-\\bar{y}_{i\\cdot})^2/(N-K), \n",
    "$$\n",
    "\n",
    "where $y_{ij}$ is the $j$th observation in the $i$th out of $K$ groups and $N$ is the overall sample size. This $F$-statistic follows the $F$-distribution with $K-1$ and $N-K$ degrees of freedom under the null hypothesis. The statistic will be large if the between-group variability is large relative to the within-group variability, which is unlikely to happen if the population means of the groups all have the same value.\n",
    "\n",
    "Note that when there are only two groups for the one-way ANOVA F-test, $F=t^2$ where $t$ is the Student's $t$ statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load iris datset\n",
    "iris = sm.datasets.get_rdataset(\"iris\").data\n",
    "iris.columns = [s.replace('.', '') for s in iris.columns]\n",
    "\n",
    "# Group means\n",
    "means = iris.groupby(\"Species\").mean().reset_index()\n",
    "print(means)\n",
    "\n",
    "# Group Stds (equal variances ?)\n",
    "stds = iris.groupby(\"Species\").std(ddof=1).reset_index()\n",
    "print(stds)\n",
    "\n",
    "# Plot groups\n",
    "ax = sns.violinplot(x=\"Species\", y=\"SepalLength\", data=iris)\n",
    "ax = sns.swarmplot(x=\"Species\", y=\"SepalLength\", data=iris,\n",
    "                   color=\"white\")\n",
    "ax = sns.swarmplot(x=\"Species\", y=\"SepalLength\",  color=\"black\", data=means, size=10)\n",
    "\n",
    "# ANOVA\n",
    "lm = ols('SepalLength ~ Species', data=iris).fit()\n",
    "sm.stats.anova_lm(lm, typ=2) # Type 2 ANOVA DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square, $\\chi^2$ (categorial ~ categorial)\n",
    "\n",
    "Computes the chi-square, $\\chi^2$, statistic and $p$-value for the hypothesis test of independence of frequencies in the observed contingency table (cross-table). The observed frequencies are tested against an expected contingency table obtained by computing expected frequencies based on the marginal sums under the assumption of independence.\n",
    "\n",
    "Example: 20 participants: 10 exposed to some chemical product and 10 non exposed (exposed = 1 or 0). Among the 20 participants 10 had cancer 10 not (cancer = 1 or 0). $\\chi^2$ tests the association between those two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Dataset:\n",
    "# 15 samples:\n",
    "# 10 first exposed\n",
    "exposed = np.array([1] * 10 + [0] * 10)\n",
    "# 8 first with cancer, 10 without, the last two with.\n",
    "cancer = np.array([1] * 8 + [0] * 10 + [1] * 2)\n",
    "\n",
    "crosstab = pd.crosstab(exposed, cancer, rownames=['exposed'],\n",
    "                       colnames=['cancer'])\n",
    "print(\"Observed table:\")\n",
    "print(\"---------------\")\n",
    "print(crosstab)\n",
    "\n",
    "chi2, pval, dof, expected = stats.chi2_contingency(crosstab)\n",
    "print(\"Statistics:\")\n",
    "print(\"-----------\")\n",
    "print(\"Chi2 = %f, pval = %f\" % (chi2, pval))\n",
    "print(\"Expected table:\")\n",
    "print(\"---------------\")\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing expected cross-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute expected cross-table based on proportion\n",
    "exposed_marg = crosstab.sum(axis=0)\n",
    "exposed_freq = exposed_marg / exposed_marg.sum()\n",
    "\n",
    "cancer_marg = crosstab.sum(axis=1)\n",
    "cancer_freq = cancer_marg / cancer_marg.sum()\n",
    "\n",
    "print('Exposed frequency? Yes: %.2f' % exposed_freq[0],\n",
    "      'No: %.2f' % exposed_freq[1])\n",
    "print('Cancer frequency? Yes: %.2f' % cancer_freq[0],\n",
    "      'No: %.2f' % cancer_freq[1])\n",
    "\n",
    "print('Expected frequencies:')\n",
    "print(np.outer(exposed_freq, cancer_freq))\n",
    "\n",
    "print('Expected cross-table (frequencies * N): ')\n",
    "print(np.outer(exposed_freq, cancer_freq) * len(exposed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric test of pairwise associations\n",
    "\n",
    "### Spearman rank-order correlation (quantitative ~ quantitative)\n",
    "\n",
    "The Spearman correlation is a non-parametric measure of the monotonicity of the relationship between two datasets.\n",
    "\n",
    "When to use it? Observe the data distribution:\n",
    "- presence of **outliers**\n",
    "- the distribution of the residuals is not Gaussian.\n",
    "\n",
    "Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as $x$ increases, so does $y$. Negative correlations imply that as $x$ increases, $y$ decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# Age uniform distribution between 20 and 40\n",
    "age = np.random.uniform(20, 60, 40)\n",
    "\n",
    "# Systolic blood presure, 2 groups:\n",
    "# - 15 subjects at 0.05 * age + 6\n",
    "# - 25 subjects at 0.15 * age + 10\n",
    "sbp = np.concatenate((0.05 * age[:15] + 6, 0.15 * age[15:] + 10)) + \\\n",
    "    .5 * np.random.normal(size=40)\n",
    "\n",
    "sns.regplot(x=age, y=sbp)\n",
    "\n",
    "# Non-Parametric Spearman\n",
    "cor, pval = stats.spearmanr(age, sbp)\n",
    "print(\"Non-Parametric Spearman cor test, cor: %.4f, pval: %.4f\" % (cor, pval))\n",
    "\n",
    "# \"Parametric Pearson cor test\n",
    "cor, pval = stats.pearsonr(age, sbp)\n",
    "print(\"Parametric Pearson cor test: cor: %.4f, pval: %.4f\" % (cor, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcoxon signed-rank test (quantitative ~ cte)\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n",
    "\n",
    "The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used when comparing two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test). It is equivalent to one-sample test of the difference of paired samples.\n",
    "\n",
    "It can be used as an alternative to the paired Student's $t$-test, $t$-test for matched pairs, or the $t$-test for dependent samples when the population cannot be assumed to be normally distributed.\n",
    "\n",
    "When to use it? Observe the data distribution:\n",
    "- presence of outliers\n",
    "- the distribution of the residuals is not Gaussian\n",
    "\n",
    "It has a lower sensitivity compared to $t$-test. May be problematic to use when the sample size is small.\n",
    "\n",
    "Null hypothesis $H_0$: difference between the pairs follows a symmetric distribution around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "n = 20\n",
    "# Buisness Volume time 0\n",
    "bv0 = np.random.normal(loc=3, scale=.1, size=n)\n",
    "# Buisness Volume time 1\n",
    "bv1 = bv0 + 0.1 + np.random.normal(loc=0, scale=.1, size=n)\n",
    "\n",
    "# create an outlier\n",
    "bv1[0] -= 10\n",
    "\n",
    "# Paired t-test\n",
    "print(stats.ttest_rel(bv0, bv1))\n",
    "\n",
    "# Wilcoxon\n",
    "print(stats.wilcoxon(bv0, bv1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann–Whitney $U$ test (quantitative ~ categorial (2 levels))\n",
    "\n",
    "In statistics, the Mann–Whitney $U$ test (also called the Mann–Whitney–Wilcoxon, Wilcoxon rank-sum test or Wilcoxon–Mann–Whitney test) is a nonparametric test of the null hypothesis that two samples come from the same population against an alternative hypothesis, especially that a particular population tends to have larger values than the other.\n",
    "\n",
    "It can be applied on unknown distributions contrary to e.g. a $t$-test that has to be applied only on normal distributions, and it is nearly as efficient as the $t$-test on normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "n = 20\n",
    "# Buismess Volume group 0\n",
    "bv0 = np.random.normal(loc=1, scale=.1, size=n)\n",
    "\n",
    "# Buismess Volume group 1\n",
    "bv1 = np.random.normal(loc=1.2, scale=.1, size=n)\n",
    "\n",
    "# create an outlier\n",
    "bv1[0] -= 10\n",
    "\n",
    "# Two-samples t-test\n",
    "print(stats.ttest_ind(bv0, bv1))\n",
    "\n",
    "# Wilcoxon\n",
    "print(stats.mannwhitneyu(bv0, bv1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model\n",
    "\n",
    "![Linear model](images/model_lm.png){width=5cm}\n",
    "\n",
    "\n",
    "Given $n$ random samples $(y_i, x_{1i}, \\ldots, x_{pi}), \\, i = 1, \\ldots, n$, the linear regression models the relation between the observations $y_i$ and the independent variables $x_i^p$ is formulated as\n",
    "\n",
    "$$\n",
    "    y_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi} + \\varepsilon_i \\qquad i = 1, \\ldots, n \n",
    "$$\n",
    "\n",
    "- The $\\beta$'s are the model parameters, ie, the regression coeficients.\n",
    "- $\\beta_0$ is the intercept or the bias.\n",
    "- $\\varepsilon_i$ are the **residuals**.\n",
    "- **An independent variable (IV)**. It is a variable that stands alone and isn't changed by the other variables you are trying to measure. For example, someone's age might be an independent variable. Other factors (such as what they eat, how much they go to school, how much television they watch) aren't going to change a person's age. In fact, when you are looking for some kind of relationship between variables you are trying to see if the independent variable causes some kind of change in the other variables, or dependent variables. In Machine Learning, these variables are also called the **predictors**.\n",
    "\n",
    "- A **dependent variable**. It is something that depends on other factors. For example, a test score could be a dependent variable because it could change depending on several factors such as how much you studied, how much sleep you got the night before you took the test, or even how hungry you were when you took it. Usually when you are looking for a relationship between two things you are trying to find out what makes the dependent variable change the way it does. In Machine Learning this variable is called a **target variable**.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "1. Independence of residuals ($\\varepsilon_i$). This assumptions **must** be satisfied\n",
    "2. Normality of residuals ($\\varepsilon_i$). Approximately normally distributed can be accepted.\n",
    "\n",
    "[Regression diagnostics:  testing the assumptions of linear regression](http://people.duke.edu/~rnau/testing.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple regression: test association between two quantitative variables\n",
    "\n",
    "Using the dataset \"salary\", explore the association between the dependant variable (e.g. Salary) and the independent variable (e.g.: Experience is quantitative), considering only non-managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "url = 'https://github.com/duchesnay/pystatsml/raw/master/datasets/salary_table.csv'\n",
    "salary = pd.read_csv(url)\n",
    "salary = salary[salary.management == 'N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Model the data\n",
    "\n",
    "Model the data on some **hypothesis** e.g.: salary is a linear function of the experience.\n",
    "\n",
    "$$\n",
    "    \\text{salary}_i = \\beta_0 + \\beta~\\text{experience}_i + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "more generally\n",
    "\n",
    "$$\n",
    "    y_i = \\beta_0 + \\beta~x_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "This can be rewritten in the matrix form using the design matrix made of values of independant variable and the intercept:\n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5  \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix}1 & x_1  \\\\1 & x_2  \\\\1 & x_3  \\\\1 & x_4  \\\\1 & x_5    \\end{bmatrix}\n",
    "  \\begin{bmatrix} \\beta_0 \\\\ \\beta_1  \\end{bmatrix}\n",
    "  +\n",
    "  \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\end{bmatrix}\\end{split}\n",
    "$$\n",
    "\n",
    "- $\\beta$: the slope or coefficient or parameter of the model,\n",
    "\n",
    "- $\\beta_0$: the **intercept** or **bias** is the second parameter of the model,\n",
    "\n",
    "- $\\epsilon_i$: is the $i$th error, or residual with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "The simple regression is equivalent to the Pearson correlation.\n",
    "\n",
    "#### 2. Fit: estimate the model parameters\n",
    "\n",
    "The goal it so estimate $\\beta$, $\\beta_0$ and $\\sigma^2$.\n",
    "\n",
    "Minimizes the **mean squared error (MSE)** or the **Sum squared error (SSE)**. The so-called **Ordinary Least Squares (OLS)** finds $\\beta, \\beta_0$ that minimizes the $SSE = \\sum_i \\epsilon_i^2$\n",
    "\n",
    "$$\n",
    "SSE = \\sum_i(y_i - \\beta~x_i - \\beta_0)^2\n",
    "$$\n",
    "\n",
    "Recall from calculus that an extreme point can be found by computing where the derivative is zero, i.e. to find the intercept, we perform the steps:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_0} = \\sum_i(y_i - \\beta~x_i - \\beta_0) = 0\\\\\n",
    "\\sum_i y_i = \\beta~\\sum_i x_i + n~\\beta_0\\\\\n",
    "n~\\bar{y} = n~\\beta~\\bar{x} + n~\\beta_0\\\\\n",
    "\\beta_0 = \\bar{y} - \\beta~\\bar{x}\n",
    "$$\n",
    "\n",
    "To find the regression coefficient, we perform the steps:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta} = \\sum_i x_i(y_i - \\beta~x_i - \\beta_0) = 0\n",
    "$$\n",
    "\n",
    "Plug in $\\beta_0$:\n",
    "\n",
    "$$\n",
    "\\sum_i x_i(y_i - \\beta~x_i - \\bar{y} + \\beta \\bar{x}) = 0\\\\\n",
    "\\sum_i x_i y_i - \\bar{y}\\sum_i x_i = \\beta \\sum_i(x_i - \\bar{x})\n",
    "$$\n",
    "\n",
    "Divide both sides by $n$:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_i x_i y_i  - \\bar{y}\\bar{x} = \\frac{1}{n}\\beta \\sum_i(x_i - \\bar{x})\\\\\n",
    "\\beta = \\frac{\\frac{1}{n}\\sum_i x_i y_i  - \\bar{y}\\bar{x}}{\\frac{1}{n}\\sum_i(x_i - \\bar{x})} = \\frac{Cov(x, y)}{Var(x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "y, x = salary.salary, salary.experience\n",
    "beta, beta0, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "print(\"y = %f x + %f,  r: %f, r-squared: %f,\\np-value: %f, std_err: %f\" \n",
    "      % (beta, beta0, r_value, r_value**2, p_value, std_err))\n",
    "\n",
    "print(\"Regression line with the scatterplot\")\n",
    "yhat = beta * x  +  beta0 # regression line\n",
    "plt.plot(x, yhat, 'r-', x, y,'o')\n",
    "plt.xlabel('Experience (years)')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "print(\"Using seaborn\")\n",
    "import seaborn as sns\n",
    "ax = sns.regplot(x=\"experience\", y=\"salary\", data=salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple regression\n",
    "\n",
    "#### Theory\n",
    "\n",
    "Muliple Linear Regression is the most basic supervised learning algorithm.\n",
    "\n",
    "Given: a set of training data $\\{x_1, ... , x_N\\}$ with corresponding targets $\\{y_1, . . . , y_N\\}$.\n",
    "\n",
    "In linear regression, we assume that the model that generates the data involves only a linear combination of the input variables, i.e.\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_P x_{iP} + \\varepsilon_i,\n",
    "$$\n",
    "\n",
    "or, simplified\n",
    "\n",
    "$$\n",
    "y_i  = \\beta_0 + \\sum_{j=1}^{P-1} \\beta_j x_i^j + \\varepsilon_i.\n",
    "$$\n",
    "\n",
    "Extending each sample with an intercept, $x_i := [1, x_i] \\in R^{P+1}$ allows us to use a more general notation based on linear algebra and write it as a simple dot product:\n",
    "\n",
    "$$\n",
    "y_i = \\mathbf{x}_i^T\\mathbf{\\beta} + \\varepsilon_i,\n",
    "$$\n",
    "\n",
    "where $\\beta \\in R^{P+1}$ is a vector of weights that define the $P+1$ parameters of the model. From now we have $P$ regressors + the intercept.\n",
    "\n",
    "Using the matrix notation:\n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5  \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix}1 & x_{11}  & \\ldots & x_{1P}\\\\1 & x_{21} & \\ldots & x_{2P}  \\\\1 & x_{31} & \\ldots & x_{3P}  \\\\1 & x_{41} & \\ldots & x_{4P}  \\\\1 & x_5 & \\ldots & x_5    \\end{bmatrix}\n",
    "  \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_P \\end{bmatrix}\n",
    "  +\n",
    "  \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\end{bmatrix}\\end{split}\n",
    "$$\n",
    "\n",
    "Let $X = [x_0^T, ... , x_N^T]$ be the ($N \\times P+1$) **design matrix** of $N$ samples of $P$ input features with one column of one and let be $y = [y_1, ... , y_N]$ be a vector of the $N$ targets.\n",
    "\n",
    "$$\n",
    "y = X \\beta + \\varepsilon\n",
    "$$\n",
    "\n",
    "Minimize the Mean Squared Error MSE loss:\n",
    "\n",
    "$$\n",
    "MSE(\\beta) =  = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\mathbf{x}_i^T\\beta)^2\n",
    "$$\n",
    "\n",
    "Using the matrix notation, the **mean squared error (MSE) loss can be rewritten**:\n",
    "\n",
    "$$\n",
    "MSE(\\beta) = \\frac{1}{N}||y - X\\beta||_2^2.\n",
    "$$\n",
    "\n",
    "The $\\beta$ that minimises the MSE can be found by:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\beta \\left(\\frac{1}{N} ||y - X\\beta||_2^2\\right) &= 0\\\\\n",
    "\\frac{1}{N}\\nabla_\\beta (y - X\\beta)^T (y - X\\beta) &= 0\\\\\n",
    "\\frac{1}{N}\\nabla_\\beta (y^Ty - 2 \\beta^TX^Ty + \\beta^T X^TX\\beta) &= 0\\\\\n",
    "    -2X^Ty + 2 X^TX\\beta &= 0\\\\\n",
    "    X^TX\\beta &= X^Ty\\\\\n",
    "    \\beta &= (X^TX)^{-1} X^Ty,\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $(X^TX)^{-1} X^T$ is a pseudo inverse of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit with `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "np.random.seed(seed=42)  # make the example reproducible\n",
    "\n",
    "# Dataset\n",
    "N, P = 50, 4\n",
    "X = np.random.normal(size= N * P).reshape((N, P))\n",
    "## Our model needs an intercept so we add a column of 1s:\n",
    "X[:, 0] = 1\n",
    "print(X[:5, :])\n",
    "\n",
    "betastar = np.array([10, 1., .5, 0.1])\n",
    "e = np.random.normal(size=N)\n",
    "y = np.dot(X, betastar) + e\n",
    "\n",
    "# Estimate the parameters\n",
    "Xpinv = linalg.pinv2(X)\n",
    "betahat = np.dot(Xpinv, y)\n",
    "print(\"Estimated beta:\\n\", betahat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model with statsmodels\n",
    "\n",
    "Sources: http://statsmodels.sourceforge.net/devel/examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple regression\n",
    "\n",
    "#### Interface with statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "## Fit and summary:\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# prediction of new values\n",
    "ypred = model.predict(X)\n",
    "\n",
    "# residuals + prediction == true values\n",
    "assert np.all(ypred + model.resid == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interface with Pandas\n",
    "\n",
    "Use `R` language syntax for data.frame. For an additive model:\n",
    "$y_i = \\beta^0 + x_i^1 \\beta^1 + x_i^2 \\beta^2 + \\epsilon_i \\equiv$ `y ~ x1 + x2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "df = pd.DataFrame(np.column_stack([X, y]), columns=['inter', 'x1','x2', 'x3', 'y'])\n",
    "print(df.columns, df.shape)\n",
    "# Build a model excluding the intercept, it is implicit\n",
    "model = smf.ols(\"y~x1 + x2 + x3\", df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple regression with categorical independent variables or factors: Analysis of covariance (ANCOVA)\n",
    "\n",
    "Analysis of covariance (ANCOVA) is a linear model that blends ANOVA and linear regression. ANCOVA evaluates whether population means of a dependent variable (DV) are equal across levels of a categorical independent variable (IV) often called a treatment, while statistically controlling for the effects of other quantitative or continuous variables that are not of primary interest, known as covariates (CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"../datasets/salary_table.csv\")\n",
    "except:\n",
    "    url = 'https://github.com/duchesnay/pystatsml/raw/master/datasets/salary_table.csv'\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "lm = smf.ols('salary ~ experience', df).fit()\n",
    "df[\"residuals\"] = lm.resid\n",
    "\n",
    "print(\"Jarque-Bera normality test p-value %.5f\" % sms.jarque_bera(lm.resid)[1])\n",
    "\n",
    "ax = sns.displot(df, x='residuals', kind=\"kde\", fill=True)\n",
    "ax = sns.displot(df, x='residuals', kind=\"kde\", hue='management', fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality assumption of the residuals can be rejected (p-value < 0.05). There is an efect of the \"management\" factor, take it into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-way AN(C)OVA\n",
    "\n",
    "- ANOVA: one categorical independent variable, i.e. one factor.\n",
    "\n",
    "- ANCOVA: ANOVA with some covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneway = smf.ols('salary ~ management + experience', df).fit()\n",
    "df[\"residuals\"] = oneway.resid\n",
    "sns.displot(df, x='residuals', kind=\"kde\", fill=True)\n",
    "print(sm.stats.anova_lm(oneway, typ=2))\n",
    "print(\"Jarque-Bera normality test p-value %.3f\" % sms.jarque_bera(oneway.resid)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of residuals is still not normal but closer to normality. \n",
    "Both management and experience are significantly associated with salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two-way AN(C)OVA\n",
    "\n",
    "Ancova with two categorical independent variables, i.e. two factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoway = smf.ols('salary ~ education + management + experience', df).fit()\n",
    "\n",
    "df[\"residuals\"] = twoway.resid\n",
    "sns.displot(df, x='residuals', kind=\"kde\", fill=True)\n",
    "print(sm.stats.anova_lm(twoway, typ=2))\n",
    "\n",
    "print(\"Jarque-Bera normality test p-value %.3f\" % sms.jarque_bera(twoway.resid)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality assumtion cannot be rejected. Assume it.\n",
    "Education, management and experience are significantly associated with salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing two nested models\n",
    "\n",
    "`oneway` is nested within `twoway`.  Comparing two nested models tells us if the additional predictors (i.e. `education`) of the full model significantly decrease the residuals. Such comparison can be done using an $F$-test on residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twoway.compare_f_test(oneway))  # return F, pval, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twoway is significantly better than one way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor coding\n",
    "\n",
    "See http://statsmodels.sourceforge.net/devel/contrasts.html\n",
    "\n",
    "By default Pandas use \"dummy coding\". Explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twoway.model.data.param_names)\n",
    "print(twoway.model.data.exog[:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrasts and post-hoc tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test of the specific contribution of experience:\n",
    "ttest_exp = twoway.t_test([0, 0, 0, 0, 1])\n",
    "ttest_exp.pvalue, ttest_exp.tvalue\n",
    "print(ttest_exp)\n",
    "\n",
    "# Alternatively, you can specify the hypothesis tests using a string\n",
    "twoway.t_test('experience')\n",
    "\n",
    "# Post-hoc is salary of Master different salary of Ph.D? \n",
    "# ie. t-test salary of Master = salary of Ph.D.\n",
    "print(twoway.t_test('education[T.Master] = education[T.Ph.D]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=42)  # make example reproducible\n",
    "\n",
    "# Dataset\n",
    "n_samples, n_features = 100, 1000\n",
    "n_info = int(n_features/10)  # number of features with information\n",
    "n1, n2 = int(n_samples/2), n_samples - int(n_samples/2)\n",
    "snr = .5\n",
    "Y = np.random.randn(n_samples, n_features)\n",
    "grp = np.array([\"g1\"] * n1 + [\"g2\"] * n2)\n",
    "\n",
    "# Add some group effect for Pinfo features\n",
    "Y[grp==\"g1\", :n_info] += snr\n",
    "\n",
    "# \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "tvals, pvals = np.full(n_features, np.NAN), np.full(n_features, np.NAN)\n",
    "for j in range(n_features):\n",
    "    tvals[j], pvals[j] = stats.ttest_ind(Y[grp==\"g1\", j], Y[grp==\"g2\", j],\n",
    "                                         equal_var=True)\n",
    "\n",
    "fig, axis = plt.subplots(3, 1, figsize=(9, 9))#, sharex='col')\n",
    "\n",
    "axis[0].plot(range(n_features), tvals, 'o')\n",
    "axis[0].set_ylabel(\"t-value\")\n",
    "\n",
    "axis[1].plot(range(n_features), pvals, 'o')\n",
    "axis[1].axhline(y=0.05, color='red', linewidth=3, label=\"p-value=0.05\")\n",
    "#axis[1].axhline(y=0.05, label=\"toto\", color='red')\n",
    "axis[1].set_ylabel(\"p-value\")\n",
    "axis[1].legend()\n",
    "\n",
    "axis[2].hist([pvals[n_info:], pvals[:n_info]], \n",
    "    stacked=True, bins=100, label=[\"Negatives\", \"Positives\"])\n",
    "axis[2].set_xlabel(\"p-value histogram\")\n",
    "axis[2].set_ylabel(\"density\")\n",
    "axis[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that under the null hypothesis the distribution of the *p*-values is uniform.\n",
    "\n",
    "Statistical measures:\n",
    "\n",
    "- **True Positive (TP)** equivalent to a hit. The test correctly concludes the presence of an effect.\n",
    "\n",
    "- True Negative (TN). The test correctly concludes the absence of an effect.\n",
    "\n",
    "- **False Positive (FP)** equivalent to a false alarm, **Type I error**. The test improperly concludes the presence of an effect. Thresholding at $p\\text{-value} < 0.05$ leads to 47 FP.\n",
    "\n",
    "- False Negative (FN) equivalent to a miss, Type II error. The test improperly concludes the absence of an effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, N = n_info, n_features - n_info  # Positives, Negatives\n",
    "TP = np.sum(pvals[:n_info ] < 0.05)  # True Positives\n",
    "FP = np.sum(pvals[n_info: ] < 0.05)  # False Positives\n",
    "print(\"No correction, FP: %i (expected: %.2f), TP: %i\" % (FP, N * 0.05, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni correction for multiple comparisons\n",
    "\n",
    "The Bonferroni correction is based on the idea that if an experimenter is testing $P$ hypotheses, then one way of maintaining the familywise error rate (FWER) is to test each individual hypothesis at a statistical significance level of $1/P$ times the desired maximum overall level.\n",
    "\n",
    "So, if the desired significance level for the whole family of tests is $\\alpha$ (usually 0.05), then the Bonferroni correction would test each individual hypothesis at a significance level of $\\alpha/P$. For example, if a trial is testing $P = 8$ hypotheses with a desired $\\alpha = 0.05$, then the Bonferroni correction would test each individual hypothesis at $\\alpha = 0.05/8 = 0.00625$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.sandbox.stats.multicomp as multicomp\n",
    "_, pvals_fwer, _, _  = multicomp.multipletests(pvals, alpha=0.05, \n",
    "                                               method='bonferroni')\n",
    "TP = np.sum(pvals_fwer[:n_info ] < 0.05)  # True Positives\n",
    "FP = np.sum(pvals_fwer[n_info: ] < 0.05)  # False Positives\n",
    "print(\"FWER correction, FP: %i, TP: %i\" % (FP, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The False discovery rate (FDR) correction for multiple comparisons\n",
    "\n",
    "FDR-controlling procedures are designed to control the expected proportion of rejected null hypotheses that were incorrect rejections (\"false discoveries\"). FDR-controlling procedures provide less stringent control of Type I errors compared to the familywise error rate (FWER) controlling procedures (such as the Bonferroni correction), which control the probability of at least one Type I error. Thus, FDR-controlling procedures have greater power, at the cost of increased rates of Type I errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.sandbox.stats.multicomp as multicomp\n",
    "_, pvals_fdr, _, _  = multicomp.multipletests(pvals, alpha=0.05, \n",
    "                                               method='fdr_bh')\n",
    "TP = np.sum(pvals_fdr[:n_info ] < 0.05)  # True Positives\n",
    "FP = np.sum(pvals_fdr[n_info: ] < 0.05)  # False Positives\n",
    "\n",
    "print(\"FDR correction, FP: %i, TP: %i\" % (FP, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "\n",
    "### Simple linear regression and correlation (application)\n",
    "\n",
    "Load the dataset: birthwt Risk Factors Associated with Low Infant Birth Weight at\n",
    "https://github.com/duchesnay/pystatsml/raw/master/datasets/birthwt.csv\n",
    "\n",
    "1. Test the association of mother’s (`bwt`) age and birth weight using the correlation test and linear regeression.\n",
    "\n",
    "2. Test the association of mother’s weight (`lwt`) and birth weight using the correlation test and linear regeression.\n",
    "\n",
    "3. Produce two scatter plot of: (i) age by birth weight; (ii) mother’s weight by birth weight.\n",
    "\n",
    "Conclusion ?\n",
    "\n",
    "\n",
    "### Simple linear regression (maths)\n",
    "\n",
    "Considering the salary and the experience of the salary table.\n",
    "`https://github.com/duchesnay/pystatsml/raw/master/datasets/salary_table.csv`\n",
    "\n",
    "\n",
    "Compute:\n",
    "\n",
    "- Estimate the model paramters $\\beta, \\beta_0$ using scipy `stats.linregress(x,y)` \n",
    "\n",
    "- Compute the predicted values $\\hat{y}$\n",
    "\n",
    "Compute:\n",
    "\n",
    "- $\\bar{y}$: `y_mu`\n",
    "\n",
    "- $SS_\\text{tot}$: `ss_tot`\n",
    "\n",
    "- $SS_\\text{reg}$: `ss_reg`\n",
    "\n",
    "- $SS_\\text{res}$: `ss_res`\n",
    "\n",
    "- Check partition of variance formula based on sum of squares by using `assert np.allclose(val1, val2, atol=1e-05)`\n",
    "\n",
    "- Compute $R^2$ and compare it with the `r_value` above\n",
    "\n",
    "- Compute the $F$ score\n",
    "\n",
    "- Compute the $p$-value:\n",
    " *  Plot the $F(1, n)$ distribution for 100 $f$ values within $[10, 25]$. Draw $P(F(1, n) > F)$, i.e. color the surface defined by the $x$ values larger than $F$ below the $F(1, n)$.\n",
    "\n",
    " * $P(F(1, n) > F)$ is the $p$-value, compute it.\n",
    " \n",
    "\n",
    "### Multiple regression\n",
    "\n",
    "Considering the simulated data used below: \n",
    "\n",
    "1. What are the dimensions of $\\mathrm{pinv}(X)$?\n",
    "\n",
    "2. Compute the MSE between the predicted values and the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "np.random.seed(seed=42)  # make the example reproducible\n",
    "\n",
    "# Dataset\n",
    "N, P = 50, 4\n",
    "X = np.random.normal(size= N * P).reshape((N, P))\n",
    "## Our model needs an intercept so we add a column of 1s:\n",
    "X[:, 0] = 1\n",
    "print(X[:5, :])\n",
    "\n",
    "betastar = np.array([10, 1., .5, 0.1])\n",
    "e = np.random.normal(size=N)\n",
    "y = np.dot(X, betastar) + e\n",
    "\n",
    "# Estimate the parameters\n",
    "Xpinv = linalg.pinv2(X)\n",
    "betahat = np.dot(Xpinv, y)\n",
    "print(\"Estimated beta:\\n\", betahat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two sample t-test (maths)\n",
    "\n",
    "Given the following two sample, test whether their means are equals.\n",
    "\n",
    "```\n",
    "height = np.array([ 1.83,  1.83,  1.73,  1.82,  1.83,\n",
    "                    1.73,1.99,  1.85,  1.68,  1.87,\n",
    "                    1.66,  1.71,  1.73,  1.64,  1.70,\n",
    "                    1.60,  1.79,  1.73,  1.62,  1.77])\n",
    "grp = np.array([\"M\"] * 10 + [\"F\"] * 10)\n",
    "```\n",
    "\n",
    "- Compute the means/std-dev per groups.\n",
    "\n",
    "- Compute the $t$-value (standard two sample t-test with equal variances).\n",
    "\n",
    "- Compute the $p$-value.\n",
    "\n",
    "- The $p$-value is one-sided: a two-sided test would test `P(T > tval)`\n",
    "  and `P(T < -tval)`. What would the two sided $p$-value be?\n",
    "\n",
    "- Compare the two-sided $p$-value with the one obtained by `stats.ttest_ind` using `assert np.allclose(arr1, arr2)`.\n",
    "\n",
    "\n",
    "### Two sample t-test (application)\n",
    "\n",
    "Risk Factors Associated with Low Infant Birth Weight: https://github.com/duchesnay/pystatsml/raw/master/datasets/birthwt.csv\n",
    "\n",
    "\n",
    "1. Explore the data\n",
    "\n",
    "2. Recode smoke factor\n",
    "\n",
    "3. Compute the means/std-dev per groups.\n",
    "\n",
    "4. Plot birth weight by smoking (box plot, violin plot or histogram)\n",
    "\n",
    "5. Test the effect of smoking on birth weight\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two sample t-test and random permutations\n",
    "\n",
    "Generate 100 samples following the model:\n",
    "\n",
    "$$\n",
    "y = g + \\varepsilon\n",
    "$$\n",
    "\n",
    "Where the noise $\\varepsilon \\sim N(1, 1)$ and $g \\in \\{0, 1\\}$ is a group indicator variable with 50 ones and 50 zeros.\n",
    "\n",
    "- Write a function `tstat(y, g)` that compute the two samples t-test of y splited in two groups defined by g.\n",
    "\n",
    "- Sample the t-statistic distribution under the null hypothesis using random permutations.\n",
    "\n",
    "- Assess the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate associations (developpement) \n",
    "\n",
    "Write a function `univar_stat(df, target, variables)` that computes the parametric statistics and $p$-values between the `target` variable (provided as as string) and all `variables` (provided as a list of string) of the pandas DataFrame `df`. The target is a quantitative variable but variables may be quantitative or qualitative. The function returns a DataFrame with four columns: `variable`, `test`, `value`, `p_value`.\n",
    "\n",
    "Apply it to the salary dataset available at https://github.com/duchesnay/pystatsml/raw/master/datasets/salary_table.csv, with target being `S`: salaries for IT staff in a corporation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple comparisons\n",
    "\n",
    "This exercise has 2 goals: apply you knowledge of statistics using vectorized numpy operations.\n",
    "Given the dataset provided for multiple comparisons, compute the two-sample $t$-test (assuming equal variance) for each (column) feature of the `Y` array given the two groups defined by `grp` variable. You should return two vectors of size `n_features`: one for the $t$-values and one for the $p$-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA\n",
    "\n",
    "Perform an ANOVA dataset described bellow\n",
    "\n",
    "- Compute between and within variances\n",
    "- Compute $F$-value: `fval`\n",
    "- Compare the $p$-value with the one obtained by `stats.f_oneway` using `assert np.allclose(arr1, arr2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "mu_k = np.array([1, 2, 3])    # means of 3 samples\n",
    "sd_k = np.array([1, 1, 1])    # sd of 3 samples\n",
    "n_k = np.array([10, 20, 30])  # sizes of 3 samples\n",
    "grp = [0, 1, 2]               # group labels\n",
    "n = np.sum(n_k)\n",
    "label = np.hstack([[k] * n_k[k] for k in [0, 1, 2]])\n",
    "\n",
    "y = np.zeros(n)\n",
    "for k in grp:\n",
    "    y[label == k] = np.random.normal(mu_k[k], sd_k[k], n_k[k])\n",
    "\n",
    "# Compute with scipy\n",
    "fval, pval = stats.f_oneway(y[label == 0], y[label == 1], y[label == 2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
