{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold learning:  non-linear dimension reduction\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Scikit-learn documentation](http://scikit-learn.org/stable/modules/manifold.html)\n",
    "\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Isomap)\n",
    "\n",
    "Nonlinear dimensionality reduction or **manifold learning**  cover unsupervised methods that attempt to identify low-dimensional manifolds within the original $P$-dimensional space that represent high data density. Then those methods provide a mapping from the high-dimensional space to the low-dimensional embedding.\n",
    "\n",
    "\n",
    "## Multi-dimensional Scaling (MDS)\n",
    "\n",
    "Resources:\n",
    "\n",
    "- http://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf\n",
    "- https://en.wikipedia.org/wiki/Multidimensional_scaling\n",
    "- Hastie, Tibshirani and Friedman (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction.* New York: Springer, Second Edition.\n",
    "\n",
    "The purpose of MDS is to find a low-dimensional projection of the data in which the pairwise distances between data points is preserved, as closely as possible (in a least-squares sense).\n",
    "\n",
    "- Let $\\mathbf{D}$ be the $(N \\times N)$ pairwise distance matrix where $d_{ij}$ is *a distance* between points $i$ and $j$.\n",
    "- The MDS concept can be extended to a wide variety of data types specified in terms of a similarity matrix.\n",
    "\n",
    "Given the dissimilarity (distance) matrix $\\mathbf{D}_{N \\times N}=[d_{ij}]$, MDS attempts to find  $K$-dimensional projections of the $N$ points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\in \\mathbb{R}^K$, concatenated in an $\\mathbf{X}_{N \\times K}$ matrix, so that $d_{ij} \\approx \\|\\mathbf{x}_i - \\mathbf{x}_j\\|$ are as close as possible. This can be obtained by the minimization of a loss function called the **stress function**\n",
    "\n",
    "$$\n",
    "\\mathrm{stress}(\\mathbf{X}) = \\sum_{i \\neq j}\\left(d_{ij} -  \\|\\mathbf{x}_i - \\mathbf{x}_j\\|\\right)^2.\n",
    "$$\n",
    "\n",
    "This loss function is known as *least-squares* or *Kruskal-Shepard* scaling.\n",
    "\n",
    "A modification of *least-squares* scaling is the *Sammon mapping*\n",
    "\n",
    "$$\n",
    "\\mathrm{stress}_{\\mathrm{Sammon}}(\\mathbf{X}) = \\sum_{i \\neq j} \\frac{(d_{ij} -  \\|\\mathbf{x}_i - \\mathbf{x}_j\\|)^2}{d_{ij}}.\n",
    "$$\n",
    "\n",
    "The Sammon mapping performs better at preserving small distances compared to the *least-squares* scaling.\n",
    "\n",
    "### Classical multidimensional scaling\n",
    "\n",
    "Also known as *principal coordinates analysis*, PCoA.\n",
    "\n",
    "- The distance matrix, $\\mathbf{D}$, is transformed to a *similarity matrix*, $\\mathbf{B}$, often using centered inner products.\n",
    "\n",
    "- The loss function becomes\n",
    "\n",
    "$$\n",
    "\\mathrm{stress}_{\\mathrm{classical}}(\\mathbf{X}) = \\sum_{i \\neq j} \\big(b_{ij} -  \\langle\\mathbf{x}_i, \\mathbf{x}_j\\rangle\\big)^2.\n",
    "$$\n",
    "\n",
    "- The stress function in classical MDS is sometimes called *strain*.\n",
    "\n",
    "- The solution for the classical MDS problems can be found from the eigenvectors of the similarity matrix.\n",
    "\n",
    "- If the distances in $\\mathbf{D}$ are Euclidean and double centered inner products are used, the results are equivalent to PCA.\n",
    "\n",
    "### Example\n",
    "\n",
    "The ``eurodist`` datset provides the road distances (in kilometers) between 21 cities in Europe.\n",
    "Given this matrix of pairwise (non-Euclidean) distances $\\mathbf{D}=[d_{ij}]$, MDS can be used to recover the coordinates of the cities in *some* Euclidean referential whose orientation is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.461678Z",
     "iopub.status.busy": "2020-10-11T22:53:11.461206Z",
     "iopub.status.idle": "2020-10-11T22:53:11.550662Z",
     "shell.execute_reply": "2020-10-11T22:53:11.550314Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pairwise distance between European cities\n",
    "try:\n",
    "    url = '../datasets/eurodist.csv'\n",
    "    df = pd.read_csv(url)\n",
    "except:\n",
    "    url = 'https://github.com/duchesnay/pystatsml/raw/master/datasets/eurodist.csv'\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "print(df.iloc[:5, :5])\n",
    "\n",
    "city = df[\"city\"]\n",
    "D = np.array(df.iloc[:, 1:])  # Distance matrix\n",
    "\n",
    "# Arbitrary choice of K=2 components\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(dissimilarity='precomputed', n_components=2, random_state=40, max_iter=3000, eps=1e-9)\n",
    "X = mds.fit_transform(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover coordinates of the cities in Euclidean referential whose orientation is arbitrary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.553745Z",
     "iopub.status.busy": "2020-10-11T22:53:11.553265Z",
     "iopub.status.idle": "2020-10-11T22:53:11.555476Z",
     "shell.execute_reply": "2020-10-11T22:53:11.555174Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "Deuclidean = metrics.pairwise.pairwise_distances(X, metric='euclidean')\n",
    "print(np.round(Deuclidean[:5, :5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.568797Z",
     "iopub.status.busy": "2020-10-11T22:53:11.568330Z",
     "iopub.status.idle": "2020-10-11T22:53:11.688507Z",
     "shell.execute_reply": "2020-10-11T22:53:11.688172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot: apply some rotation and flip\n",
    "theta = 80 * np.pi / 180.\n",
    "rot = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                [np.sin(theta),  np.cos(theta)]])\n",
    "Xr = np.dot(X, rot)\n",
    "# flip x\n",
    "Xr[:, 0] *= -1\n",
    "plt.scatter(Xr[:, 0], Xr[:, 1])\n",
    "\n",
    "for i in range(len(city)):\n",
    "    plt.text(Xr[i, 0], Xr[i, 1], city[i])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the number of components\n",
    "\n",
    "We must choose $K^* \\in \\{1, \\ldots,  K\\}$ the number of required components. Plotting the values of the stress function, obtained using $k \\leq N-1$ components. In general, start with $1, \\ldots K \\leq 4$. Choose $K^*$ where you can clearly distinguish an *elbow* in the stress curve.\n",
    "\n",
    "Thus, in the plot below, we choose to retain information accounted for by the first *two* components, since this is where the *elbow* is in the stress curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.693707Z",
     "iopub.status.busy": "2020-10-11T22:53:11.692712Z",
     "iopub.status.idle": "2020-10-11T22:53:12.198962Z",
     "shell.execute_reply": "2020-10-11T22:53:12.199281Z"
    }
   },
   "outputs": [],
   "source": [
    "k_range = range(1, min(5, D.shape[0]-1))\n",
    "stress = [MDS(dissimilarity='precomputed', n_components=k,\n",
    "           random_state=42, max_iter=300, eps=1e-9).fit(D).stress_ for k in k_range]\n",
    "\n",
    "print(stress)\n",
    "plt.plot(k_range, stress)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"stress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Apply MDS from `sklearn` on the `iris` dataset available at: \n",
    "\n",
    "https://github.com/duchesnay/pystatsml/raw/master/datasets/iris.csv\n",
    "\n",
    "- Center and scale the dataset.\n",
    "\n",
    "- Compute Euclidean pairwise distances matrix.\n",
    "\n",
    "- Select the number of components.\n",
    "\n",
    "- Show that classical MDS on Euclidean pairwise distances matrix is equivalent to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold learning\n",
    "\n",
    "Dataset S curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "X, color = datasets.make_s_curve(1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isomap \n",
    "\n",
    "Isomap is a nonlinear dimensionality reduction method that combines a procedure to compute the distance matrix with MDS. The distances calculation is based on geodesic distances evaluated on neighborhood graph:\n",
    "\n",
    "1. Determine the neighbors of each point. All points in some fixed radius or K nearest neighbors.\n",
    "\n",
    "2. Construct a neighborhood graph. Each point is connected to other if it is a K nearest neighbor. Edge length equal to Euclidean distance.\n",
    "\n",
    "3. Compute shortest path between pairwise of points $d_{ij}$ to build the distance matrix $\\mathbf{D}$. \n",
    "\n",
    "4. Apply MDS on  $\\mathbf{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = manifold.Isomap(n_neighbors=10, n_components=2)\n",
    "X_isomap = isomap.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE \n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne)\n",
    "\n",
    "Principles\n",
    "\n",
    "1. Construct a (Gaussian) probability distribution between pairs of object in input (high-dimensional) space.\n",
    "2. Construct a (student) ) probability distribution between pairs of object in embeded (low-dimensional) space.\n",
    "3. Minimize the Kullback–Leibler divergence (KL divergence) between the two distributions.\n",
    "\n",
    "Features\n",
    "\n",
    "- Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold\n",
    "- t-SNE will focus on the **local structure** of the data and will tend to extract clustered **local groups** of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:12.204064Z",
     "iopub.status.busy": "2020-10-11T22:53:12.203634Z",
     "iopub.status.idle": "2020-10-11T22:53:12.793580Z",
     "shell.execute_reply": "2020-10-11T22:53:12.793933Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.suptitle(\"Manifold Learning\", fontsize=14)\n",
    "\n",
    "ax = fig.add_subplot(131, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(4, -72)\n",
    "plt.title('2D \"S shape\" manifold in 3D')\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "plt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Isomap\")\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE\")\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Run [Manifold learning on handwritten digits: Locally Linear Embedding, Isomap with scikit-learn](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
