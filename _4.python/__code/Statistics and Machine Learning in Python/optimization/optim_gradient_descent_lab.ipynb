{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Gradient Descent with numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:54:03.865332Z",
     "iopub.status.busy": "2020-10-11T22:54:03.864941Z",
     "iopub.status.idle": "2020-10-11T22:54:04.065456Z",
     "shell.execute_reply": "2020-10-11T22:54:04.065002Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Simple regression problem\n",
    "\n",
    "[source Daniel Godoy\n",
    "](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)\n",
    "\n",
    "$$\n",
    "y = w_0 + w x + \\varepsilon\n",
    "$$\n",
    "\n",
    "### Data Generation\n",
    "\n",
    "Let’s start generating some synthetic data: we start with a vector of 100 points for our feature x and create our labels using a = 1, b = 2 and some Gaussian noise.\n",
    "Next, let’s split our synthetic data into train and validation sets, shuffling the array of indices and using the first 80 shuffled points for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:54:04.069373Z",
     "iopub.status.busy": "2020-10-11T22:54:04.069009Z",
     "iopub.status.idle": "2020-10-11T22:54:04.070848Z",
     "shell.execute_reply": "2020-10-11T22:54:04.070486Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent: Theory\n",
    "\n",
    "If you are comfortable with the inner workings of gradient descent, feel free to skip this section. It goes beyond the scope of this post to fully explain how gradient descent works, but I’ll cover the four basic steps you’d need to go through to compute it.\n",
    "\n",
    "#### Step 1: Compute the Loss\n",
    "\n",
    "For a regression problem, the loss is given by the Mean Square Error (MSE), that is, the average of all squared differences between labels (y) and predictions (a + bx).\n",
    "It is worth mentioning that, if we use all points in the training set (N) to compute the loss, we are performing a batch gradient descent. If we were to use a single point at each time, it would be a stochastic gradient descent. Anything else (n) in-between 1 and N characterizes a mini-batch gradient descent.\n",
    "\n",
    "\\begin{align*}\n",
    "MSE &= \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2\\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i - w_0 - w x_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "#### Step 2: Compute the Gradients\n",
    "\n",
    "A gradient is a partial derivative — why partial? Because one computes it with respect to (w.r.t.) a single parameter. We have two parameters, a and b, so we must compute two partial derivatives.\n",
    "A derivative tells you how much a given quantity changes when you slightly vary some other quantity. In our case, how much does our MSE loss change when we vary each one of our two parameters?\n",
    "The right-most part of the equations below is what you usually see in implementations of gradient descent for a simple linear regression. In the intermediate step, I show you all elements that pop-up from the application of the chain rule, so you know how the final expression came to be.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial MSE}{\\partial w_0} &= \\frac{\\partial MSE}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_0}\\\\\n",
    " &= \\frac{1}{N}\\sum_{i=1}^{N} 2 (y_i - w_0 - w x_i) (-1) =  \\frac{1}{N}\\sum_{i=1}^{N} -2 (y_i - \\hat{y}_i)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial MSE}{\\partial w} &= \\frac{\\partial MSE}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w}\\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N} 2 (y_i - w_0 - w x_i) (-1 x_i) =  \\frac{1}{N}\\sum_{i=1}^{N}-2 x_i(y_i - \\hat{y}_i)\n",
    "\\end{align*}\n",
    "\n",
    "#### Step 3: Update the Parameters\n",
    "\n",
    "In the final step, we use the gradients to update the parameters. Since we are trying to minimize our losses, we reverse the sign of the gradient for the update.\n",
    "\n",
    "There is still another parameter to consider: the learning rate, denoted by the Greek letter eta (that looks like the letter n), which is the multiplicative factor that we need to apply to the gradient for the parameter update.\n",
    "\n",
    "\\begin{align*}\n",
    "w_0 &= w_0 - \\eta \\frac{\\partial MSE}{\\partial w_0}\\\\\n",
    "  &= w_0 - \\eta \\frac{1}{N}\\sum_{i=1}^{N} (-2) (y_i - \\hat{y}_i)\\\\\n",
    "w &= w - \\eta \\frac{\\partial MSE}{\\partial w}\\\\\n",
    "&= w - \\eta  \\frac{1}{N}\\sum_{i=1}^{N} (-2) x_i(y_i - \\hat{y}_i)\n",
    "\\end{align*}\n",
    "\n",
    "How to choose a learning rate? That is a topic on its own and beyond the scope of this post as well.\n",
    "\n",
    "See:\n",
    "- LeCun Y.A., Bottou L., Orr G.B., Müller KR. (2012) [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf). In: Montavon G., Orr G.B., Müller KR. (eds) Neural Networks: Tricks of the Trade. Lecture Notes in Computer Science, vol 7700. Springer, Berlin, Heidelberg\n",
    "\n",
    "- Introduction to [Gradient Descent Algorithm](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/) (along with variants) in Machine Learning: Gradient Descent with Momentum, ADAGRAD and ADAM.\n",
    "\n",
    "#### Step 4: Repeat!\n",
    "\n",
    "Now we use the updated parameters to go back to Step 1 and restart the process.\n",
    "\n",
    "An epoch is complete whenever every point has been already used for computing the loss. For batch gradient descent, this is trivial, as it uses all points for computing the loss — one epoch is the same as one update. For stochastic gradient descent, one epoch means N updates, while for mini-batch (of size n), one epoch has N/n updates.\n",
    "\n",
    "Repeating this process over and over, for many epochs, is, in a nutshell, training a model.\n",
    "\n",
    "### Gradient Descent: 1D with numpy\n",
    "\n",
    "It’s time to implement our linear regression model using gradient descent using Numpy only.\n",
    "\n",
    "For training a model, there are two initialization steps:\n",
    "\n",
    "- Random initialization of parameters/weights (we have only two, a and b);\n",
    "- Initialization of hyper-parameters (in our case, only learning rate and number of epochs);\n",
    "\n",
    "Make sure to always initialize your random seed to ensure reproducibility of your results. As usual, the random seed is 42, the least random of all random seeds one could possibly choose :-)\n",
    "\n",
    "For each epoch, there are four training steps:\n",
    "- Compute model’s predictions — this is the forward pass;\n",
    "- Compute the loss, using predictions;\n",
    "- Compute the gradients for every parameter;\n",
    "- Update the parameters;\n",
    "\n",
    "Just keep in mind that, if you don’t use batch gradient descent (our example does), you’ll have to write an inner loop to perform the four training steps for either each individual point (stochastic) or n points (mini-batch). We’ll see a mini-batch example later down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:54:04.095087Z",
     "iopub.status.busy": "2020-10-11T22:54:04.079634Z",
     "iopub.status.idle": "2020-10-11T22:54:04.462155Z",
     "shell.execute_reply": "2020-10-11T22:54:04.462468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Computes our model's predicted output\n",
    "    yhat = b + w * x_train\n",
    "    \n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Computes gradients for both \"a\" and \"b\" parameters\n",
    "    b_grad = -2 * error.mean()\n",
    "    w_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    \n",
    "print(\"Numpy:\", w, b)\n",
    "\n",
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(\"Sklearn:\",linr.coef_, linr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent: Multivariate with numpy\n",
    "\n",
    "Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:54:04.466506Z",
     "iopub.status.busy": "2020-10-11T22:54:04.466194Z",
     "iopub.status.idle": "2020-10-11T22:54:04.468046Z",
     "shell.execute_reply": "2020-10-11T22:54:04.467737Z"
    }
   },
   "outputs": [],
   "source": [
    "N, P = 100, 5\n",
    "\n",
    "# Data Generation\n",
    "w_ = np.array([1, 0.5, 0.1] + [0] * (P - 3))\n",
    "b_ = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(N, P)\n",
    "y = np.dot(x, w_) + b_ + np.random.randn(N)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:54:04.503687Z",
     "iopub.status.busy": "2020-10-11T22:54:04.492900Z",
     "iopub.status.idle": "2020-10-11T22:54:04.507360Z",
     "shell.execute_reply": "2020-10-11T22:54:04.507017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(P)\n",
    "\n",
    "#print(b, w)\n",
    "\n",
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Computes our model's predicted output\n",
    "    yhat = b + np.dot(x_train, w)\n",
    "\n",
    "    # How wrong is our model? That's the error!\n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    #print(yhat.shape, y_train.shape, error.shape, loss.shape, x_train.shape)\n",
    "\n",
    "    # Computes gradients for both \"a\" and \"b\" parameters\n",
    "    b_grad = -2 * error.mean()\n",
    "    # w_grad = -2 * (x_train * error).mean()\n",
    "    w_grad = -2 * (x_train * error[:, np.newaxis]).mean(axis=0)\n",
    "\n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "\n",
    "print(\"Numpy:\", w, b)\n",
    "\n",
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(\"Sklearn:\",linr.coef_, linr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weigths vector associated to each output class is a \"template\" pattern of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
