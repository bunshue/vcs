{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 Auto-Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2019-06-27 22:03:23,011:EnsembleBuilder(1):657d208bbd472a68fa05392b12a54be2] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-06-27 22:03:23,016:EnsembleBuilder(1):657d208bbd472a68fa05392b12a54be2] No models better than random - using Dummy Score!\n",
      "Time limit for a single run is higher than total time limit. Capping the limit for a single run to the total time given to SMAC (119.877192)\n",
      "1\n",
      "['/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000003.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000004.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000005.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000006.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000007.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000008.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000009.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000010.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000011.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000012.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000013.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000014.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000015.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000016.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000017.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000018.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000019.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000020.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000021.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000022.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000023.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000024.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000025.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000026.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000027.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000028.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000029.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000030.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000031.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000032.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000033.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000034.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000035.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000036.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000037.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000038.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000039.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000040.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000041.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000042.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000043.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000044.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000045.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000046.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000047.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000048.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000049.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000050.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000051.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000052.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000053.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000054.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000055.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000056.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000057.ensemble', '/tmp/autosklearn_tmp_5676_9303/.auto-sklearn/ensembles/1.0000000058.ensemble']\n",
      "0.9311440677966102\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "  \n",
    "data = sm.datasets.anes96.load_pandas().data\n",
    "label = 'vote'\n",
    "features = [i for i in data.columns if i != label]\n",
    "X_train = data[features]\n",
    "y_train = data[label]\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=120, per_run_time_limit=120, # 兩分鐘\n",
    "    include_estimators=[\"random_forest\"])\n",
    "automl.fit(X_train, y_train)\n",
    "print(automl.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3 Auto-ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingClassifier to predict vote\n",
      "Started at:\n",
      "2019-06-27 22:05:18\n",
      "[1] random_holdout_set_from_training_data's score is: -0.209\n",
      "[2] random_holdout_set_from_training_data's score is: -0.183\n",
      "[3] random_holdout_set_from_training_data's score is: -0.162\n",
      "[4] random_holdout_set_from_training_data's score is: -0.145\n",
      "[5] random_holdout_set_from_training_data's score is: -0.133\n",
      "[6] random_holdout_set_from_training_data's score is: -0.121\n",
      "[7] random_holdout_set_from_training_data's score is: -0.112\n",
      "[8] random_holdout_set_from_training_data's score is: -0.105\n",
      "[9] random_holdout_set_from_training_data's score is: -0.1\n",
      "[10] random_holdout_set_from_training_data's score is: -0.096\n",
      "[11] random_holdout_set_from_training_data's score is: -0.091\n",
      "[12] random_holdout_set_from_training_data's score is: -0.088\n",
      "[13] random_holdout_set_from_training_data's score is: -0.085\n",
      "[14] random_holdout_set_from_training_data's score is: -0.083\n",
      "[15] random_holdout_set_from_training_data's score is: -0.081\n",
      "[16] random_holdout_set_from_training_data's score is: -0.079\n",
      "[17] random_holdout_set_from_training_data's score is: -0.077\n",
      "[18] random_holdout_set_from_training_data's score is: -0.076\n",
      "[19] random_holdout_set_from_training_data's score is: -0.075\n",
      "[20] random_holdout_set_from_training_data's score is: -0.073\n",
      "[21] random_holdout_set_from_training_data's score is: -0.072\n",
      "[22] random_holdout_set_from_training_data's score is: -0.071\n",
      "[23] random_holdout_set_from_training_data's score is: -0.07\n",
      "[24] random_holdout_set_from_training_data's score is: -0.07\n",
      "[25] random_holdout_set_from_training_data's score is: -0.069\n",
      "[26] random_holdout_set_from_training_data's score is: -0.069\n",
      "[27] random_holdout_set_from_training_data's score is: -0.068\n",
      "[28] random_holdout_set_from_training_data's score is: -0.068\n",
      "[29] random_holdout_set_from_training_data's score is: -0.067\n",
      "[30] random_holdout_set_from_training_data's score is: -0.067\n",
      "[31] random_holdout_set_from_training_data's score is: -0.067\n",
      "[32] random_holdout_set_from_training_data's score is: -0.067\n",
      "[33] random_holdout_set_from_training_data's score is: -0.066\n",
      "[34] random_holdout_set_from_training_data's score is: -0.066\n",
      "[35] random_holdout_set_from_training_data's score is: -0.066\n",
      "[36] random_holdout_set_from_training_data's score is: -0.066\n",
      "[37] random_holdout_set_from_training_data's score is: -0.066\n",
      "[38] random_holdout_set_from_training_data's score is: -0.066\n",
      "[39] random_holdout_set_from_training_data's score is: -0.066\n",
      "[40] random_holdout_set_from_training_data's score is: -0.066\n",
      "[41] random_holdout_set_from_training_data's score is: -0.066\n",
      "[42] random_holdout_set_from_training_data's score is: -0.066\n",
      "[43] random_holdout_set_from_training_data's score is: -0.066\n",
      "[44] random_holdout_set_from_training_data's score is: -0.066\n",
      "[45] random_holdout_set_from_training_data's score is: -0.065\n",
      "[46] random_holdout_set_from_training_data's score is: -0.066\n",
      "[47] random_holdout_set_from_training_data's score is: -0.066\n",
      "[48] random_holdout_set_from_training_data's score is: -0.065\n",
      "[49] random_holdout_set_from_training_data's score is: -0.065\n",
      "[50] random_holdout_set_from_training_data's score is: -0.065\n",
      "[52] random_holdout_set_from_training_data's score is: -0.065\n",
      "[54] random_holdout_set_from_training_data's score is: -0.064\n",
      "[56] random_holdout_set_from_training_data's score is: -0.065\n",
      "[58] random_holdout_set_from_training_data's score is: -0.065\n",
      "[60] random_holdout_set_from_training_data's score is: -0.065\n",
      "[62] random_holdout_set_from_training_data's score is: -0.065\n",
      "[64] random_holdout_set_from_training_data's score is: -0.065\n",
      "[66] random_holdout_set_from_training_data's score is: -0.065\n",
      "[68] random_holdout_set_from_training_data's score is: -0.065\n",
      "[70] random_holdout_set_from_training_data's score is: -0.064\n",
      "[72] random_holdout_set_from_training_data's score is: -0.065\n",
      "[74] random_holdout_set_from_training_data's score is: -0.064\n",
      "[76] random_holdout_set_from_training_data's score is: -0.065\n",
      "[78] random_holdout_set_from_training_data's score is: -0.065\n",
      "[80] random_holdout_set_from_training_data's score is: -0.065\n",
      "[82] random_holdout_set_from_training_data's score is: -0.065\n",
      "[84] random_holdout_set_from_training_data's score is: -0.065\n",
      "[86] random_holdout_set_from_training_data's score is: -0.065\n",
      "[88] random_holdout_set_from_training_data's score is: -0.065\n",
      "[90] random_holdout_set_from_training_data's score is: -0.065\n",
      "[92] random_holdout_set_from_training_data's score is: -0.066\n",
      "[94] random_holdout_set_from_training_data's score is: -0.066\n",
      "The number of estimators that were the best for this training dataset: 54\n",
      "The best score on the holdout set: -0.06439447673208941\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:00\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingClassifier\n",
      "predicting vote\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 22 | income=1.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 37 | income=16.0    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 35 | income=14.0    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 33 | income=12.0    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 32 | income=11.0    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 31 | income=10.0    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 30 | income=9.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 29 | income=8.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 26 | income=5.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 25 | income=4.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 24 | income=3.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 23 | income=2.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 19 | educ=5.0       |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 18 | educ=4.0       |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 16 | educ=2.0       |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 45 | income=24.0    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 13 | TVnews=6.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 12 | TVnews=5.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 11 | TVnews=4.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 10 | TVnews=3.0     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 15 | educ=1.0       |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 40 | income=19.0    |       0.0016 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 36 | income=15.0    |       0.0016 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 41 | income=20.0    |       0.0016 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 42 | income=21.0    |       0.0025 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 39 | income=18.0    |       0.0031 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "|  8 | TVnews=1.0     |       0.0031 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 44 | income=23.0    |       0.0032 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 21 | educ=7.0       |       0.0061 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 20 | educ=6.0       |       0.0064 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "|  7 | TVnews=0.0     |       0.0075 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 27 | income=6.0     |       0.0088 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 34 | income=13.0    |       0.0109 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 17 | educ=3.0       |       0.0118 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "|  9 | TVnews=2.0     |       0.0122 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 38 | income=17.0    |       0.0134 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 14 | TVnews=7.0     |       0.0137 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 28 | income=7.0     |       0.0176 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "|  0 | popul          |       0.0228 | 541.0166 |            0.0010 |           -0.0030 |    0.0014 |    0.0044 |    0.0000 |    0.0000 |\n",
      "| 43 | income=22.0    |       0.0360 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "|  6 | logpopul       |       0.0469 |   1.5927 |            0.0023 |           -0.0054 |    0.0039 |    0.0088 |    0.0000 |    0.0018 |\n",
      "|  5 | age            |       0.0507 |   8.2072 |           -0.0001 |            0.0003 |    0.0042 |    0.0045 |    0.0000 |    0.0000 |\n",
      "|  1 | selfLR         |       0.1004 |   0.7188 |           -0.0443 |            0.0251 |    0.0489 |    0.0305 |    0.0109 |    0.0100 |\n",
      "|  3 | DoleLR         |       0.1097 |   0.6343 |            0.0250 |           -0.0445 |    0.0373 |    0.0492 |    0.0104 |    0.0071 |\n",
      "|  2 | ClinLR         |       0.1198 |   0.6915 |            0.0423 |           -0.0492 |    0.0460 |    0.0517 |    0.0144 |    0.0169 |\n",
      "|  4 | PID            |       0.3886 |   1.1361 |           -0.0593 |            0.0331 |    0.0625 |    0.0352 |    0.0068 |    0.0073 |\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our brier-score-loss, which is the default value we optimized for while training, and is the value returned from .score() unless you requested a custom scoring metric\n",
      "It is a measure of how close the PROBABILITY predictions are.\n",
      "0.0439\n",
      "\n",
      "Here is the trained estimator's overall accuracy (when it predicts a label, how frequently is that the correct label?)\n",
      "93.9%\n",
      "\n",
      "Here is a confusion matrix showing predictions vs. actuals by label:\n",
      "Predicted >    0    1  All\n",
      "v Actual v                \n",
      "0.0          531   20  551\n",
      "1.0           38  355  393\n",
      "All          569  375  944\n",
      "\n",
      "Here is predictive value by class:\n",
      "Class:  1 = 0.9466666666666667\n",
      "Class:  0 = 0.9332161687170475\n",
      "+--------------------------------+-----------------------------------+--------------------------------+\n",
      "| Bucket Edges                   |   Predicted Probability Of Bucket |   Actual Probability of Bucket |\n",
      "|--------------------------------+-----------------------------------+--------------------------------|\n",
      "| (0.006860000000000001, 0.0164] |                            0.0135 |                         0.0000 |\n",
      "| (0.0164, 0.0218]               |                            0.0192 |                         0.0000 |\n",
      "| (0.0218, 0.0389]               |                            0.0293 |                         0.0000 |\n",
      "| (0.0389, 0.0599]               |                            0.0497 |                         0.0105 |\n",
      "| (0.0599, 0.147]                |                            0.0915 |                         0.0638 |\n",
      "| (0.147, 0.49]                  |                            0.2878 |                         0.3191 |\n",
      "| (0.49, 0.893]                  |                            0.7542 |                         0.7895 |\n",
      "| (0.893, 0.949]                 |                            0.9279 |                         0.9787 |\n",
      "| (0.949, 0.975]                 |                            0.9667 |                         1.0000 |\n",
      "| (0.975, 0.986]                 |                            0.9801 |                         1.0000 |\n",
      "+--------------------------------+-----------------------------------+--------------------------------+\n",
      "\n",
      "Here is the accuracy of our trained estimator at each level of predicted probabilities\n",
      "For a verbose description of what this means, please visit the docs:\n",
      "http://auto-ml.readthedocs.io/en/latest/analytics.html#interpreting-predicted-probability-buckets-for-classifiers\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0439486935641498"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auto_ml import Predictor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.anes96.load_pandas().data\n",
    "column_descriptions = {\n",
    "    'vote': 'output',\n",
    "    'TVnews': 'categorical',\n",
    "    'educ': 'categorical',\n",
    "    'income': 'categorical',\n",
    "}\n",
    "\n",
    "ml_predictor = Predictor(type_of_estimator='classifier', \n",
    "                         column_descriptions=column_descriptions)\n",
    "model = ml_predictor.train(data)\n",
    "model.score(data, data.vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.4 Auto-Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Directory: /tmp/autokeras_RLV8C1\n",
      "Preprocessing the images.\n",
      "Preprocessing finished.\n",
      "\n",
      "Initializing search.\n",
      "Initialization finished.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 0               |\n",
      "+----------------------------------------------+\n",
      "                                                                                                    \n",
      "No loss decrease after 5 epochs.\n",
      "\n",
      "\n",
      "Saving model.\n",
      "+--------------------------------------------------------------------------+\n",
      "|        Model ID        |          Loss          |      Metric Value      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           0            |  0.23364192992448807   |   0.9800000000000001   |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 1               |\n",
      "+----------------------------------------------+\n",
      "                                                                                                    \n",
      "No loss decrease after 5 epochs.\n",
      "\n",
      "\n",
      "Saving model.\n",
      "+--------------------------------------------------------------------------+\n",
      "|        Model ID        |          Loss          |      Metric Value      |\n",
      "+--------------------------------------------------------------------------+\n",
      "|           1            |  0.12853578999638557   |   0.9879999999999999   |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 2               |\n",
      "+----------------------------------------------+\n",
      "Epoch-8, Current Metric - 0.994: 470 batch [14:47,  1.91s/ batch]                                   Time is out.\n",
      "Epoch-15, Current Metric - 0.9949:  21%|████▋                 | 100/469 [04:23<17:05,  2.78s/ batch]"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from autokeras import ImageClassifier\n",
    "from autokeras.constant import Constant\n",
    "import autokeras\n",
    "from keras.utils import plot_model\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape + (1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,))\n",
    "    clf = ImageClassifier(verbose=True, augment=False)\n",
    "    clf.fit(x_train, y_train, time_limit=500 * 60)\n",
    "    clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)\n",
    "    y = clf.evaluate(x_test, y_test)\n",
    "    print(y * 100)\n",
    "    clf.export_keras_model('model.h5')\n",
    "    plot_model(clf, to_file='model.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
