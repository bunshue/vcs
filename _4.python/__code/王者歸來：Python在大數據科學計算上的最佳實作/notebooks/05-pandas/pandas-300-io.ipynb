{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.show_dimensions\", False)\n",
    "pd.set_option(\"display.float_format\", \"{:4.2g}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檔案的輸入輸出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **LINK**\n",
    "\n",
    "> http://air.epmap.org/\n",
    "\n",
    "> 空氣質量資料來源：青悅空氣質量歷史資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_list[0].count()     df_list[0].dtypes   \n",
      "------------------  -----------------------\n",
      "時間       100        時間       datetime64[ns]\n",
      "監測點       90        監測點              object\n",
      "AQI      100        AQI               int64\n",
      "PM2.5    100        PM2.5             int64\n",
      "PM10      98        PM10            float64\n",
      "dtype: int64        dtype: object          \n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "\n",
    "for df in pd.read_csv(\n",
    "        u\"data/aqi/上海市_201406.csv\", \n",
    "        encoding=\"utf-8-sig\",  #檔案解碼\n",
    "        chunksize=100,         #一次讀入的行數\n",
    "        usecols=[u\"時間\", u\"監測點\", \"AQI\", \"PM2.5\", \"PM10\"], #只讀入這些列\n",
    "        na_values=[\"-\", \"—\"],  #這些字串表示缺失資料\n",
    "        parse_dates=[0]):      #第一列為時間列\n",
    "    df_list.append(df)  #在這裡處理資料\n",
    "\n",
    "%C df_list[0].count(); df_list[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "print type(df.loc[0, u\"監測點\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **LINK**\n",
    "\n",
    "> http://www.nsmc.cma.gov.cn/FENGYUNCast/docs/HDF5.0_chinese.pdf\n",
    "\n",
    "> 中文的HDF5使用簡介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore(\"a.hdf5\", complib=\"blosc\", complevel=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dataframes/df1', '/dataframes/df2', '/series/s1']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame(np.random.rand(100000, 4), columns=list(\"ABCD\"))\n",
    "df2 = pd.DataFrame(np.random.randint(0, 10000, (10000, 3)), \n",
    "                   columns=[\"One\", \"Two\", \"Three\"])\n",
    "s1 = pd.Series(np.random.rand(1000))\n",
    "store[\"dataframes/df1\"] = df1\n",
    "store[\"dataframes/df2\"] = df2\n",
    "store[\"series/s1\"] = s1\n",
    "print store.keys()\n",
    "print df1.equals(store[\"dataframes/df1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **LINK**\n",
    "\n",
    "> http://pytables.github.io/usersguide/libref/hierarchy_classes.html\n",
    ">\n",
    "> `pytables`官方文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dataframes (Group) u''\n",
      "/series (Group) u''\n",
      "/dataframes/df1 (Group) u''\n",
      "/dataframes/df2 (Group) u''\n",
      "/series/s1 (Group) u''\n",
      "/series/s1/index (CArray(1000,), shuffle, blosc(9)) ''\n",
      "/series/s1/values (CArray(1000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/axis0 (CArray(4,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/axis1 (CArray(100000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/block0_items (CArray(4,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/block0_values (CArray(100000, 4), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/axis0 (CArray(3,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/axis1 (CArray(10000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/block0_items (CArray(3,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/block0_values (CArray(10000, 3), shuffle, blosc(9)) ''\n"
     ]
    }
   ],
   "source": [
    "root = store.get_node(\"//\")\n",
    "for node in root._f_walknodes():\n",
    "    print node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100100, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.append('dataframes/df_dynamic1', df1, append=False) #❶\n",
    "df3 = pd.DataFrame(np.random.rand(100, 4), columns=list(\"ABCD\"))\n",
    "store.append('dataframes/df_dynamic1', df3) #❷\n",
    "store['dataframes/df_dynamic1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        A     B    C     D\n",
      "98   0.95 0.072 0.78  0.18\n",
      "99   0.19 0.043 0.24 0.075\n",
      "100  0.21  0.78 0.86  0.47\n",
      "101  0.71  0.87 0.63  0.74\n",
      "98  0.058  0.18 0.91 0.083\n",
      "99   0.47  0.81 0.71  0.59\n"
     ]
    }
   ],
   "source": [
    "print store.select('dataframes/df_dynamic1', where='index > 97 & index < 102')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         A       B    C     D\n",
      "3656  0.99  0.0018 0.67  0.47\n",
      "5091     1   0.004 0.43  0.15\n",
      "17671    1  0.0042 0.99  0.31\n",
      "41052    1 0.00081  0.9  0.32\n",
      "45307    1  0.0093 0.72 0.065\n",
      "67976 0.99  0.0096 0.93  0.79\n",
      "69078    1  0.0055 0.97  0.88\n",
      "87871    1   0.008 0.59  0.35\n",
      "94421 0.99  0.0049 0.36   0.9\n"
     ]
    }
   ],
   "source": [
    "store.append('dataframes/df_dynamic1', df1, append=False, data_columns=[\"A\", \"B\"])\n",
    "print store.select('dataframes/df_dynamic1', where='A > 0.99 & B < 0.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**\n",
    "\n",
    "> 由於所有從CSV檔案讀入`DataFrame`物件的行索引都為預設值，因此HDF5檔案中的資料的行索引並不是唯一的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_aqi_files(fn_pattern):\n",
    "    from glob import glob\n",
    "    from os import path\n",
    "    \n",
    "    UTF8_BOM = b\"\\xEF\\xBB\\xBF\"\n",
    "    \n",
    "    cols = \"時間,城市,監測點,質量等級,AQI,PM2.5,PM10,CO,NO2,O3,SO2\".split(\",\")\n",
    "    float_dtypes = {col:float for col in \"AQI,PM2.5,PM10,CO,NO2,O3,SO2\".split(\",\")}\n",
    "    names_map = {\"時間\":\"Time\", \n",
    "                 \"監測點\":\"Position\", \n",
    "                 \"質量等級\":\"Level\", \n",
    "                 \"城市\":\"City\", \n",
    "                 \"PM2.5\":\"PM2_5\"}\n",
    "    \n",
    "    for fn in glob(fn_pattern):\n",
    "        with open(fn, \"rb\") as f:\n",
    "            sig = f.read(3) #❶\n",
    "            if sig != UTF8_BOM:\n",
    "                f.seek(0, 0)\n",
    "            df = pd.read_csv(f, \n",
    "                             parse_dates=[0], \n",
    "                             na_values=[\"-\", \"—\"], \n",
    "                             usecols=cols, \n",
    "                             dtype=float_dtypes) #❷\n",
    "        df.rename_axis(names_map, axis=1, inplace=True)  \n",
    "        df.dropna(inplace=True)\n",
    "        yield df\n",
    "\n",
    "\n",
    "store = pd.HDFStore(\"data/aqi/aqi.hdf5\", complib=\"blosc\", complevel=9)\n",
    "string_size = {\"City\": 12, \"Position\": 30, \"Level\":12}\n",
    "\n",
    "for idx, df in enumerate(read_aqi_files(u\"data/aqi/*.csv\")):\n",
    "    store.append('aqi', df, append=idx!=0, min_itemsize=string_size, data_columns=True) #❸\n",
    "    \n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337250\n"
     ]
    }
   ],
   "source": [
    "store = pd.HDFStore(\"data/aqi/aqi.hdf5\")\n",
    "df_aqi = store.select(\"aqi\")\n",
    "print len(df_aqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "df_polluted = store.select(\"aqi\", where=\"PM2_5 > 500\")\n",
    "print len(df_polluted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀寫資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///data/aqi/aqi.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    engine.execute(\"DROP TABLE aqi\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_cols = [\"Position\", \"City\", \"Level\"]\n",
    "\n",
    "for df in read_aqi_files(\"data/aqi/*.csv\"):\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].str.decode(\"utf8\")\n",
    "    df.to_sql(\"aqi\", engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aqi = pd.read_sql(\"aqi\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "df_polluted = pd.read_sql(\"select * from aqi where PM2_5 > 500\", engine)\n",
    "print len(df_polluted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pickle序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aqi.to_pickle(\"data/aqi/aqi.pickle\")\n",
    "df_aqi2 = pd.read_pickle(\"data/aqi/aqi.pickle\")\n",
    "df_aqi.equals(df_aqi2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
