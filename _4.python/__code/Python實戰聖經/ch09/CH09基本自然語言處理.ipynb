{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH09基本自然語言處理.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH4Ov_bhOcpg"
      },
      "source": [
        "## OpenCC：繁體簡體轉換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkjdnk5jMbNH"
      },
      "source": [
        "!pip install opencc-python-reimplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "116baE4IPe9r"
      },
      "source": [
        "from opencc import OpenCC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z50FhX1UPA4f"
      },
      "source": [
        "cc = OpenCC('t2s')\n",
        "text = '自然語言認知和理解是讓電腦把輸入的語言變成有意思的符號和關係，然後根據目的再處理。自然語言生成系統則是把計算機數據轉化為自然語言。'\n",
        "print(cc.convert(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GztCqY-P-EJ"
      },
      "source": [
        "cc = OpenCC('s2t')\n",
        "text = '自然语言认知和理解是让电脑把输入的语言变成有意思的符号和关系，然后根据目的再处理。自然语言生成系统则是把计算机数据转化为自然语言。'\n",
        "print(cc.convert(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eYxrbmmQKKS"
      },
      "source": [
        "text = '滑鼠在螢幕上移動'\n",
        "cc = OpenCC('t2s')\n",
        "print('一般轉換：{}'.format(cc.convert(text)))\n",
        "cc = OpenCC('tw2sp')\n",
        "print('慣用語轉換：{}'.format(cc.convert(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bquCfUxhSobL"
      },
      "source": [
        "text = '鼠标在屏幕上移动'\n",
        "cc = OpenCC('s2t')\n",
        "print('一般轉換：{}'.format(cc.convert(text)))\n",
        "cc = OpenCC('s2twp')\n",
        "print('片語轉換：{}'.format(cc.convert(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLcajNGjTW1t"
      },
      "source": [
        "## lotecc：繁簡批次檔案轉換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5wXdE43Tf99"
      },
      "source": [
        "!pip install lotecc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Io5AgTa3a72"
      },
      "source": [
        "from lotecc import lote_chinese_conversion as lotecc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NGFEySl4lmx"
      },
      "source": [
        "converted = lotecc(conversion='tw2sp',\n",
        "                   input='tradition1.txt',\n",
        "                   output='tradition1.txt',\n",
        "                   in_enc='utf-8',\n",
        "                   out_enc='utf-8',\n",
        "                   suffix='_t')\n",
        "print(converted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G03f0P2W8zEf"
      },
      "source": [
        "converted = lotecc(conversion='s2twp',\n",
        "                   input='simple',\n",
        "                   output='simple',\n",
        "                   in_enc='utf-8',\n",
        "                   out_enc='utf-8',\n",
        "                   suffix='_t')\n",
        "# print(converted)\n",
        "for source, output in converted:\n",
        "    print(f'原始檔案 <{source}> 轉換為 <{output}>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvN-7FtoDxvo"
      },
      "source": [
        "## jieba：最常用中文分詞工具"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcqHlTENF1kI"
      },
      "source": [
        "import jieba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tz8QiWlGSa_"
      },
      "source": [
        "sentence = '我今天要到台北松山機場出差！'\n",
        "breakword = jieba.cut(sentence, cut_all=False)\n",
        "print('精確模式：' + '|'.join(breakword))   \n",
        "\n",
        "breakword = jieba.cut(sentence, cut_all=True)\n",
        "print('全文模式：' + '|'.join(breakword))   \n",
        "\n",
        "breakword = jieba.cut_for_search(sentence)\n",
        "print('搜索引擎模式：' + '|'.join(breakword)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MhRsBPSaKJx"
      },
      "source": [
        "!wget -O dict.txt.big.txt https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/dict.txt.big"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qdDP1NjGtcL"
      },
      "source": [
        "jieba.set_dictionary('dict.txt.big.txt')\n",
        "sentence = '我今天要到台北松山機場出差！'\n",
        "breakword = jieba.cut(sentence, cut_all=False)\n",
        "print('|'.join(breakword)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zebt_j8KItuR"
      },
      "source": [
        "jieba.set_dictionary('dict.txt.big.txt')\n",
        "sentence = '這部電影很好看，是我的朋友陳國文主演的。'\n",
        "breakword = jieba.cut(sentence, cut_all=False)\n",
        "print('|'.join(breakword))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQHw3wRGI8a2"
      },
      "source": [
        "jieba.set_dictionary('dict.txt.big.txt')\n",
        "jieba.load_userdict('user_dict_test.txt')\n",
        "sentence = '這部電影很好看，是我的朋友陳國文主演的。'\n",
        "breakword = jieba.cut(sentence, cut_all=False)\n",
        "print('|'.join(breakword))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Vt0QIkKKb1"
      },
      "source": [
        "jieba.set_dictionary('dict.txt.big.txt')\n",
        "jieba.load_userdict('user_dict_test.txt')\n",
        "with open('stopWord_test.txt', 'r', encoding='utf-8-sig') as f:\n",
        "    stops = f.read().split('\\n')   \n",
        "sentence = '這部電影很好看，是我的朋友陳國文主演的。'\n",
        "breakword = jieba.cut(sentence, cut_all=False)\n",
        "words = []\n",
        "for word in breakword:\n",
        "    if word not in stops:\n",
        "        words.append(word)\n",
        "print('|'.join(words)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeBwzGtt9ZUK"
      },
      "source": [
        "## pywordseg：繁體中文斷詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFdZcEAi9T_o"
      },
      "source": [
        "!pip install pywordseg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Hm4mvS-biK"
      },
      "source": [
        "from pywordseg import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJSZhacx-skB"
      },
      "source": [
        "seg = Wordseg(batch_size=64, device=\"cuda:0\", embedding='elmo', elmo_use_cuda=True, mode=\"TW\")\n",
        "words = seg.cut([\"這部電影很好看，是我的朋友陳國文主演的。\"])\n",
        "print('|'.join(words[0])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h9-ZemuBqAU"
      },
      "source": [
        "seg = Wordseg(batch_size=64, embedding='elmo', elmo_use_cuda=False, mode=\"TW\")\n",
        "words = seg.cut([\"今天天氣真好啊!\", \"路遙知馬力，日久見人心。\"])\n",
        "for i in range(len(words)):\n",
        "  print('{}. {}'.format(i, '|'.join(words[i]))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDX79LGOEqja"
      },
      "source": [
        "seg = Wordseg(batch_size=64, embedding='elmo', elmo_use_cuda=False, mode=\"TW\")\n",
        "words = seg.cut([\"這部電影很好看，是我的朋友陳國文主演的。\"])\n",
        "with open('stopWord_test.txt', 'r', encoding='utf-8-sig') as f:\n",
        "    stops = f.read().split('\\n')   \n",
        "stopwords = []\n",
        "for word in words[0]:\n",
        "    if word not in stops:\n",
        "        stopwords.append(word)\n",
        "print('|'.join(stopwords)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb7PYprhG-SZ"
      },
      "source": [
        "## sumy：對網頁或文章進行摘要"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7qCq6W3HB2m"
      },
      "source": [
        "!pip install sumy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa6nWg-zOAcc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1kg6Pe_K4sn"
      },
      "source": [
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXogsV3rpGgR"
      },
      "source": [
        "LANGUAGE = \"chinese\"\n",
        "# LANGUAGE = \"english\"\n",
        "SENTENCES_COUNT = 5\n",
        "# SENTENCES_COUNT = 10\n",
        "url = \"https://news.ltn.com.tw/news/life/breakingnews/3649202\"\n",
        "# url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
        "parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "summarizer = Summarizer(Stemmer(LANGUAGE))\n",
        "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "sumies = summarizer(parser.document, SENTENCES_COUNT)\n",
        "for i, sentence in enumerate(sumies):\n",
        "    print('{}. {}'.format(i+1, sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQOmPiNEJovX"
      },
      "source": [
        "LANGUAGE = \"chinese\"\n",
        "SENTENCES_COUNT = 5\n",
        "parser = PlaintextParser.from_file(\"article1.txt\", Tokenizer(LANGUAGE))\n",
        "summarizer = Summarizer(Stemmer(LANGUAGE))\n",
        "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "sumies = summarizer(parser.document, SENTENCES_COUNT)\n",
        "for i, sentence in enumerate(sumies):\n",
        "    print('{}. {}'.format(i+1, sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKOQmB6WbGbf"
      },
      "source": [
        "## wordcloud：文字雲"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wQXozUI02w4"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import jieba\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "text = open('travel.txt', \"r\",encoding=\"utf-8\").read()\n",
        "jieba.set_dictionary('dict.txt.big.txt')\n",
        "with open('stopWord_cloud.txt', 'r', encoding='utf-8-sig') as f:\n",
        "#with open('stopWord_cloudmod.txt', 'r', encoding='utf-8-sig') as f:\n",
        "    stops = f.read().split('\\n')   \n",
        "terms = []\n",
        "for t in jieba.cut(text, cut_all=False):\n",
        "    if t not in stops:\n",
        "        terms.append(t)\n",
        "diction = Counter(terms)\n",
        "fontfile = requests.get(\"https://drive.google.com/uc?id=1QdaqR8Setf4HEulrIW79UEV_Lg_fuoWz&export=download\")\n",
        "with open('taipei_sans_tc_beta.ttf', 'wb') as f:\n",
        "  f.write(fontfile.content)\n",
        "wordcloud = WordCloud(font_path='taipei_sans_tc_beta.ttf') \n",
        "#mask = np.array(Image.open(\"heart.png\")) \n",
        "#wordcloud = WordCloud(background_color=\"white\",mask=mask,font_path='taipei_sans_tc_beta.ttf') \n",
        "wordcloud.generate_from_frequencies(frequencies=diction)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "wordcloud.to_file(\"bookCloud.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}