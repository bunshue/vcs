{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(123456)\n\ndef f(x):\n    return np.sin(x)\n\ndef sample(size):\n    max_v = 20\n    step = size/max_v\n    x = [x/step for x in range(size)]\n    y = [f(x)+np.random.uniform(-0.25,0.25) for x in x]\n    return np.array(x).reshape(-1,1), np.array(y).reshape(-1,1)\n\n\n# =============================================================================\n# HIGH BIAS - UNDERFIT\n# =============================================================================\nfrom sklearn.linear_model import LinearRegression\nx, y = sample(100)\n\n\nlr = LinearRegression()\nlr.fit(x, y)\npreds  =  lr.predict(x)\nplt.figure(figsize = (8, 8))\nplt.scatter(x, y, label='data')\nplt.plot(x, preds, color='orange', label='model')\nplt.title('Biased Model')\nplt.legend()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T08:04:16.534693Z","iopub.execute_input":"2021-12-16T08:04:16.535054Z","iopub.status.idle":"2021-12-16T08:04:18.154163Z","shell.execute_reply.started":"2021-12-16T08:04:16.534957Z","shell.execute_reply":"2021-12-16T08:04:18.153063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =============================================================================\n# HIGH VARIANCE - OVERFIT\n# =============================================================================\nfrom sklearn.tree import DecisionTreeRegressor\nx, y = sample(100)\n\ndt = DecisionTreeRegressor()\ndt.fit(x, y)\nplt.figure(figsize = (8, 8))\nplt.scatter(x, y, label='training data')\nx, y = sample(100)\npreds  =  dt.predict(x)\nplt.plot(x, preds, color='orange', label='model')\nplt.scatter(x, y, label='test data')\nplt.title('High Variance Model')\nplt.legend()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T08:04:18.156286Z","iopub.execute_input":"2021-12-16T08:04:18.156628Z","iopub.status.idle":"2021-12-16T08:04:18.581161Z","shell.execute_reply.started":"2021-12-16T08:04:18.156585Z","shell.execute_reply":"2021-12-16T08:04:18.580089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =============================================================================\n# TRADEOFF\n# =============================================================================\ndef bias(complexity):\n    return 100/complexity\n\ndef variance(complexity):\n    return np.exp(complexity/28)\n\nr = range(5, 100)\n\nvariance_ = np.array([variance(x) for x in r])\nbias_ =  np.array([bias(x) for x in r])\nsum_ = variance_ + bias_\nmins = np.argmin(sum_)\nmin_line = [mins for x in range(0, int(max(sum_)))]\n\n\nplt.figure(figsize = (8, 8))\nplt.plot(bias_, label=r'$bias^2$', linestyle='-')\nplt.plot(variance_, label='variance', linestyle=':')\nplt.plot(sum_, label='error', linestyle='-.')\nplt.plot(min_line, [x for x in range(0, int(max(sum_)))], linestyle='--')\nplt.title('Minimizing Error')\nplt.legend()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T08:04:18.582691Z","iopub.execute_input":"2021-12-16T08:04:18.583008Z","iopub.status.idle":"2021-12-16T08:04:19.055587Z","shell.execute_reply.started":"2021-12-16T08:04:18.582965Z","shell.execute_reply":"2021-12-16T08:04:19.054447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =============================================================================\n# BEST MODEL\n# =============================================================================\nfrom sklearn.tree import DecisionTreeRegressor\nx, y = sample(100)\n\nplt.figure(figsize = (8, 8))\nplt.scatter(x, y, label='training data')\n\npreds  =  f(x)\nplt.plot(x, preds, color='orange', label='model')\nplt.title('Perfect Model')\nplt.legend()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T08:04:19.057326Z","iopub.execute_input":"2021-12-16T08:04:19.057592Z","iopub.status.idle":"2021-12-16T08:04:19.360139Z","shell.execute_reply.started":"2021-12-16T08:04:19.057555Z","shell.execute_reply":"2021-12-16T08:04:19.359111Z"},"trusted":true},"execution_count":null,"outputs":[]}]}