{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 匯入函式庫\nfrom sklearn.base import BaseEstimator\nfrom copy import deepcopy\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nclass Stacking(BaseEstimator):\n\n    # 定義函式初始化\n    def __init__(self, learner_levels):\n        # 接收基學習器、超學習器、以及堆疊中每一層分別有多少學習器\n        # 複製學習器\n        self.level_sizes = []\n        self.learners = []\n        self.learner_levels = learner_levels\n        for learning_level in self.learner_levels:\n\n            self.level_sizes.append(len(learning_level))\n            level_learners = []\n            for learner in learning_level:\n                level_learners.append(deepcopy(learner))\n            self.learners.append(level_learners)\n\n    # fit 函式\n    # 用第i-1層的基學習器預測值來訓練第i層的基學習器\n    def fit(self, x, y):\n        # 第1層基學習器的訓練資料即為原始資料\n        meta_data = [x]\n        meta_targets = [y]\n        for i in range(len(self.learners)):\n            level_size = self.level_sizes[i]\n\n            # 建立第i層預測值的儲存空間\n            data_z = np.zeros((level_size, len(x)))\n            target_z = np.zeros(len(x))\n\n            # 取得第i層訓練資料集\n            train_x = meta_data[i]\n            train_y = meta_targets[i]\n\n            # 建立交叉驗證\n            KF = KFold(n_splits=3)\n            meta_index = 0\n            for train_indices, test_indices in KF.split(x):\n                for j in range(len(self.learners[i])):\n                    # 使用前K-1折訓練第j個基學習器\n                    learner = self.learners[i][j]\n                    learner.fit(train_x[train_indices], train_y[train_indices])\n                    # 使用第K折驗證第j個基學習器\n                    predictions = learner.predict(train_x[test_indices])\n                    # 儲存第K折第j個基學習器預測結果\n                    data_z[j][meta_index:meta_index+len(test_indices)] = predictions\n\n                # 儲存第i層基學習器的預測結果\n                # 作為第i+1層基學習器的訓練資料\n                target_z[meta_index:meta_index+len(test_indices)] = train_y[test_indices]\n                meta_index += len(test_indices)\n\n            # Add the data and targets to the meta data lists\n            data_z = data_z.transpose()\n            meta_data.append(data_z)\n            meta_targets.append(target_z)\n\n\n            # 使用完整的訓練資料來訓練基學習器\n            for learner in self.learners[i]:\n                    learner.fit(train_x, train_y)\n\n    # predict 函式\n    def predict(self, x):\n\n        # 儲存每一層的預測\n        meta_data = [x]\n        for i in range(len(self.learners)):\n            level_size = self.level_sizes[i]\n\n            data_z = np.zeros((level_size, len(x)))\n\n            test_x = meta_data[i]\n\n            KF = KFold(n_splits=3)\n            for train_indices, test_indices in KF.split(x):\n                for j in range(len(self.learners[i])):\n\n                    learner = self.learners[i][j]\n                    predictions = learner.predict(test_x)\n                    data_z[j] = predictions\n\n            # 儲存第i層基學習器的預測結果\n            # 作為第i+1層基學習器的輸入\n            data_z = data_z.transpose()\n            meta_data.append(data_z)\n\n        # 傳回預測結果\n        return meta_data[-1]\n\n    # predict_proba 函式\n    def predict_proba(self, x):\n\n        # 儲存每一層的預測\n        meta_data = [x]\n        for i in range(len(self.learners)-1):\n            level_size = self.level_sizes[i]\n\n            data_z = np.zeros((level_size, len(x)))\n\n            test_x = meta_data[i]\n\n            KF = KFold(n_splits=5)\n            for train_indices, test_indices in KF.split(x):\n                for j in range(len(self.learners[i])):\n\n                    learner = self.learners[i][j]\n                    predictions = learner.predict(test_x)\n                    data_z[j] = predictions\n\n            # 儲存第i層基學習器的預測結果\n            # 作為第i+1層基學習器的輸入\n            data_z = data_z.transpose()\n            meta_data.append(data_z)\n\n        # 傳回預測結果\n        learner = self.learners[-1][-1]\n        return learner.predict_proba(meta_data[-1])","metadata":{"execution":{"iopub.status.busy":"2021-12-23T07:43:37.900987Z","iopub.execute_input":"2021-12-23T07:43:37.901545Z","iopub.status.idle":"2021-12-23T07:43:38.918901Z","shell.execute_reply.started":"2021-12-23T07:43:37.901438Z","shell.execute_reply":"2021-12-23T07:43:38.917941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- 第 1 部分 ---\n# 載入函式庫與資料集\nimport numpy as np\nimport pandas as pd\n\n#from stacking_classifier import Stacking\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nnp.random.seed(123456)\ndata = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\n\n# 把資料分為 70% 訓練資料集與 30% 測試資料集\nx_train, x_test, y_train, y_test = train_test_split(data.drop('Class', axis=1).values, \n                                                    data.Class.values, \n                                                    test_size=0.3)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-23T07:43:38.921276Z","iopub.execute_input":"2021-12-23T07:43:38.921734Z","iopub.status.idle":"2021-12-23T07:43:43.121444Z","shell.execute_reply.started":"2021-12-23T07:43:38.921686Z","shell.execute_reply":"2021-12-23T07:43:43.1205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- 第 2 部分 ---\n# 進行集成\nbase_classifiers = [DecisionTreeClassifier(max_depth = 10),\n                    GaussianNB(),\n                    LogisticRegression(solver = 'liblinear')]\n\nmeta_learners = [LogisticRegression(solver = 'liblinear')]\n\nensemble = Stacking(learner_levels = [base_classifiers,\n                                      meta_learners])\nensemble.fit(x_train, y_train)\nprint('Stacking f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Stacking recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T07:43:43.122703Z","iopub.execute_input":"2021-12-23T07:43:43.122962Z","iopub.status.idle":"2021-12-23T07:44:31.070967Z","shell.execute_reply.started":"2021-12-23T07:43:43.122931Z","shell.execute_reply":"2021-12-23T07:44:31.06996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- 第 3 部分 ---\n# 篩選特徵\nthreshold = 0.1\ncorrelations = data.corr()['Class'].drop('Class')\nfs = list(correlations[(abs(correlations) > threshold)].index.values)\nfs.append('Class')\ndata = data[fs]\n\nx_train_f, x_test_f, y_train_f, y_test_f = train_test_split(data.drop('Class', axis=1).values, \n                                                            data.Class.values, \n                                                            test_size=0.3)\nensemble = Stacking(learner_levels = [base_classifiers,\n                                      meta_learners])\nensemble.fit(x_train_f, y_train_f)\nprint('Stacking f1', metrics.f1_score(y_test_f, ensemble.predict(x_test_f)))\nprint('Stacking recall', metrics.recall_score(y_test_f, ensemble.predict(x_test_f)))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T07:44:31.074059Z","iopub.execute_input":"2021-12-23T07:44:31.074569Z","iopub.status.idle":"2021-12-23T07:44:48.665458Z","shell.execute_reply.started":"2021-12-23T07:44:31.074523Z","shell.execute_reply":"2021-12-23T07:44:48.664239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- 第 4 部分 ---\n# 增加基學習器\nbase_classifiers = [DecisionTreeClassifier(max_depth = 10),\n                    DecisionTreeClassifier(max_depth = 7),\n                    DecisionTreeClassifier(max_depth = 6),\n                    GaussianNB(),\n                    LogisticRegression(solver = 'liblinear')]\n\nensemble = Stacking(learner_levels = [base_classifiers,\n                                      meta_learners])\nensemble.fit(x_train, y_train)\nprint('Stacking f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Stacking recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n\nensemble = Stacking(learner_levels = [base_classifiers,\n                                      meta_learners])\nensemble.fit(x_train_f, y_train_f)\nprint('Stacking f1', metrics.f1_score(y_test_f, ensemble.predict(x_test_f)))\nprint('Stacking recall', metrics.recall_score(y_test_f, ensemble.predict(x_test_f)))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T07:44:48.666968Z","iopub.execute_input":"2021-12-23T07:44:48.672616Z","iopub.status.idle":"2021-12-23T07:46:56.010794Z","shell.execute_reply.started":"2021-12-23T07:44:48.672539Z","shell.execute_reply":"2021-12-23T07:46:56.009804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- 第 5 部分 ---\n# 增加一層\nbase_classifiers = [DecisionTreeClassifier(max_depth = 10),\n                    DecisionTreeClassifier(max_depth = 7),\n                    DecisionTreeClassifier(max_depth = 6),\n                    GaussianNB(),\n                    LogisticRegression(solver = 'liblinear')]\n\nsecond_learners = [DecisionTreeClassifier(max_depth = 2),\n                   LinearSVC()]\n\nensemble = Stacking(learner_levels = [base_classifiers,\n                                      second_learners,\n                                      meta_learners])\nensemble.fit(x_train, y_train)\nprint('Stacking f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Stacking recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n\nensemble = Stacking(learner_levels = [base_classifiers,\n                                      second_learners,\n                                      meta_learners])\nensemble.fit(x_train_f, y_train_f)\nprint('Stacking f1', metrics.f1_score(y_test_f, ensemble.predict(x_test_f)))\nprint('Stacking recall', metrics.recall_score(y_test_f, ensemble.predict(x_test_f)))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T07:46:56.012356Z","iopub.execute_input":"2021-12-23T07:46:56.01296Z","iopub.status.idle":"2021-12-23T07:49:20.889Z","shell.execute_reply.started":"2021-12-23T07:46:56.01291Z","shell.execute_reply":"2021-12-23T07:49:20.888074Z"},"trusted":true},"execution_count":null,"outputs":[]}]}