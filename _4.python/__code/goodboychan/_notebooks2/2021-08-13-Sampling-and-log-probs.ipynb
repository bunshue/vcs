{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and log probs\n",
    "\n",
    "> In this post, we will take a look at how broadcasting rules can be applied to the prob and log_prob methods of a distribution method. This is the summary of lecture \"Probabilistic Deep Learning with Tensorflow 2\" from Imperial College London.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Coursera, Tensorflow_probability, ICL]\n",
    "- image: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.5.0\n",
      "Tensorflow Probability Version:  0.13.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Tensorflow Probability Version: \", tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build simple exponential distribution that have 2 by 3 batch shaped. And it is univariate distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Exponential 'Exponential' batch_shape=[2, 3] event_shape=[] dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = tfd.Exponential(rate=[[1., 1.5, 0.8], [0.3, 0.4, 1.8]])\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert it Multivariate distribution with `Independent` Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'IndependentExponential' batch_shape=[2] event_shape=[3] dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp = tfd.Independent(exp)\n",
    "ind_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't use `reinterpreted_batch_ndims` keyword, so it will convert all but the first batch dimension into the event_shape, which is the default operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 3), dtype=float32, numpy=\n",
       "array([[[1.1482651 , 0.5365022 , 0.27021408],\n",
       "        [5.6145535 , 5.1724033 , 0.75714827]],\n",
       "\n",
       "       [[1.6911694 , 0.66507334, 1.3505032 ],\n",
       "        [3.0767057 , 1.6154473 , 0.00633118]],\n",
       "\n",
       "       [[0.9210652 , 1.2446263 , 2.043678  ],\n",
       "        [1.4890236 , 1.2732824 , 0.45793444]],\n",
       "\n",
       "       [[1.8119018 , 0.49963415, 0.09335292],\n",
       "        [2.0591164 , 3.0031517 , 0.0490962 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh that when we convert the univariate distribution to independent distribution and sample it, its shape will be (sample_size, batch_size, event_shape).\n",
    "\n",
    "Let's take a look another distribution, which use `reinterpreted_batch_ndims` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = [\n",
    "    [[[1., 1.5, 0.8], [0.3, 0.4, 1.8]]],\n",
    "    [[[0.2, 0.4, 1.4], [0.4, 1.1, 0.9]]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(rates).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Exponential 'Exponential' batch_shape=[2, 1, 2, 3] event_shape=[] dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = tfd.Exponential(rate=rates)\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'IndependentExponential' batch_shape=[2, 1] event_shape=[2, 3] dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp = tfd.Independent(exp, reinterpreted_batch_ndims=2)\n",
    "ind_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a rank 2 batch_shape and rank 2 event_shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 2, 1, 2, 3), dtype=float32, numpy=\n",
       "array([[[[[[8.52314979e-02, 2.20484066e+00, 1.24831609e-01],\n",
       "           [5.72363853e+00, 3.56653333e+00, 1.73619881e-01]]],\n",
       "\n",
       "\n",
       "         [[[4.51495945e-01, 5.31205177e+00, 9.25723433e-01],\n",
       "           [6.85539341e+00, 1.23257709e+00, 8.64088893e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.35191846e+00, 2.72666621e+00, 9.93554652e-01],\n",
       "           [2.71719074e+00, 4.16195679e+00, 7.69947097e-02]]],\n",
       "\n",
       "\n",
       "         [[[2.47778010e+00, 1.24962020e+00, 1.23250462e-01],\n",
       "           [2.38104010e+00, 1.49862719e+00, 5.34713209e-01]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "       [[[[[1.12703860e+00, 1.69036031e-01, 1.75236121e-01],\n",
       "           [4.30009747e+00, 7.73049667e-02, 9.20982718e-01]]],\n",
       "\n",
       "\n",
       "         [[[2.07261410e+01, 4.24093342e+00, 9.07527208e-02],\n",
       "           [4.11562204e+00, 2.51152706e+00, 3.07851344e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.05817151e+00, 5.20415246e-01, 1.24326766e+00],\n",
       "           [6.05947375e-01, 9.40155125e+00, 3.41252506e-01]]],\n",
       "\n",
       "\n",
       "         [[[6.36713266e+00, 3.30024123e+00, 3.68020654e-01],\n",
       "           [5.06028198e-02, 6.59128129e-02, 1.67161095e+00]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "       [[[[[9.82080624e-02, 2.37932968e+00, 1.01604521e+00],\n",
       "           [5.45345068e+00, 3.30391824e-01, 2.25491732e-01]]],\n",
       "\n",
       "\n",
       "         [[[5.43883753e+00, 8.75549436e-01, 6.05356740e-03],\n",
       "           [2.01855755e+00, 9.21276867e-01, 2.28063658e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.91371128e-01, 1.38863182e+00, 2.60386395e+00],\n",
       "           [1.99791002e+00, 3.85638475e+00, 6.53963163e-02]]],\n",
       "\n",
       "\n",
       "         [[[2.21251488e+01, 1.21040082e+00, 1.30126461e-01],\n",
       "           [2.82561398e+00, 9.84178245e-01, 1.09785414e+00]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "       [[[[[1.06780671e-01, 8.60901594e-01, 7.70675957e-01],\n",
       "           [6.49312556e-01, 7.30921552e-02, 3.12643439e-01]]],\n",
       "\n",
       "\n",
       "         [[[8.84963870e-01, 1.66497517e+00, 4.34715062e-01],\n",
       "           [1.08664826e-01, 9.62030329e-03, 1.07902026e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.40111685e-01, 3.47986722e+00, 3.60813737e-01],\n",
       "           [1.61084032e+00, 9.10717964e-01, 1.13252953e-01]]],\n",
       "\n",
       "\n",
       "         [[[5.73364735e-01, 2.30145860e+00, 2.11053286e-02],\n",
       "           [3.76549268e+00, 2.19317222e+00, 2.94480515e+00]]]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.sample([4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we sample rank 2 data, its output tensor has rank 6 tensor, which has same manner of generating single sample. In details, it has [4, 2] sample size, [2, 1] batchs, and [2, 3] events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-4.2501583],\n",
       "       [-5.3156004]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.log_prob(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want some log probability of single data, its output is broadcasting and has batch_size shape. As a general ruls, the log prob (and prob) method broadcast its input against the batch and event_shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-4.770158],\n",
       "       [-5.8856  ]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.log_prob([[0.3, 0.5, 0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what if we want to gather log probability of complex tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform((5, 1, 1, 2, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2, 1), dtype=float32, numpy=\n",
       "array([[[-2.1549933],\n",
       "        [-3.6172578]],\n",
       "\n",
       "       [[-4.4256163],\n",
       "        [-5.612159 ]],\n",
       "\n",
       "       [[-6.6976185],\n",
       "        [-7.158457 ]],\n",
       "\n",
       "       [[-4.3900895],\n",
       "        [-5.1580105]],\n",
       "\n",
       "       [[-2.5316038],\n",
       "        [-4.051901 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_exp.log_prob(tf.random.uniform((5, 1, 1, 2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Multivariate Distribution\n",
    "loc = [[0.5, 1], [0.1, 0], [0, 0.2]]\n",
    "scale_diag = [[2., 3], [1., 3], [4., 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.MultivariateNormalDiag 'MultivariateNormalDiag' batch_shape=[3] event_shape=[2] dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_distributions = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\n",
    "normal_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3, 2), dtype=float32, numpy=\n",
       "array([[[ 0.30364102,  2.3371062 ],\n",
       "        [ 1.118043  , -0.4802311 ],\n",
       "        [-5.099995  , -4.291627  ]],\n",
       "\n",
       "       [[-0.8409153 , -2.1168094 ],\n",
       "        [ 1.4500505 , -2.828692  ],\n",
       "        [ 1.3774487 , -0.36408925]],\n",
       "\n",
       "       [[-1.7672858 , -3.1279678 ],\n",
       "        [ 2.3426754 , -1.0637026 ],\n",
       "        [-0.3274685 ,  2.4647486 ]],\n",
       "\n",
       "       [[-0.42931175, -0.46005905],\n",
       "        [ 1.943181  , -4.173031  ],\n",
       "        [ 4.5748634 , -1.8085063 ]],\n",
       "\n",
       "       [[ 2.2362173 ,  3.2581265 ],\n",
       "        [-0.18285483, -3.6155434 ],\n",
       "        [-1.196823  ,  8.819077  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "normal_distributions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Normal batched distribution\n",
    "# We are broadcasting batch shapes of `loc` and `scale_diag`\n",
    "# against each other\n",
    "\n",
    "loc = [[[0.3, 1.5, 1.], [0.2, 0.4, 2.8]],\n",
    "       [[2., 2.3, 8], [1.4, 1., 1.3]]]\n",
    "scale_diag = [0.4, 1., 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.MultivariateNormalDiag 'MultivariateNormalDiag' batch_shape=[2, 2] event_shape=[3] dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_distributions = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\n",
    "normal_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'IndependentMultivariateNormalDiag' batch_shape=[2] event_shape=[2, 3] dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use independent to move part of the batch shape\n",
    "\n",
    "ind_normal_distribution = tfd.Independent(normal_distributions, \n",
    "                                          reinterpreted_batch_ndims=1)\n",
    "ind_normal_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 2, 2, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw some samples\n",
    "samples = ind_normal_distribution.sample(5)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-11.195248, -76.89377 ], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [B, E] shaped input\n",
    "inp = tf.random.uniform((2, 2, 3))\n",
    "ind_normal_distribution.log_prob(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-10.743417, -75.82509 ], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [E] shaped input (broadcasting over batch size)\n",
    "inp = tf.random.uniform((2, 3))\n",
    "ind_normal_distribution.log_prob(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2), dtype=float32, numpy=\n",
       "array([[ -9.456959, -74.69543 ],\n",
       "       [-10.222152, -70.511894],\n",
       "       [ -9.343542, -79.319824],\n",
       "       [ -9.667246, -75.2012  ],\n",
       "       [-10.971153, -62.26688 ],\n",
       "       [ -9.592494, -68.68516 ],\n",
       "       [ -9.568048, -74.8657  ],\n",
       "       [-12.416011, -68.85194 ],\n",
       "       [ -8.812517, -66.78156 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [S, B, E] shaped input\n",
    "inp = tf.random.uniform((9, 2, 2, 3))\n",
    "ind_normal_distribution.log_prob(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Nested component \"MultivariateNormalDiag_scale_matvec_linear_operator\" in composition \"MultivariateNormalDiag_chain_of_MultivariateNormalDiag_shift_of_MultivariateNormalDiag_scale_matvec_linear_operator\" operates on inputs with increased degrees of freedom. This may result in an incorrect log_det_jacobian.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[-11.335012, -81.39036 ],\n",
       "       [ -9.282346, -63.680298],\n",
       "       [ -9.340945, -68.03868 ],\n",
       "       [-12.372825, -66.20646 ],\n",
       "       [-10.076466, -80.59617 ]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [S, b, e] shaped input, where [b, e] is broadcastable over [B, E]\n",
    "inp = tf.random.uniform((5, 1, 2, 1))\n",
    "ind_normal_distribution.log_prob(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes example\n",
    "\n",
    "Let's now use what we have learned and continue the naive bayes classifier we were building last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function get_data which:\n",
    "#   1) Fetches the 20 newsgroup dataset\n",
    "#   2) Performs a word count on the articles and binarizes the result\n",
    "#   3) Returns the data as a numpy matrix with the labels\n",
    "\n",
    "def get_data(categories):\n",
    "    \n",
    "    newsgroups_train_data = fetch_20newsgroups(data_home='./dataset/20_Newsgroup_Data/',\n",
    "                                               subset='train', categories=categories)\n",
    "    newsgroups_test_data = fetch_20newsgroups(data_home='./dataset/20_Newsgroup_Data/',\n",
    "                                              subset='test', categories=categories)\n",
    "\n",
    "    n_documents = len(newsgroups_train_data['data'])\n",
    "    count_vectorizer = CountVectorizer(input='content', binary=True,max_df=0.25, min_df=1.01/n_documents)\n",
    "    \n",
    "    train_binary_bag_of_words = count_vectorizer.fit_transform(newsgroups_train_data['data'])\n",
    "    test_binary_bag_of_words = count_vectorizer.transform(newsgroups_test_data['data']) \n",
    "\n",
    "    return (train_binary_bag_of_words.todense(), newsgroups_train_data['target']),  (test_binary_bag_of_words.todense(), newsgroups_test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to conduct Laplace smoothing. This adds a base level of probability for a given feature\n",
    "# to occur in every class.\n",
    "\n",
    "def laplace_smoothing(labels, binary_data, n_classes):\n",
    "    # Compute the parameter estimates (adjusted fraction of documents in class that contain word)\n",
    "    n_words = binary_data.shape[1]\n",
    "    alpha = 1 # parameters for Laplace smoothing\n",
    "    theta = np.zeros([n_classes, n_words]) # stores parameter values - prob. word given class\n",
    "    for c_k in range(n_classes): # 0, 1, ..., 19\n",
    "        class_mask = (labels == c_k)\n",
    "        N = class_mask.sum() # number of articles in class\n",
    "        theta[c_k, :] = (binary_data[class_mask, :].sum(axis=0) + alpha)/(N + alpha*2)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a subset of the 20 newsgroup dataset\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = get_data(categories=categories)\n",
    "smoothed_counts = laplace_smoothing(labels=train_labels, binary_data=train_data, n_classes=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now make our NB classifier we need to build three functions:\n",
    "* Compute the class priors\n",
    "* Build our class conditional distributions\n",
    "* Put it all together and classify our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which computes the prior probability of every class based on frequency of occurence in \n",
    "# the dataset\n",
    "\n",
    "def class_priors(n_classes, labels):\n",
    "    counts = np.zeros(n_classes)\n",
    "    for c_k in range(n_classes):\n",
    "        counts[c_k] = np.sum(np.where(labels==c_k, 1, 0))\n",
    "    priors = counts / np.sum(counts)\n",
    "    print('The class priors are {}'.format(priors))\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class priors are [0.2359882  0.28711898 0.29154376 0.18534907]\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "\n",
    "priors = class_priors(n_classes=len(categories), labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'IndependentBernoulli' batch_shape=[4] event_shape=[17495] dtype=int32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will do a function that given the feature occurence counts returns a Bernoulli distribution of \n",
    "# batch_shape=number of classes and event_shape=number of features.\n",
    "\n",
    "def make_distribution(probs):\n",
    "    batch_of_bernoulli = tfd.Bernoulli(probs=probs)\n",
    "    dist = tfd.Independent(batch_of_bernoulli, reinterpreted_batch_ndims=1)\n",
    "    return dist\n",
    "\n",
    "tf_dist = make_distribution(smoothed_counts)\n",
    "tf_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final function predict_sample which given the distribution, a test sample, and the class priors:\n",
    "#   1) Computes the class conditional probabilities given the sample\n",
    "#   2) Forms the joint likelihood\n",
    "#   3) Normalises the joint likelihood and returns the log prob\n",
    "\n",
    "def predict_sample(dist, sample, priors):\n",
    "    cond_probs = dist.log_prob(sample)\n",
    "    joint_likelihood = tf.add(np.log(priors), cond_probs)\n",
    "    norm_factor = tf.math.reduce_logsumexp(joint_likelihood, axis=-1, keepdims=True)\n",
    "    log_prob = joint_likelihood - norm_factor\n",
    "    \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
       "array([-6.1736343e+01, -1.5258789e-05, -1.1620026e+01, -6.3327866e+01],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting one example from our test data\n",
    "\n",
    "log_probs = predict_sample(tf_dist, test_data[0], priors)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7848499112849504\n"
     ]
    }
   ],
   "source": [
    "# Loop over our test data and classify\n",
    "\n",
    "probabilities = []\n",
    "\n",
    "for sample, label in zip(test_data, test_labels):\n",
    "    probabilities.append(tf.exp(predict_sample(tf_dist, sample, priors)))\n",
    "\n",
    "probabilities = np.asarray(probabilities)\n",
    "predict_class = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "print('F1 score: ', f1_score(test_labels, predict_class, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 from sklearn:  0.7848499112849504\n"
     ]
    }
   ],
   "source": [
    "# Make a Bernoulli Naive Bayes classifier using sklearn with the same level of alpha smoothing\n",
    "\n",
    "clf = BernoulliNB(alpha=1)\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "print('F1 from sklearn: ', f1_score(test_labels, pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
