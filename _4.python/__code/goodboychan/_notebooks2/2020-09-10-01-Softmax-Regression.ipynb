{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "> In this post, it will cover the basic concept of softmax regression, also known as multinomial classification. And it will explain what the hypothesis and cost function, and how to solve it with gradient descent as we saw previously. Also we will try to implement it with tensorflow 2.x\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Tensorflow, Machine_Learning]\n",
    "- image: images/sigmoid.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rc('font', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Previously, we covered logistic regression, which can handle the classification task, especially on binary classification. Basic concept of logistic regression is the same as the one in linear regression. For the simplicity, we omit the bias term.\n",
    "\n",
    "$$ H_{\\theta}(X) = \\theta^TX $$\n",
    "\n",
    "But we need to classify the data, not predict the value. So we tried to predict the probability of whether it is True or False, and decided the label based on decision boundary. So we introduced new type of hypothesis, the sigmoid (or logistic) function.\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Its output range is from 0 to 1. So it is reasonable choice to calculate the probability. All we need to do it calculating the original hypothesis ($H_{\\theta}(X)$) and use it in sigmoid function as an argument. ($g(H_{\\theta}(X)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.arange(-10, 10, 0.1)\n",
    "y = 1 / (1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(z, y, color='red');\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Sigmoid function $g(z)$');\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the way how we can handle the binary classification. Then how can we apply it multinomial classification that have more than two labels to classify?\n",
    "\n",
    "## Multinomial Classification\n",
    "Actually, Multinomial classification is the extended version of binary classification. Suppose we have three labels, $A, B, C$, And we don't know the trick of multinomial classification. how can we classfity them?\n",
    "\n",
    "The simplest method is divide and conquer. We can divide the big problem into three small problem like,\n",
    "\n",
    "- Whether it is $A$ or not.\n",
    "- Whether it is $B$ or not.\n",
    "- Whether it is $C$ or not.\n",
    "\n",
    "The hypothesis of binary classification is to predict the probability. So we can combine the three hypothesis of binary classification.\n",
    "\n",
    "$$ \\begin{aligned} H_{\\theta}(X) = WX &= \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\\\ &= \\begin{bmatrix} w_{11} x_1 + w_{12} x_2 + w_{13} x_3 \\\\ w_{21} x_1 + w_{22} x_2 + w_{23} x_3 \\\\ w_{31} x_1 + w_{32} x_2 + w_{33} x_3 \\end{bmatrix} = \\begin{bmatrix} \\bar{y_{A}} \\\\ \\bar{y_{B}} \\\\ \\bar{y_{C}} \\end{bmatrix} \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's (3,1) matrix. And the shape of output applying sigmoid function will also (3,1) matrix. What does it mean of each row? If we apply the sigmoid function on each row, It'll be the probability of whether it is True or False. So the first element is the probability of whether it is $A$ or not, and so on. All we have to do is applying sigmoid function on each row, right?\n",
    "\n",
    "## Softmax function\n",
    "\n",
    "It will work, eventually. But there is more effective way to calculate the probability. If we apply the sigmoid on each value, let's say the probability of $A$ is 0.55, and $B$ is 0.66%, and $C$ is 0.44. How can we classify it. It has high probability of $B$, but we cannot ignore the probability of $A$ and $C$. So this kind of result is hard to interpret what label it is.\n",
    "\n",
    "There is the way to calculate the probability of all, that sums up to 1. He we introduce the new additional function the **softmax function**.\n",
    "\n",
    "$$ \\sigma(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^{K} e^{y_j}}  $$\n",
    "\n",
    "The rule of softmax function is to convert the score (the output of matrix multiplication) to probability. And Sum of all probability is 1. All we need to do is find the maximum probability of each row, define its labels. Usually, it can be calculated with argmax function, that is to find the argument to make maximum of its value.\n",
    "\n",
    "## Cost function of Multinomial classification\n",
    "\n",
    "We can bring the cost function of binary classification here, the cross-entropy.\n",
    "\n",
    "$$ \\text{C.E.} = -y\\log(p) - (1-y)\\log(1-p) $$.\n",
    "\n",
    "In multinomial classification, we can modify the cross entropy function.\n",
    "\n",
    "$$ \\text{C.E.} = -\\sum_{i} y_i \\log (\\bar{y_i}) $$\n",
    "\n",
    " The rule of cost function is measure the score of classification. So if the model incorrectly classify the label, cost function must return the low cost, and cost function must give the high cost if it is correctly classified. Even if it is in the case of multinomial classification, it can apply it with same manner. \n",
    " \n",
    " As a result, we define the cost function. And we can apply gradient descent to find weight vector to make the cost minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implment with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to implment softmax regression with simple dataset. First, define the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array(\n",
    "    [[1, 2, 1, 1],\n",
    "     [2, 1, 3, 2],\n",
    "     [3, 1, 3, 4],\n",
    "     [4, 1, 5, 5],\n",
    "     [1, 7, 5, 5],\n",
    "     [1, 2, 5, 6],\n",
    "     [1, 6, 6, 6],\n",
    "     [1, 7, 7, 7]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array(\n",
    "    [[0, 0, 1],\n",
    "     [0, 0, 1],\n",
    "     [0, 0, 1],\n",
    "     [0, 1, 0],\n",
    "     [0, 1, 0],\n",
    "     [0, 1, 0],\n",
    "     [1, 0, 0],\n",
    "     [1, 0, 0]], dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three classes, but for the simplicity, `y_data` is modified with one-hot encoding.\n",
    "And we need to initialize Weight vector $W$ and bias $b$. Usually, it initializes with random value from normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-0.03031273, -1.060833  , -1.20091   ],\n",
      "       [-0.5629968 , -0.50404245,  1.4590237 ],\n",
      "       [-1.2940224 , -0.48054248, -0.13930833],\n",
      "       [-0.31837052, -0.72089845, -1.0884795 ]], dtype=float32)> <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.25864193,  1.4588804 , -1.203377  ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random.normal((x_data.shape[1], y_data.shape[1])), name='weight')\n",
    "b = tf.Variable(tf.random.normal((y_data.shape[1], )), name='bias')\n",
    "\n",
    "print(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define the softmax function with tensorflow. Of course, you can implement it manually with numpy, but tensorflow also offers softmax fuction as an API.\n",
    "```python\n",
    "h = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it with sample data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.11066143 0.22252838 0.66681015]], shape=(1, 3), dtype=float32)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(softmax([x_data[0]]))\n",
    "print(softmax([x_data[0]]).numpy().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define the cost function and gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def loss_fn(X, Y):\n",
    "    logits = softmax(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    return cost_mean\n",
    "\n",
    "def gradient(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(X, Y)\n",
    "    grads = tape.gradient(loss, [W, b])\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.2738\n",
      "Epoch: 500, Loss: 0.7892\n",
      "Epoch: 1000, Loss: 0.6457\n",
      "Epoch: 1500, Loss: 0.5712\n",
      "Epoch: 2000, Loss: 0.5234\n",
      "Epoch: 2500, Loss: 0.4884\n",
      "Epoch: 3000, Loss: 0.4606\n",
      "Epoch: 3500, Loss: 0.4373\n",
      "Epoch: 4000, Loss: 0.4171\n",
      "Epoch: 4500, Loss: 0.3993\n"
     ]
    }
   ],
   "source": [
    "# Optimizer (Stochastic Gradient Descent)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for e in range(5000):\n",
    "    grads = gradient(x_data, y_data)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
    "    if e % 500 == 0:\n",
    "        print('Epoch: {}, Loss: {:.4f}'.format(e, loss_fn(x_data, y_data).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the model performance, we need to check validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 2 2 1 0 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b = softmax(x_data)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted result with actual data, we can find that one data is mis-classified, but most of data are correctly classified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression for animal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we apply softmax regressino animal classification. The dataset is from [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/Zoo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antelope</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bass</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boar</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  \\\n",
       "0  aardvark   1   0   0   1   0   0   1   1   1   1   0   0   4   0   0   1   \n",
       "1  antelope   1   0   0   1   0   0   0   1   1   1   0   0   4   1   0   1   \n",
       "2      bass   0   0   1   0   0   1   1   1   1   0   0   1   0   1   0   0   \n",
       "3      bear   1   0   0   1   0   0   1   1   1   1   0   0   4   0   0   1   \n",
       "4      boar   1   0   0   1   0   0   1   1   1   1   0   0   4   1   0   1   \n",
       "\n",
       "   17  \n",
       "0   1  \n",
       "1   1  \n",
       "2   4  \n",
       "3   1  \n",
       "4   1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset/zoo.data', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in our case, we don't need the name of each animals. We want to classify animal's type, regardless of name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17\n",
       "0   1   0   0   1   0   0   1   1   1   1   0   0   4   0   0   1   1\n",
       "1   1   0   0   1   0   0   0   1   1   1   0   0   4   1   0   1   1\n",
       "2   0   0   1   0   0   1   1   1   1   0   0   1   0   1   0   0   4\n",
       "3   1   0   0   1   0   0   1   1   1   1   0   0   4   0   0   1   1\n",
       "4   1   0   0   1   0   0   1   1   1   1   0   0   4   1   0   1   1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(0, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we look at the label of data (the 17th column), we can find that the range is from 1 to 7. we need to shift it by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   1       101 non-null    int64\n",
      " 1   2       101 non-null    int64\n",
      " 2   3       101 non-null    int64\n",
      " 3   4       101 non-null    int64\n",
      " 4   5       101 non-null    int64\n",
      " 5   6       101 non-null    int64\n",
      " 6   7       101 non-null    int64\n",
      " 7   8       101 non-null    int64\n",
      " 8   9       101 non-null    int64\n",
      " 9   10      101 non-null    int64\n",
      " 10  11      101 non-null    int64\n",
      " 11  12      101 non-null    int64\n",
      " 12  13      101 non-null    int64\n",
      " 13  14      101 non-null    int64\n",
      " 14  15      101 non-null    int64\n",
      " 15  16      101 non-null    int64\n",
      " 16  17      101 non-null    int64\n",
      "dtypes: int64(17)\n",
      "memory usage: 13.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>2.841584</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>2.831683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496921</td>\n",
       "      <td>0.400495</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>0.427750</td>\n",
       "      <td>0.481335</td>\n",
       "      <td>0.499505</td>\n",
       "      <td>0.491512</td>\n",
       "      <td>0.384605</td>\n",
       "      <td>0.407844</td>\n",
       "      <td>0.271410</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>2.033385</td>\n",
       "      <td>0.439397</td>\n",
       "      <td>0.336552</td>\n",
       "      <td>0.498314</td>\n",
       "      <td>2.102709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1           2           3           4           5           6   \\\n",
       "count  101.000000  101.000000  101.000000  101.000000  101.000000  101.000000   \n",
       "mean     0.425743    0.198020    0.584158    0.405941    0.237624    0.356436   \n",
       "std      0.496921    0.400495    0.495325    0.493522    0.427750    0.481335   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    1.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    0.000000    1.000000    1.000000    0.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               7           8           9           10          11          12  \\\n",
       "count  101.000000  101.000000  101.000000  101.000000  101.000000  101.000000   \n",
       "mean     0.554455    0.603960    0.821782    0.792079    0.079208    0.168317   \n",
       "std      0.499505    0.491512    0.384605    0.407844    0.271410    0.376013   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "50%      1.000000    1.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               13          14          15          16          17  \n",
       "count  101.000000  101.000000  101.000000  101.000000  101.000000  \n",
       "mean     2.841584    0.742574    0.128713    0.435644    2.831683  \n",
       "std      2.033385    0.439397    0.336552    0.498314    2.102709  \n",
       "min      0.000000    0.000000    0.000000    0.000000    1.000000  \n",
       "25%      2.000000    0.000000    0.000000    0.000000    1.000000  \n",
       "50%      4.000000    1.000000    0.000000    0.000000    2.000000  \n",
       "75%      4.000000    1.000000    0.000000    1.000000    4.000000  \n",
       "max      8.000000    1.000000    1.000000    1.000000    7.000000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>2.841584</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>1.831683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496921</td>\n",
       "      <td>0.400495</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>0.427750</td>\n",
       "      <td>0.481335</td>\n",
       "      <td>0.499505</td>\n",
       "      <td>0.491512</td>\n",
       "      <td>0.384605</td>\n",
       "      <td>0.407844</td>\n",
       "      <td>0.271410</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>2.033385</td>\n",
       "      <td>0.439397</td>\n",
       "      <td>0.336552</td>\n",
       "      <td>0.498314</td>\n",
       "      <td>2.102709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1           2           3           4           5           6   \\\n",
       "count  101.000000  101.000000  101.000000  101.000000  101.000000  101.000000   \n",
       "mean     0.425743    0.198020    0.584158    0.405941    0.237624    0.356436   \n",
       "std      0.496921    0.400495    0.495325    0.493522    0.427750    0.481335   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    1.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    0.000000    1.000000    1.000000    0.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               7           8           9           10          11          12  \\\n",
       "count  101.000000  101.000000  101.000000  101.000000  101.000000  101.000000   \n",
       "mean     0.554455    0.603960    0.821782    0.792079    0.079208    0.168317   \n",
       "std      0.499505    0.491512    0.384605    0.407844    0.271410    0.376013   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "50%      1.000000    1.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               13          14          15          16          17  \n",
       "count  101.000000  101.000000  101.000000  101.000000  101.000000  \n",
       "mean     2.841584    0.742574    0.128713    0.435644    1.831683  \n",
       "std      2.033385    0.439397    0.336552    0.498314    2.102709  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      2.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "50%      4.000000    1.000000    0.000000    0.000000    1.000000  \n",
       "75%      4.000000    1.000000    0.000000    1.000000    3.000000  \n",
       "max      8.000000    1.000000    1.000000    1.000000    6.000000  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[17] = df[17] - 1\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as you can see from the contents, we need to convert the numeric data with one-hot encoding. Tensorflow offers some APIs for one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0]\n",
      " [0]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "# Separate dataset and labels\n",
    "X = df.iloc[:, :-1].to_numpy(dtype=np.float32)\n",
    "y = df.iloc[:, [-1]].to_numpy()\n",
    "\n",
    "# Make y data with onehot encoding\n",
    "Y_one_hot = tf.one_hot(list(y), depth=7)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, 7])\n",
    "print(Y_one_hot[:3].numpy())\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: When we try to use tf.one_hot, keep in mind that, If the input indices is rank $N$, the output will have rank $N+1$. The new axis is created at dimension axis. (default case: the new axis is appended at the end) ([link]( https://www.tensorflow.org/api_docs/python/tf/one_hot )). So you must reshape it to maintain the shape of matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tasks we have done is a part of data preprocessing. So now it is the time to building dataset, and implement the learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and bias initialization\n",
    "W = tf.Variable(tf.random.normal([X.shape[1], 7]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([7, ]), name='bias')\n",
    "variables = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we built the softmax function and cost function(cross entropy) manually, But the Tensorflow also made a fancy API to do it once. As we define the logit function ($H_{\\theta}(X)$) and labels, we can use it for tensorflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit function\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "# Softmax function \n",
    "def softmax(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "# Loss function for cross entropy\n",
    "def loss_fn(X, y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    return tf.reduce_mean(cost_i)\n",
    "\n",
    "# Calculate gradient\n",
    "def grad_fn(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(X, y)\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    return grads\n",
    "\n",
    "# Predict function for validation\n",
    "def prediction(X, y):\n",
    "    pred = tf.argmax(softmax(X), 1)\n",
    "    correct = tf.equal(pred, tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.8638, Acc: 0.0990\n",
      "Epoch: 100, Loss: 0.7650, Acc: 0.8218\n",
      "Epoch: 200, Loss: 0.5063, Acc: 0.8713\n",
      "Epoch: 300, Loss: 0.3788, Acc: 0.9109\n",
      "Epoch: 400, Loss: 0.2988, Acc: 0.9208\n",
      "Epoch: 500, Loss: 0.2443, Acc: 0.9406\n",
      "Epoch: 600, Loss: 0.2055, Acc: 0.9505\n",
      "Epoch: 700, Loss: 0.1766, Acc: 0.9505\n",
      "Epoch: 800, Loss: 0.1543, Acc: 0.9505\n",
      "Epoch: 900, Loss: 0.1366, Acc: 0.9604\n",
      "Epoch: 1000, Loss: 0.1224, Acc: 0.9802\n",
      "Epoch: 1100, Loss: 0.1107, Acc: 0.9901\n",
      "Epoch: 1200, Loss: 0.1009, Acc: 1.0000\n",
      "Epoch: 1300, Loss: 0.0927, Acc: 1.0000\n",
      "Epoch: 1400, Loss: 0.0857, Acc: 1.0000\n",
      "Epoch: 1500, Loss: 0.0797, Acc: 1.0000\n",
      "Epoch: 1600, Loss: 0.0745, Acc: 1.0000\n",
      "Epoch: 1700, Loss: 0.0699, Acc: 1.0000\n",
      "Epoch: 1800, Loss: 0.0659, Acc: 1.0000\n",
      "Epoch: 1900, Loss: 0.0623, Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# You can use another optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "for e in range(2000):\n",
    "    grads = grad_fn(X, Y_one_hot)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        print('Epoch: {}, Loss: {:.4f}, Acc: {:.4f}'.format(e, \n",
    "                                                            loss_fn(X, Y_one_hot).numpy(), \n",
    "                                                            prediction(X, Y_one_hot).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this post, we covered the softmax function for multinomial classification. Actually, Multinomial classification is extended version of binary classification, so we can apply almost same approach of logistic regression here. But, to easily interpret the result, we substitute sigmoid function to softmax, and get the probability of each labels. We can also implement it with tensorflow 2.x."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
