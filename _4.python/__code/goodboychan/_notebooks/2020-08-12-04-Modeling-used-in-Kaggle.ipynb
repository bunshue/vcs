{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "> Time to bring everything together and build some models! In this last chapter, you will build a base model before tuning some hyperparameters and improving your results with ensembles. You will then get some final tips and tricks to help you compete more efficiently. This is the Summary of lecture \"Winning a Kaggle Competition in Python\", via datacamp.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Datacamp, Kaggle, Machine_Learning]\n",
    "- image: images/modeling_stage.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "- Modeling stage\n",
    "![modeling](image/modeling_stage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate validation score\n",
    "Throughout this chapter, you will work with New York City Taxi competition data. The problem is to predict the fare amount for a taxi ride in New York City. The competition metric is the root mean squared error.\n",
    "\n",
    "The first goal is to evaluate the Baseline model on the validation data. You will replicate the simplest Baseline based on the mean of `\"fare_amount\"`. Recall that as a validation strategy we used a 30% holdout split with `validation_train` as train and `validation_test` as holdout DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv('./dataset/taxi_train_chapter_4.csv')\n",
    "test = pd.read_csv('./dataset/taxi_test_chapter_4.csv')\n",
    "\n",
    "validation_train, validation_test = train_test_split(train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE for Baseline I model: 9.431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Calculate the mean fare_amount on the validation_train data\n",
    "naive_prediction = np.mean(validation_train['fare_amount'])\n",
    "\n",
    "# Assign naive prediction to all the holdout observations\n",
    "validation_test = validation_test.copy()\n",
    "validation_test['pred'] = naive_prediction\n",
    "\n",
    "# Measure the local RMSE\n",
    "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
    "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline based on the date\n",
    "We've already built 3 different baseline models. To get more practice, let's build a couple more. The first model is based on the grouping variables. It's clear that the ride fare could depend on the part of the day. For example, prices could be higher during the rush hours.\n",
    "\n",
    "Your goal is to build a baseline model that will assign the average `\"fare_amount\"` for the corresponding hour. For now, you will create the model for the whole train data and make predictions for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\n",
    "test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\n",
    "\n",
    "# Get pickup hour from the pickup_datetime column\n",
    "train['hour'] = train['pickup_datetime'].dt.hour\n",
    "test['hour'] = test['pickup_datetime'].dt.hour\n",
    "\n",
    "# Calculate average fare_amount grouped by pickup hour\n",
    "hour_groups = train.groupby('hour')['fare_amount'].mean()\n",
    "\n",
    "# Make predictions on the test set\n",
    "test['fare_amount'] = test.hour.map(hour_groups)\n",
    "\n",
    "# Write predictions\n",
    "test[['id', 'fare_amount']].to_csv('hour_mean_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,fare_amount\r\n",
      "0,11.199879638916757\r\n",
      "1,11.199879638916757\r\n",
      "2,11.241585365853654\r\n",
      "3,10.964889086069206\r\n",
      "4,10.964889086069206\r\n",
      "5,10.964889086069206\r\n",
      "6,11.094688755020092\r\n",
      "7,11.094688755020092\r\n",
      "8,11.094688755020092\r\n"
     ]
    }
   ],
   "source": [
    "!head hour_mean_sub.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline based on the gradient boosting\n",
    "Let's build a final baseline based on the Random Forest. You've seen a huge score improvement moving from the grouping baseline to the Gradient Boosting in the video. Now, you will use `sklearn`'s Random Forest to further improve this score.\n",
    "\n",
    "The goal of this exercise is to take numeric features and train a Random Forest model without any tuning. After that, you could make test predictions and validate the result on the Public Leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select only numeric features\n",
    "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "            'dropoff_latitude', 'passenger_count', 'hour']\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(train[features], train.fare_amount)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['fare_amount'] = rf.predict(test[features])\n",
    "\n",
    "# Write predictions\n",
    "test[['id', 'fare_amount']].to_csv('rf_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,fare_amount\r\n",
      "0,8.658000000000001\r\n",
      "1,8.472000000000003\r\n",
      "2,5.587\r\n",
      "3,9.402000000000001\r\n",
      "4,13.478000000000002\r\n",
      "5,8.375000000000007\r\n",
      "6,5.745999999999999\r\n",
      "7,54.50429999999999\r\n",
      "8,11.93\r\n"
     ]
    }
   ],
   "source": [
    "!head rf_sub.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "- Ridge Regression\n",
    "    - Least squares linear regression\n",
    "$$ \\text{Loss} = \\sum_{i=1}^N (y_i - \\hat{y_i})^2 \\rightarrow \\text{min} $$\n",
    "    - Ridge Regression\n",
    "$$ \\text{Loss} = \\sum_{i=1}^N (y_i - \\hat{y_i})^2 + \\alpha \\sum_{j=1}^K w_j^2 \\rightarrow \\text{min} $$\n",
    "    - $\\alpha$ is hyperparameter\n",
    "- Hyperparameter optimization strategies\n",
    "    - Grid Search - Choose the predefined grid of hyperparamter values\n",
    "![gs](image/grid_search.png)\n",
    "    - Random Search - Choose the search space of hyperparamter values\n",
    "![rs](image/random_search.png)\n",
    "    - Bayesian optimization - Choose the search space of hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "Recall that we've created a baseline Gradient Boosting model in the previous lesson. Your goal now is to find the best `max_depth` hyperparameter value for this Gradient Boosting model. This hyperparameter limits the number of nodes in each individual tree. You will be using K-fold cross-validation to measure the local performance of the model for each hyperparameter value.\n",
    "\n",
    "You're given a function `get_cv_score()`, which takes the train dataset and dictionary of the model parameters as arguments and returns the overall validation RMSE score over 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def get_cv_score(train, params):\n",
    "    # Create KFold object\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "    rmse_scores = []\n",
    "    \n",
    "    # Loop through each split\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "        # Train a Gradient Boosting model\n",
    "        gb = GradientBoostingRegressor(random_state=123, **params).fit(cv_train[features], cv_train.fare_amount)\n",
    "    \n",
    "        # Make predictions on the test data\n",
    "        pred = gb.predict(cv_test[features])\n",
    "    \n",
    "        fold_score = np.sqrt(mean_squared_error(cv_test['fare_amount'], pred))\n",
    "        rmse_scores.append(fold_score)\n",
    "    \n",
    "    return np.round(np.mean(rmse_scores) + np.std(rmse_scores), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 5.67086, 6: 5.36931, 9: 5.35615, 12: 5.49952, 15: 5.70183}\n"
     ]
    }
   ],
   "source": [
    "# Possible max depth values\n",
    "max_depth_grid = [3, 6, 9, 12, 15]\n",
    "results = {}\n",
    "\n",
    "# For each value in the grid\n",
    "for max_depth_candidate in max_depth_grid:\n",
    "    # Specify parameters for the model\n",
    "    params = {'max_depth': max_depth_candidate}\n",
    "    \n",
    "    # Calculate validation score for a particular hyperparameter\n",
    "    validation_score = get_cv_score(train, params)\n",
    "    \n",
    "    # Save the results for each max depth value\n",
    "    results[max_depth_candidate] = validation_score\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D grid search\n",
    "The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations. However, in such cases, the grid search space is rapidly expanding. For example, if we have 2 parameters with 10 possible values, it will yield 100 experiment runs.\n",
    "\n",
    "Your goal is to find the best hyperparameter couple of `max_depth` and `subsample` for the Gradient Boosting model. `subsample` is a fraction of observations to be used for fitting the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(3, 0.8): 5.65813, (3, 0.9): 5.65228, (3, 1.0): 5.67086, (5, 0.8): 5.34925, (5, 0.9): 5.44507, (5, 1.0): 5.3132, (7, 0.8): 5.39073, (7, 0.9): 5.40612, (7, 1.0): 5.35909}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Hyperparameter grids\n",
    "max_depth_grid = [3, 5, 7]\n",
    "subsample_grid = [0.8, 0.9, 1.0]\n",
    "results = {}\n",
    "\n",
    "# For each couple in the grid\n",
    "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
    "    params = {'max_depth': max_depth_candidate,\n",
    "              'subsample': subsample_candidate}\n",
    "    validation_score = get_cv_score(train, params)\n",
    "    # Save the results fro each couple\n",
    "    results[(max_depth_candidate, subsample_candidate)] = validation_score\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensembling\n",
    "- Model blending\n",
    "- Model stacking\n",
    "    1. Split train data into two parts\n",
    "    2. Train multiple models on Part 1\n",
    "    3. Make predictions on Part 2\n",
    "    4. Make predictions on the test data\n",
    "    5. Train a new model on Part 2 using predictions as features\n",
    "    6. Make predictions on the test data using the 2nd level model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model blending\n",
    "You will start creating model ensembles with a blending technique.\n",
    "\n",
    "Your goal is to train 2 different models on the New York City Taxi competition data. Make predictions on the test data and then blend them using a simple arithmetic mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./dataset/taxi_train_distance.csv')\n",
    "test = pd.read_csv('./dataset/taxi_test_distance.csv')\n",
    "\n",
    "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', \n",
    "            'passenger_count', 'distance_km', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gb_pred</th>\n",
       "      <th>rf_pred</th>\n",
       "      <th>blend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.661374</td>\n",
       "      <td>9.057</td>\n",
       "      <td>9.359187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304288</td>\n",
       "      <td>8.269</td>\n",
       "      <td>8.786644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.795140</td>\n",
       "      <td>4.936</td>\n",
       "      <td>5.365570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gb_pred  rf_pred     blend\n",
       "0  9.661374    9.057  9.359187\n",
       "1  9.304288    8.269  8.786644\n",
       "2  5.795140    4.936  5.365570"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Gradient Boosting model\n",
    "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['gb_pred'] = gb.predict(test[features])\n",
    "test['rf_pred'] = rf.predict(test[features])\n",
    "\n",
    "# Find mean of model predictions\n",
    "test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n",
    "test[['gb_pred', 'rf_pred', 'blend']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stacking I\n",
    "Now it's time for stacking. To implement the stacking approach, you will follow the 6 steps:\n",
    "\n",
    "1. Split train data into two parts\n",
    "2. Train multiple models on Part 1\n",
    "3. Make predictions on Part 2\n",
    "4. Make predictions on the test data\n",
    "5. Train a new model on Part 2 using predictions as features\n",
    "6. Make predictions on the test data using the 2nd level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into two parts\n",
    "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
    "\n",
    "# Train a Gradient Boosting model on Part 1\n",
    "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
    "\n",
    "# Train a Random Forest model on Part 1\n",
    "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)\n",
    "\n",
    "# Make predictions on the Part 2 data\n",
    "part_2 = part_2.copy()\n",
    "part_2['gb_pred'] = gb.predict(part_2[features])\n",
    "part_2['rf_pred'] = rf.predict(part_2[features])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test = test.copy()\n",
    "test['gb_pred'] = gb.predict(test[features])\n",
    "test['rf_pred'] = rf.predict(test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stacking II\n",
    "what you've done so far in the stacking implementation:\n",
    "\n",
    "1. Split train data into two parts\n",
    "2. Train multiple models on Part 1\n",
    "3. Make predictions on Part 2\n",
    "4. Make predictions on the test data\n",
    "\n",
    "Now, your goal is to create a second level model using predictions from steps 3 and 4 as features. So, this model is trained on Part 2 data and then you can make stacking predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27882028 0.72809648]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create linear regression model without the intercept\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Train 2nd level model on the Part 2 data\n",
    "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
    "\n",
    "# Make stacking predictions on the test data\n",
    "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
    "\n",
    "# Look at the model coefficients\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the 2nd level model is some simple model like Linear or Logistic Regressions. Also, note that you were not using intercept in the Linear Regression just to combine pure model predictions. Looking at the coefficients, it's clear that 2nd level model has more trust to the Random Forest: 0.7 versus 0.3 for the Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Kaggle forum ideas\n",
    "Unfortunately, not all the Forum posts and Kernels are necessarily useful for your model. So instead of blindly incorporating ideas into your pipeline, you should test them first.\n",
    "\n",
    "You're given a function `get_cv_score()`, which takes a train dataset as an argument and returns the overall validation root mean squared error over 3-fold cross-validation. \n",
    "\n",
    "You should try different suggestions from the Kaggle Forum and check whether they improve your validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_score(train):\n",
    "    features = ['pickup_longitude', 'pickup_latitude',\n",
    "            'dropoff_longitude', 'dropoff_latitude',\n",
    "            'passenger_count', 'distance_km', 'hour', 'weird_feature']\n",
    "    \n",
    "    features = [x for x in features if x in train.columns]\n",
    "    \n",
    "    # Create KFold object\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "    rmse_scores = []\n",
    "    \n",
    "    # Loop through each split\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "        # Train a Gradient Boosting model\n",
    "        gb = GradientBoostingRegressor(random_state=123).fit(cv_train[features], cv_train.fare_amount)\n",
    "    \n",
    "        # Make predictions on the test data\n",
    "        pred = gb.predict(cv_test[features])\n",
    "    \n",
    "        fold_score = np.sqrt(mean_squared_error(cv_test['fare_amount'], pred))\n",
    "        rmse_scores.append(fold_score)\n",
    "    \n",
    "    return np.round(np.mean(rmse_scores) + np.std(rmse_scores), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial score is 6.49932 and the new score is 6.42315\n"
     ]
    }
   ],
   "source": [
    "# Drop passenger_count column\n",
    "new_train_1 = train.drop('passenger_count', axis=1)\n",
    "\n",
    "# Compare validation scores\n",
    "initial_score = get_cv_score(train)\n",
    "new_score = get_cv_score(new_train_1)\n",
    "\n",
    "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial score is 6.49932 and the new score is 6.50495\n"
     ]
    }
   ],
   "source": [
    "# Create copy of the initial train DataFrame\n",
    "new_train_2 = train.copy()\n",
    "\n",
    "# Find sum of pickup latitude and ride distance\n",
    "new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\n",
    "\n",
    "# Compare validation scores\n",
    "initial_score = get_cv_score(train)\n",
    "new_score = get_cv_score(new_train_2)\n",
    "\n",
    "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
