# BackPropgation.py

from numpy import *
import operator
import Untils
import matplotlib.pyplot as plt


# 传递函数:
def logistic(inX):
    return 1.0 / (1.0 + exp(-inX))


# 传递函数的导函数
def dlogit(inX1, inX2):
    return multiply(inX2, (1.0 - inX2))


# 矩阵各元素平方之和
def errorfunc(inX):
    return sum(power(inX, 2)) / 2.0


# 加载student.txt数据集
def loadDataSet(filename):
    dataMat = []
    labelMat = []
    fr = open(filename)  # testSet.txt
    for line in fr.readlines():
        lineArr = line.strip().split()
        dataMat.append([float(lineArr[0]), float(lineArr[1]), 1.0])
        labelMat.append(int(lineArr[2]))
    return dataMat, labelMat


# 数据标准化(归一化):student.txt数据集
def normalize(dataMat):
    # 标准化
    dataMat[:, 0] = (dataMat[:, 0] - mean(dataMat[:, 0])) / std(dataMat[:, 0])
    dataMat[:, 1] = (dataMat[:, 1] - mean(dataMat[:, 1])) / std(dataMat[:, 1])
    return dataMat


def bpNet(dataSet, classLabels):
    # 数据集矩阵化
    SampIn = mat(dataSet).T
    expected = mat(classLabels)
    m, n = shape(dataSet)
    # 网络参数
    eb = 0.01  # 误差容限
    eta = 0.05  # 学习率
    mc = 0.3  # 动量因子
    maxiter = 2000  # 最大迭代次数
    errlist = []  # 误差列表

    # 构造网络
    # 初始化网络
    nSampNum = m
    # 样本数量
    nSampDim = n - 1
    # 样本维度
    nHidden = 4
    # 隐含层神经元
    nOut = 1
    # 输出层

    # 隐含层参数
    hi_w = 2.0 * (random.rand(nHidden, nSampDim) - 0.5)
    hi_b = 2.0 * (random.rand(nHidden, 1) - 0.5)
    hi_wb = mat(Untils.mergMatrix(mat(hi_w), mat(hi_b)))

    # 输出层参数
    out_w = 2.0 * (random.rand(nOut, nHidden) - 0.5)
    out_b = 2.0 * (random.rand(nOut, 1) - 0.5)
    out_wb = mat(Untils.mergMatrix(mat(out_w), mat(out_b)))
    # 默认旧权值
    dout_wbOld = 0.0
    dhi_wbOld = 0.0

    for i in xrange(maxiter):
        # 1. 工作信号正向传播

        # 1.1 输入层到隐含层
        hi_input = hi_wb * SampIn
        hi_output = logistic(hi_input)
        hi2out = Untils.mergMatrix(hi_output.T, ones((nSampNum, 1))).T

        # 1.2 隐含层到输出层
        out_input = out_wb * hi2out
        out_output = logistic(out_input)

        # 2. 误差计算
        err = expected - out_output
        sse = errorfunc(err)
        errlist.append(sse)
        # 2.1 判断是否收敛
        if sse <= eb:
            print("iteration:", i + 1)
            break

        # 3.误差信号反向传播
        # 3.1 DELTA为输出层到隐含层梯度
        DELTA = multiply(err, dlogit(out_input, out_output))
        wDelta = out_wb[:, :-1].T * DELTA

        # 3.2 delta为隐含层到输入层梯度
        delta = multiply(wDelta, dlogit(hi_input, hi_output))
        dout_wb = DELTA * hi2out.T

        # 3.3 输入层的权值更新
        dhi_wb = delta * SampIn.T

        # 3.4 更新输出层和隐含层权值
        if i == 0:
            out_wb = out_wb + eta * dout_wb
            hi_wb = hi_wb + eta * dhi_wb
        else:
            out_wb = out_wb + (1.0 - mc) * eta * dout_wb + mc * dout_wbOld
            hi_wb = hi_wb + (1.0 - mc) * eta * dhi_wb + mc * dhi_wbOld
        dout_wbOld = dout_wb
        dhi_wbOld = dhi_wb
    return errlist, out_wb, hi_wb


def BPClassfier(start, end, WEX, wex):
    x = linspace(start, end, 30)
    xx = mat(ones((30, 30)))
    xx[:, 0:30] = x
    yy = xx.T
    z = ones((len(xx), len(yy)))
    for i in range(len(xx)):
        for j in range(len(yy)):
            xi = []
            tauex = []
            tautemp = []
            mat(xi.append([xx[i, j], yy[i, j], 1]))
            hi_input = wex * (mat(xi).T)
            hi_out = logistic(hi_input)
            taumrow, taucol = shape(hi_out)
            tauex = mat(ones((1, taumrow + 1)))
            tauex[:, 0:taumrow] = (hi_out.T)[:, 0:taumrow]
            HM = WEX * (mat(tauex).T)
            out = logistic(HM)
            z[i, j] = out
    return x, z
