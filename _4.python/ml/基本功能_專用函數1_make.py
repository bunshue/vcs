"""
åŸºæœ¬åŠŸèƒ½_å°ˆç”¨å‡½æ•¸1

ä½¿ç”¨ scikit-learn(sklearn) çš„ å‡½æ•¸

sklearnå…§å»ºè³‡æ–™é›†é›†åˆ

é è¨­æ–¹æ³•å»ºç«‹è³‡æ–™é›†

ä¸€äº›ç°¡æ˜“çš„é‹ç®—
"""

"""
æ©Ÿå™¨å­¸ç¿’ç­†è¨˜ï¼šå¸¸ç”¨æ•¸æ“šé›†ä¹‹scikit-learnç”Ÿæˆåˆ†é¡å’Œèšé¡æ•¸æ“šé›†
æœ¬æ–‡ä»‹ç´¹åˆ†é¡å’Œèšé¡æ•¸æ“šé›†çš„ç”Ÿæˆï¼ŒåŒ…æ‹¬ä»¥ä¸‹9å€‹æ¥å£å‡½æ•¸ï¼Œå…¶ä¸­ï¼Œ
æœ‰å…­å€‹æ˜¯ç”¨äºå–®æ¨™ç°½é¡æ•¸æ“šç”Ÿæˆï¼š

(1) make_blobs()
(2) make_classification()
(3) make_gaussian_quantiles()
(4) make_hastie_10_2()
(5) make_circles()
(6) make_moons()

ä¸€å€‹ç”¨äºå¤šæ¨™ç°½é¡æ•¸æ“šç”Ÿæˆ:
(7) make_multilabel_classification()

é‚„æœ‰å…©å€‹ç”¨äºé›™èšé¡æ•¸æ“šé›†ç”Ÿæˆï¼š
(8) make_biclusters
(9) make_checkerboard
"""


print("------------------------------------------------------------")  # 60å€‹

# å…±åŒ
import os
import sys
import time
import math
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  # æµ·ç”Ÿ, è‡ªå‹•æŠŠåœ–ç•«å¾—æ¯”è¼ƒå¥½çœ‹

font_filename = "C:/_git/vcs/_1.data/______test_files1/_font/msch.ttf"
# è¨­å®šä¸­æ–‡å­—å‹åŠè² è™Ÿæ­£ç¢ºé¡¯ç¤º
# è¨­å®šä¸­æ–‡å­—å‹æª”
plt.rcParams["font.sans-serif"] = "Microsoft JhengHei"  # å°‡å­—é«”æ›æˆ Microsoft JhengHei
# è¨­å®šè² è™Ÿ
plt.rcParams["axes.unicode_minus"] = False  # è®“è² è™Ÿå¯æ­£å¸¸é¡¯ç¤º
plt.rcParams["font.size"] = 12  # è¨­å®šå­—å‹å¤§å°

print("------------------------------------------------------------")  # 60å€‹

import ssl

ssl._create_default_https_context = ssl._create_stdlib_context

print("------------------------------------------------------------")  # 60å€‹

from sklearn.datasets import make_regression  # è¿´æ­¸è³‡æ–™é›†
from sklearn.datasets import make_blobs  # é›†ç¾¤è³‡æ–™é›†
from sklearn.datasets import make_classification  # åˆ†é¡è³‡æ–™é›†
from sklearn.datasets import make_moons  # éç·šæ€§çš„è³‡æ–™é›†
from sklearn.datasets import make_circles  # åœ“å½¢åˆ†ä½ˆçš„è³‡æ–™é›†
from sklearn.datasets import make_gaussian_quantiles
from sklearn.datasets import make_hastie_10_2
from sklearn.datasets import make_multilabel_classification as make_ml_clf
from sklearn.datasets import make_biclusters
from sklearn.datasets import make_checkerboard

print("------------------------------------------------------------")  # 60å€‹

import sklearn
from sklearn import datasets
from sklearn import svm
from sklearn import metrics

print(sklearn.__version__)
# print(dir(datasets))
print(sklearn)


def show():
    plt.show()
    pass


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

plt.figure(num="make_regression", figsize=(16, 9))

print("------------------------------")  # 30å€‹
plt.subplot(231)

plt.title("make_regression è¿´æ­¸è³‡æ–™é›†")

N = 50  # n_samples, æ¨£æœ¬æ•¸
M = 1  # n_features, ç‰¹å¾µæ•¸(è³‡æ–™çš„ç¶­åº¦)
T = 1  # n_targets, æ¨™ç±¤é¡åˆ¥
NOISE = 10  # noise, åˆ†æ•£ç¨‹åº¦

# X, y = datasets.make_regression(n_samples=N, n_features=M, n_targets=T, noise=NOISE)

print("make_regression,", N, "å€‹æ¨£æœ¬, ", M, "å€‹ç‰¹å¾µ")

X, y, coef = make_regression(
    n_samples=N,
    n_features=M,
    noise=20,
    coef=True,
    random_state=9487
    # n_targets=1, noise=1.5,
)
print(X.shape, y.shape)
print(coef)

plt.scatter(X[:, 0], y)
plt.plot([min(X), max(X)], [min(X) * coef, max(X) * coef], "r")

print("------------------------------")  # 30å€‹
plt.subplot(232)

print("------------------------------")  # 30å€‹
plt.subplot(233)

print("------------------------------")  # 30å€‹
plt.subplot(234)

plt.title("make_classification åˆ†é¡è³‡æ–™é›†")

print("åˆ†é¡è³‡æ–™é›†")

N = 50  # n_samples, æ¨£æœ¬æ•¸
M = 1  # n_features, ç‰¹å¾µæ•¸(è³‡æ–™çš„ç¶­åº¦)
print("make_classification,", N, "å€‹æ¨£æœ¬, ", M, "å€‹ç‰¹å¾µ")

X, y = make_classification(
    n_samples=N,
    n_classes=3,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=9487,
)
print(X.shape)

# ç¹ªåœ–
# æ¨£æœ¬é»çš„å½¢ç‹€
markers = ["x", "o", "^"]

# é‡å°é¡åˆ¥å„ç•«ä¸€å€‹æ•£ä½ˆåœ–
for k in range(3):
    X_0 = []
    X_1 = []
    for i in range(len(y)):
        if y[i] == k:
            X_0.append(X[i, 0])
            X_1.append(X[i, 1])
            plt.scatter(X_0, X_1, marker=markers[k], s=50)

print("------------------------------")  # 30å€‹
plt.subplot(235)

print("------------------------------")  # 30å€‹
plt.subplot(236)

print("ä½¿ç”¨center_box")

N = 1000  # n_samples, æ¨£æœ¬æ•¸
M = 2  # n_features, ç‰¹å¾µæ•¸(è³‡æ–™çš„ç¶­åº¦)
GROUPS = 6  # centers, åˆ†ç¾¤æ•¸
STD = 0.3  # cluster_std, è³‡æ–™æ¨™æº–å·®
print("make_blobs,", N, "å€‹æ¨£æœ¬, ", M, "å€‹ç‰¹å¾µ, åˆ†æˆ", GROUPS, "ç¾¤")

X, y, centers = make_blobs(
    n_samples=N,
    n_features=M,
    centers=GROUPS,
    cluster_std=STD,
    center_box=(-10.0, 10.0),
    return_centers=True,
)

print(GROUPS, "ç¾¤ çš„ä¸­å¿ƒé» :\n", centers)

plt.scatter(*zip(*X))

# æ¨™è¨˜ç¾¤é›†ä¸­å¿ƒ
plt.scatter(centers[:, 0], centers[:, 1], marker="*", s=200, color="r")
# ç›®å‰é‚„ä¸æœƒæŠŠæ¡†ç•«å‡ºä¾† center_box

plt.suptitle("å„ç¨® make_regression")

show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

plt.figure(num="make_blobs", figsize=(16, 9))

N = 10  # n_samples, æ¨£æœ¬æ•¸
M = 4  # n_features, ç‰¹å¾µæ•¸(è³‡æ–™çš„ç¶­åº¦)
GROUPS = 3  # centers, åˆ†ç¾¤æ•¸
STD = 1  # cluster_std, è³‡æ–™æ¨™æº–å·®
print("make_blobs,", N, "å€‹æ¨£æœ¬, ", M, "å€‹ç‰¹å¾µ, åˆ†æˆ", GROUPS, "ç¾¤")

X, y, centers = make_blobs(
    n_samples=N, centers=GROUPS, cluster_std=STD, n_features=M, return_centers=True
)

print(GROUPS, "ç¾¤ çš„ä¸­å¿ƒé» : ", centers)
print(GROUPS, "å€‹ç›®æ¨™")
print("è³‡æ–™çš„ç¶­åº¦")
print("X :\t", X.shape)
print("y :\t", y.shape)
print("è³‡æ–™çš„å…§å®¹")
print("X :\n", X)
print("y :\n", y)

print("------------------------------")  # 30å€‹
plt.subplot(231)
N, M, GROUPS, STD = 100, 2, 3, 1
X, y, centers = make_blobs(
    n_samples=N, centers=GROUPS, cluster_std=STD, n_features=M, return_centers=True
)
plt.scatter(X[:, 0], X[:, 1], c=y)
# æ¨™è¨˜ç¾¤é›†ä¸­å¿ƒ
plt.scatter(centers[:, 0], centers[:, 1], marker="*", s=200, color="r")
plt.title("sd=1, 3ç¾¤")

print("------------------------------")  # 30å€‹
plt.subplot(232)
N, M, GROUPS, STD = 100, 2, 5, 1
X, y, centers = make_blobs(
    n_samples=N, centers=GROUPS, cluster_std=STD, n_features=M, return_centers=True
)
plt.scatter(X[:, 0], X[:, 1], c=y)
# æ¨™è¨˜ç¾¤é›†ä¸­å¿ƒ
plt.scatter(centers[:, 0], centers[:, 1], marker="*", s=200, color="r")
plt.title("sd=1, 5ç¾¤")

print("------------------------------")  # 30å€‹
plt.subplot(233)
print("æ¯ç¾¤ä¸åŒå¤§å°, æŒ‡å®šä¸­å¿ƒä½ç½®")

N0, N1, N2, N3 = 50, 150, 250, 400  # æ¨£æœ¬æ•¸
cx0, cy0 = 100, 120  # ç¬¬0ç¾¤çš„ä¸­å¿ƒä½ç½®
cx1, cy1 = 250, 300  # ç¬¬1ç¾¤çš„ä¸­å¿ƒä½ç½®
cx2, cy2 = 700, 150  # ç¬¬2ç¾¤çš„ä¸­å¿ƒä½ç½®
cx3, cy3 = 300, 500  # ç¬¬3ç¾¤çš„ä¸­å¿ƒä½ç½®

X, y, centers = make_blobs(
    n_samples=[N0, N1, N2, N3],
    n_features=2,
    centers=[[cx0, cy0], [cx1, cy1], [cx2, cy2], [cx3, cy3]],
    cluster_std=50,
    return_centers=True,
)

plt.scatter(X[:, 0], X[:, 1], c=y)
# æ¨™è¨˜ç¾¤é›†ä¸­å¿ƒ
plt.scatter(centers[:, 0], centers[:, 1], marker="*", s=200, color="r")
plt.title("æ¯ç¾¤ä¸åŒå¤§å°, æŒ‡å®šä¸­å¿ƒä½ç½®")

print("------------------------------")  # 30å€‹
plt.subplot(234)
N, M, GROUPS = 100, 2, 3
X, y, centers = make_blobs(
    n_samples=N, centers=GROUPS, n_features=M, return_centers=True
)
plt.scatter(X[:, 0], X[:, 1], c=y)
# æ¨™è¨˜ç¾¤é›†ä¸­å¿ƒ
plt.scatter(centers[:, 0], centers[:, 1], marker="*", s=200, color="r")
plt.axis([-15, 15, -15, 15])
plt.title("ç„¡ sd")

print("------------------------------")  # 30å€‹
plt.subplot(235)
N, M, GROUPS, STD = 100, 2, 3, 3
X, y, centers = make_blobs(
    n_samples=N, centers=GROUPS, cluster_std=STD, n_features=M, return_centers=True
)
plt.scatter(X[:, 0], X[:, 1], c=y)
# æ¨™è¨˜ç¾¤é›†ä¸­å¿ƒ
plt.scatter(centers[:, 0], centers[:, 1], marker="*", s=200, color="r")
plt.axis([-15, 15, -15, 15])
plt.title("sd=3")

print("------------------------------")  # 30å€‹
plt.subplot(236)
N, M, GROUPS, STD = 100, 2, 3, 6
X, y, centers = make_blobs(
    n_samples=N, centers=GROUPS, cluster_std=STD, n_features=M, return_centers=True
)
plt.scatter(X[:, 0], X[:, 1], c=y)
# æ¨™è¨˜ç¾¤é›†ä¸­å¿ƒ
plt.scatter(centers[:, 0], centers[:, 1], marker="*", s=200, color="r")
plt.axis([-15, 15, -15, 15])
plt.title("sd=6")

plt.suptitle("å„ç¨® make_blobs")

show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

plt.figure(num="make_å…¶ä»–", figsize=(16, 9))

print("------------------------------")  # 30å€‹
plt.subplot(231)

plt.title("make_moons éç·šæ€§çš„è³‡æ–™é›†")

N = 100  # make_moons æœªæŒ‡å®šå€‹æ•¸, å°±æ˜¯100å€‹
X, y = make_moons(n_samples=N, noise=0.05)
print(X.shape)

# é‡å°é¡åˆ¥å„ç•«ä¸€å€‹æ•£ä½ˆåœ–
colors = ["red", "blue"]
for k in range(2):
    X_0 = []
    X_1 = []
    for i in range(len(y)):
        if y[i] == k:
            X_0.append(X[i, 0])
            X_1.append(X[i, 1])
            plt.scatter(X_0, X_1, s=50, c=colors[k])

print("------------------------------")  # 30å€‹
plt.subplot(232)

plt.title("make_circles åœ“å½¢åˆ†ä½ˆçš„è³‡æ–™é›†")

X, y = make_circles(n_samples=N, noise=0.05)
print(X.shape)

# é‡å°é¡åˆ¥å„ç•«ä¸€å€‹æ•£ä½ˆåœ–
colors = ["red", "blue"]
for k in range(2):
    X_0 = []
    X_1 = []
    for i in range(len(y)):
        if y[i] == k:
            X_0.append(X[i, 0])
            X_1.append(X[i, 1])
            plt.scatter(X_0, X_1, s=50, c=colors[k])

print("------------------------------")  # 30å€‹
plt.subplot(233)

print("------------------------------")  # 30å€‹
plt.subplot(234)

print("------------------------------")  # 30å€‹
plt.subplot(235)

print("------------------------------")  # 30å€‹
plt.subplot(236)

plt.suptitle("å„ç¨® make_å…¶ä»–")

show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


plt.figure(num="make_regression", figsize=(16, 9))

print("------------------------------")  # 30å€‹
plt.subplot(231)

print("------------------------------")  # 30å€‹
plt.subplot(232)

print("------------------------------")  # 30å€‹
plt.subplot(233)

print("------------------------------")  # 30å€‹
plt.subplot(234)

print("------------------------------")  # 30å€‹
plt.subplot(235)

print("------------------------------")  # 30å€‹
plt.subplot(236)

plt.suptitle("å„ç¨® make_regression")

show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

""" ç•«åœ¨ä¸€èµ· TBD
plt.figure(
    num="sklearnå…§å»ºè³‡æ–™é›†é›†åˆ",
    figsize=(16, 9),
    dpi=100,
    facecolor="whitesmoke",
    edgecolor="r",
    linewidth=1,
    frameon=True,
)

print("------------------------------")  # 30å€‹
#plt.subplot(231)
#plt.title("make_classification")
"""

"""
2. make_classification

make_blobs()å’Œmake_classification()éƒ½ç”¨äºç”Ÿæˆå¤šé¡åˆ¥çš„æ•¸æ“šé›†ï¼Œ
æ¯å€‹é¡åˆ¥éƒ½æ˜¯ç”±ä¸€å€‹æˆ–è€…å¤šå€‹æ­£æ…‹åˆ†å¸ƒç°‡(normally-distributed cluster)æ§‹æˆã€‚

make_blobså°äºå„ç°‡çš„ä¸­å¿ƒå’Œæ¨™æº–åå·®æä¾›äº†æ›´æ–¹ä¾¿çš„æ§åˆ¶é¸é …ï¼Œ
é€šå¸¸ç”¨äºèšé¡ç®—æ³•çš„æ¼”ç¤ºã€‚è€Œmake_classificationå‰‡æ›´åŠ å´é‡äºé€šéå„ç¨®æ‰‹æ®µå°å…¥å„ç¨®â€œå™ªè²â€çš„å½±éŸ¿ï¼Œ
æ¯”å¦‚èªªï¼Œç›¸é—œçš„ã€å†—ä½™çš„ã€æ²’æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾ï¼›æ¯å€‹é¡åˆ†æˆå¤šå€‹æ­£æ…‹åˆ†å¸ƒç°‡ï¼›ç‰¹å¾ç©ºé–“çš„ç·šæ€§è®Šæ›ç­‰ç­‰ã€‚
"""

# make_classification()ç”ŸæˆäºŒåˆ†é¡æ•¸æ“šé›†

N = 500  # n_samples, æ¨£æœ¬æ•¸
M = 5  # n_features, ç‰¹å¾µæ•¸(è³‡æ–™çš„ç¶­åº¦)
print("make_classification,", N, "å€‹æ¨£æœ¬, ", M, "å€‹ç‰¹å¾µ")

X, y = make_classification(
    n_samples=N,
    n_features=M,
    n_redundant=0,
    n_clusters_per_class=1,
    n_informative=1,
    n_classes=2,
    random_state=9487,
)

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue"}
fig, ax = plt.subplots()
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

print(X.shape, y.shape)
plt.title("make_classification")
show()

print("------------------------------")  # 30å€‹
# plt.subplot(234)
# plt.title("make_moons")

""" 
4. make_moons
        make_moons()å‡½æ•¸ç”Ÿæˆä¸€å€‹äºŒåˆ†é¡å•é¡Œæ•¸æ“šé›†ï¼Œ
        å®ƒç”Ÿæˆå…©å€‹åŠæœˆå½¢å°æ‡‰å…©å€‹åˆ†é¡ã€‚å¯ä»¥é€šénoiseåƒæ•¸ä¾†æ§åˆ¶å™ªè²é‡ã€‚
        é©åˆäºéç·šæ€§åˆ†é¡ç®—æ³•çš„æ¼”ç¤ºã€‚
"""

# make_moons: Generate isotropic Gaussian blobs for clustering.
# ç¶“å¸¸ç”¨äºéç·šæ€§åˆ†é¡ç¤ºä¾‹ã€‚

N = 500  # æ¨£æœ¬æ•¸
X, y = make_moons(n_samples=N, shuffle=True, noise=0.1, random_state=9487)

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue"}
fig, ax = plt.subplots()
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

plt.title("make_moons")
show()

print("------------------------------")  # 30å€‹
# plt.subplot(235)
# plt.title("make_circles")

"""
5. make_circles
é¡§åæ€ç¾©ï¼Œæ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ§‹æˆä¸€å€‹åœ“å½¢ã€‚
"""
# make_circles: generates a binary classification problem with datasets that fall into concentric circles.
# Make a large circle containing a smaller circle in 2d.
# A simple toy dataset to visualize clustering and classification algorithms, suitable for algorithms that can learn complex non-linear manifolds.

N = 500  # æ¨£æœ¬æ•¸
X, y = make_circles(n_samples=N, noise=0.05)
# 'noise' is used to control the amount of noise in the shapes.

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue"}
fig, ax = plt.subplots(figsize=[6, 6])
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

plt.title("make_circles")
show()

print("------------------------------")  # 30å€‹
# plt.subplot(236)
# plt.title("make_gaussian_quantiles")

"""
6. make_gaussian_quantiles
make_gaussian_quantiles()é¦–å…ˆç”Ÿæˆä¸€å€‹å¤šç¶­æ­£æ…‹åˆ†å¸ƒæ¨£æœ¬é›†ï¼Œ
ç„¶åï¼Œå°‡é€™æ¨£æœ¬é›†åŸºäºåˆ†ä½é»(quantiles)åˆ†å‰²æˆå¤šå€‹(n_classes=3 by default)åµŒå¥—çš„å¤šç¶­åŒå¿ƒè¶…çƒï¼Œ
æ¯å€‹è¶…çƒå±¬äºä¸€é¡ï¼Œå¹¶ä½¿å¾—å¤§è‡´å„é¡çš„æ¨£æœ¬åŸºæœ¬ç›¸ç­‰ã€‚
åŸºäºåˆ†ä½é»é€²è¡Œåˆ†å‰²æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ
ä»¥ä¸€ç¶­æ­£æ…‹åˆ†å¸ƒç‚ºä¾‹ï¼Œå¤§è‡´ä¾†èªªå°±æ˜¯é€™æ¨£åˆ†å‰²çš„ã€‚
å‡è¨­n_classes = 3ï¼Œå› æ­¤å°æ‡‰çš„å…©å€‹åˆ†å‰²ç”¨çš„åˆ†ä½é»å°±æ˜¯33%å’Œ66%ã€‚
å–æ¨£æœ¬ä¸­ä½äº[0, 33%]åˆ†ä½å€é–“çš„ä½œç‚ºç¬¬ä¸€é¡ï¼Œä½äº[33%, 66%]åˆ†ä½å€é–“çš„ä½œç‚ºç¬¬äºŒé¡ï¼Œ
ä½äº[66%, 100%]åˆ†ä½å€é–“çš„ä½œç‚ºç¬¬ä¸‰é¡ã€‚
å°äºå¤šç¶­æ•¸æ“šï¼Œæ˜¯åŸºäºå°æ‡‰çš„ğœ’2åˆ†å¸ƒçš„åˆ†ä½æ•¸ä¾†é€²è¡Œåˆ†é¡ã€‚
"""

N = 1000  # æ¨£æœ¬æ•¸
data, target = make_gaussian_quantiles(n_samples=N, cov=1.0, n_classes=3)

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=data[:, 0], y=data[:, 1], label=target))
colors = {0: "red", 1: "blue", 2: "k"}
fig, ax = plt.subplots(figsize=[6, 6])
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

plt.title("make_gaussian_quantiles")
show()

# ä»¥ä¸Šç•«ä¸€èµ· æœ‰å•é¡Œ df plot~~~~~


"""
7. make_hastie_10_2
é€™å€‹å‡½æ•¸æ˜¯å°ˆé–€ç”¨äºä»¥ä¸‹Hastieçš„æ©Ÿå™¨å­¸ç¿’ç¶“å…¸æ•™æä¸­ä¾‹10.2æ‰€æåŠçš„æ•¸æ“šé›†çš„ç”Ÿæˆï¼Œç”¨äºäºŒåˆ†é¡å•é¡Œã€‚ç‚ºä¸€æœ¬æ›¸ä¸­çš„ä¸€å€‹ä¾‹å­å°ˆé–€åˆ—äº†ä¸€å€‹å‡½æ•¸ï¼Œç¢ºå¯¦æ˜¯å¾ˆæ‹¼ã€‚å¯ä»¥çœ‹ä½œæ˜¯make_gaussian_quantilesçš„ä¸€ç¨®ç‰¹ä¾‹ï¼Œæˆ–è€…åéä¾†èªªmake_gaussian_quantilesæ˜¯make_hastie_10_2çš„æ¨å»£ã€‚
T. Hastie, R. Tibshirani and J. Friedman, â€œElements of Statistical Learning Ed. 2â€, Springer, 2009.ã€‹
è©²æ•¸æ“šé›†æœ‰10å€‹ç‰¹å¾ï¼Œæ˜¯i.i.dï¼ˆç¨ç«‹åŒåˆ†å¸ƒï¼‰çš„æ¨™æº–æ­£æ…‹åˆ†å¸ƒï¼Œtarget yå®šç¾©å¦‚ä¸‹ï¼š
y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1
"""

N = 1000  # æ¨£æœ¬æ•¸
data, target = make_hastie_10_2(n_samples=N, random_state=9487)

# target[target==-1] = 0  # åŸæ•¸æ“šé›†ç”Ÿæˆçš„targetç‚º[1,-1],é€™é‡Œè®Šæ›ç‚º[1,0]
# target = target.astype('int32') # è®Šæ›æˆæ•´æ•¸

df = pd.DataFrame(data)
df["target"] = target

print(df)

"""
é€™æ˜¯ä¸€å€‹10ç¶­çš„æ•¸æ“šï¼Œæ‰€ä»¥ä¸å®¹æ˜“ä»¥æ•£é»åœ–çš„å½¢å¼é€²è¡Œåœ–ç¤ºåŒ–ã€‚ä»¥ä¸‹é€šéåœ–ç¤ºçš„æ–¹å¼çœ‹çœ‹å„å€‹ç¶­åº¦æ˜¯ä¸æ˜¯ç¨ç«‹åŒåˆ†å¸ƒï¼ˆi.i.dï¼‰çš„æ¨™æº–é«˜æ–¯åˆ†å¸ƒã€‚
"""
from scipy.stats import norm

plt.figure(figsize=(20, 6))
for k in range(10):
    df[k].plot(kind="kde", secondary_y=True, label="feature#" + str(k))

x = np.linspace(-8, 8, 1000)
plt.plot(x, norm.pdf(x), "r-", lw=2, alpha=0.6, label="theoretic std norm pdf")

plt.title("make_hastie_10_2")
plt.legend()
show()

# å¦‚ä¸Šåœ–å¯çŸ¥ï¼Œ10å€‹ç‰¹å¾åˆ†é‡ç¢ºå¯¦åŸºæœ¬ä¸Šéƒ½æ˜¯èˆ‡æ¨™æº–æ­£æ…‹åˆ†å¸ƒå»åˆçš„ã€‚

"""
8. å¤šæ¨™ç°½æ•¸æ“šé›†ç”Ÿæˆ
å¤šæ¨™ç°½æ•¸æ“šé›†ç”¨äºç•¶å­˜åœ¨å¤šå„é¡åˆ¥ï¼Œè€Œå¾…åˆ†é¡çš„æ•¸æ“šå¯èƒ½å±¬äºå…¶ä¸­çš„ä¸€é¡æˆ–è€…åŒæ™‚å±¬äºå¤šå€‹é¡åˆ¥ï¼Œæˆ–è€…ç”šè‡³ä¸å±¬äºä»»ä½•é¡åˆ¥ã€‚æ¯”å¦‚èªªï¼Œç•¶éœ€è¦è­˜åˆ¥åœ¨ä¸€å¼µåœ–åƒä¸­æ‰€åŒ…å«çš„äº¤é€šä¿¡è™Ÿç­‰çš„é¡å‹ã€‚ä¸€å¼µåœ–ç‰‡å¯èƒ½ä¸åŒ…å«ä¿¡è™Ÿç‡ˆï¼Œä¹Ÿå¯èƒ½åªåŒ…å«ä¸€å€‹ç´…ç‡ˆæˆ–ç¶ ç‡ˆæˆ–é»ƒç‡ˆï¼Œä¹Ÿå¯èƒ½åŒæ™‚åŒ…å«ä¸€å€‹ç´…ç‡ˆå’Œç¶ ç‡ˆï¼ˆå¦‚æœé€™å¼µåœ–ç‰‡è¦†è“‹äº†ä¸€å€‹åå­—è·¯å£çš„å…©å€‹æ–¹å‘çš„ä¿¡è™Ÿç‡ˆçš„è©±ï¼‰ã€‚
"""
x, y = make_ml_clf(n_samples=1000, n_features=10, n_classes=3, random_state=9487)
print(x.shape, y.shape)
print(y[:10, :])

"""
å¯ä»¥çœ‹å‡ºï¼Œç”±äºæ˜¯å¤šåˆ†é¡ï¼ˆæœ¬ä¾‹æ˜¯3åˆ†é¡ï¼‰å¤šæ¨™ç°½çš„ï¼Œ
æ‰€ä»¥target(label)é‡‡ç”¨äº†one-hotç·¨ç¢¼çš„å½¢å¼ï¼Œ
æ¯å€‹æ•¸æ“šæ¨£æœ¬çš„labelä¸­å¯èƒ½æœ‰ä¸€å€‹æˆ–å¤šå€‹1ï¼Œè¡¨ç¤ºå±¬äº1å€‹é¡åˆ¥æˆ–è€…å¤šå€‹é¡åˆ¥ã€‚
ç•¶ç„¶ï¼Œé›–ç„¶ä»¥ä¸Šæ²’æœ‰é¡¯ç¤ºå‡ºä¾†ï¼Œä¹Ÿå­˜åœ¨ä¸å±¬äºä»»ä½•é¡åˆ¥çš„æ¨£æœ¬ï¼Œå³å…¶labelç‚ºå…¨é›¶å‘é‡ã€‚
"""

"""
9. make_biclusters
make_biclustersç”¨äºç”Ÿæˆå…·æœ‰æ’å®šå¡Šå°è§’ç·šçµæ§‹(constant block diagonal structure)
çš„æ•¸çµ„ä»¥é€²è¡Œé›™å‘èšé¡ã€‚æ‰€è¬‚â€œé›™å‘èšé¡â€ï¼Œæ˜¯æŒ‡å°è®Šé‡å’Œå¯¦ä¾‹åŒæ™‚èšé¡ã€‚
æœ¬æ•¸æ“šé›†å¯ä»¥ç”¨äºè­œå”èšé¡(SpectralCoclustering)ç®—æ³•çš„ç¤ºä¾‹ã€‚
"""

data, rows, columns = make_biclusters(
    shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=9487
)

plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Original dataset")
show()

"""
10. make_checkerboard
make_checkerboard()ç”¨äºç”Ÿæˆä¸€å€‹å…·æœ‰æ£‹ç›¤æ ¼çµæ§‹çš„æ•¸çµ„ï¼Œä»¥é€²è¡Œé›™å‘èšé¡ã€‚
"""

data, rows, columns = make_checkerboard(
    shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=9487
)

plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Original dataset")
show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

# ç‘å£«æ²
"""
===================================
Swiss Roll reduction with LLE
===================================
An illustration of Swiss Roll reduction
with locally linear embedding
"""

# This import is needed to modify the way figure behaves
from mpl_toolkits.mplot3d import Axes3D

#----------------------------------------------------------------------
# Locally linear embedding of the swiss roll

from sklearn import manifold

X, color = datasets.make_swiss_roll(n_samples=1500)

print("Computing LLE embedding")
X_r, err = manifold.locally_linear_embedding(X, n_neighbors=12,
                                             n_components=2)
print("Done. Reconstruction error: %g" % err)

#----------------------------------------------------------------------
# Plot result

fig = plt.figure()

ax = fig.add_subplot(211, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)

ax.set_title("Original data")
ax = fig.add_subplot(212)
ax.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral)
plt.axis('tight')
plt.xticks([]), plt.yticks([])
plt.title('Projected data')
plt.show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

print("------------------------------------------------------------")  # 60å€‹
print("ä½œæ¥­å®Œæˆ")
print("------------------------------------------------------------")  # 60å€‹
sys.exit()


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

"""
sklearn ä½¿ç”¨make_regressionç”Ÿæˆå›æ­¸æ¨£æœ¬æ•¸æ“šåŠNumPyæ“¬åˆ

1. ä»‹ç´¹
sklearnçš„make_regressionå‡½æ•¸èƒ½ç”Ÿæˆå›æ­¸æ¨£æœ¬æ•¸æ“šã€‚

2. å‡½æ•¸èªæ³•
make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, 
                effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)

3. åƒæ•¸èªªæ˜ï¼š
n_samplesï¼šæ¨£æœ¬æ•¸
n_featuresï¼šç‰¹å¾æ•¸(è‡ªè®Šé‡å€‹æ•¸)
n_informativeï¼šåƒèˆ‡å»ºæ¨¡ç‰¹å¾æ•¸
n_targetsï¼šå› è®Šé‡å€‹æ•¸
noiseï¼šå™ªéŸ³
biasï¼šåå·®(æˆªè·)
coefï¼šæ˜¯å¦è¼¸å‡ºcoefæ¨™è­˜
random_stateï¼šéš¨æ©Ÿç‹€æ…‹è‹¥ç‚ºå›ºå®šå€¼å‰‡æ¯æ¬¡ç”¢ç”Ÿçš„æ•¸æ“šéƒ½ä¸€æ¨£
"""
print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹
