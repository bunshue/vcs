"""
åŸºæœ¬åŠŸèƒ½_å°ˆç”¨å‡½æ•¸1

ä½¿ç”¨ scikit-learn(sklearn) çš„ å‡½æ•¸

é è¨­æ–¹æ³•å»ºç«‹è³‡æ–™é›†

ä¸€äº›ç°¡æ˜“çš„é‹ç®—

"""

print("------------------------------------------------------------")  # 60å€‹

# å…±åŒ
import os
import sys
import time
import math
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  # æµ·ç”Ÿ, è‡ªå‹•æŠŠåœ–ç•«å¾—æ¯”è¼ƒå¥½çœ‹

font_filename = "C:/_git/vcs/_1.data/______test_files1/_font/msch.ttf"
# è¨­å®šä¸­æ–‡å­—å‹åŠè² è™Ÿæ­£ç¢ºé¡¯ç¤º
# è¨­å®šä¸­æ–‡å­—å‹æª”
plt.rcParams["font.sans-serif"] = "Microsoft JhengHei"  # å°‡å­—é«”æ›æˆ Microsoft JhengHei
# è¨­å®šè² è™Ÿ
plt.rcParams["axes.unicode_minus"] = False  # è®“è² è™Ÿå¯æ­£å¸¸é¡¯ç¤º
plt.rcParams["font.size"] = 12  # è¨­å®šå­—å‹å¤§å°

print("------------------------------------------------------------")  # 60å€‹

from sklearn.datasets import make_regression  # è¿´æ­¸è³‡æ–™é›†
from sklearn.datasets import make_blobs  # é›†ç¾¤è³‡æ–™é›†
from sklearn.datasets import make_classification  # åˆ†é¡è³‡æ–™é›†
from sklearn.datasets import make_moons  # éç·šæ€§çš„è³‡æ–™é›†
from sklearn.datasets import make_circles  # åœ“å½¢åˆ†ä½ˆçš„è³‡æ–™é›†

print("------------------------------------------------------------")  # 60å€‹

import ssl

ssl._create_default_https_context = ssl._create_stdlib_context

print("------------------------------------------------------------")  # 60å€‹

import sklearn
from sklearn import datasets, svm, metrics

print(sklearn.__version__)
print(dir(datasets))
print(sklearn)

print("------------------------------------------------------------")  # 60å€‹

plt.figure(
    num="sklearnå…§å»ºè³‡æ–™é›†é›†åˆ",
    figsize=(16, 9),
    dpi=100,
    facecolor="whitesmoke",
    edgecolor="r",
    linewidth=1,
    frameon=True,
)

print("------------------------------")  # 30å€‹
plt.subplot(231)
plt.title("make_regression è¿´æ­¸è³‡æ–™é›†")

X, y, coef = make_regression(
    n_samples=100, n_features=1, noise=20, coef=True, random_state=9487
)
print(X.shape)

print(coef)

plt.scatter(X[:, 0], y)
plt.plot([min(X), max(X)], [min(X) * coef, max(X) * coef], "r")

print("------------------------------")  # 30å€‹
plt.subplot(232)
plt.title("make_blobs é›†ç¾¤è³‡æ–™é›†")

X, y, centers = make_blobs(
    n_samples=100, centers=3, cluster_std=1, n_features=2, return_centers=True
)
print(X.shape)
print(centers)

# æ¨£æœ¬é»çš„å½¢ç‹€
markers = ["x", "o", "^"]

# é‡å°é¡åˆ¥å„ç•«ä¸€å€‹æ•£ä½ˆåœ–
for k in range(3):
    X_0 = []
    X_1 = []
    for i in range(len(y)):
        if y[i] == k:
            X_0.append(X[i, 0])
            X_1.append(X[i, 1])
            plt.scatter(X_0, X_1, marker=markers[k], s=50)

# ç¹ªè£½é›†ç¾¤ä¸­å¿ƒé»
X_0 = []
X_1 = []
for i in range(len(centers)):
    X_0.append(centers[i, 0])
    X_1.append(centers[i, 1])
plt.scatter(X_0, X_1, marker="s", s=200, alpha=0.5)

print("------------------------------")  # 30å€‹
plt.subplot(233)
plt.title("make_classification åˆ†é¡è³‡æ–™é›†")

print("åˆ†é¡è³‡æ–™é›†")

X, y = make_classification(
    n_samples=100,
    n_classes=3,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=9487,
)
print(X.shape)

# ç¹ªåœ–
# æ¨£æœ¬é»çš„å½¢ç‹€
markers = ["x", "o", "^"]

# é‡å°é¡åˆ¥å„ç•«ä¸€å€‹æ•£ä½ˆåœ–
for k in range(3):
    X_0 = []
    X_1 = []
    for i in range(len(y)):
        if y[i] == k:
            X_0.append(X[i, 0])
            X_1.append(X[i, 1])
            plt.scatter(X_0, X_1, marker=markers[k], s=50)

print("------------------------------")  # 30å€‹
plt.subplot(234)
plt.title("make_moons éç·šæ€§çš„è³‡æ–™é›†")

X, y = make_moons(n_samples=100, noise=0.05)
print(X.shape)

# é‡å°é¡åˆ¥å„ç•«ä¸€å€‹æ•£ä½ˆåœ–
colors = ["red", "blue"]
for k in range(2):
    X_0 = []
    X_1 = []
    for i in range(len(y)):
        if y[i] == k:
            X_0.append(X[i, 0])
            X_1.append(X[i, 1])
            plt.scatter(X_0, X_1, s=50, c=colors[k])


print("------------------------------")  # 30å€‹
plt.subplot(235)
plt.title("make_circles åœ“å½¢åˆ†ä½ˆçš„è³‡æ–™é›†")

X, y = make_circles(n_samples=100, noise=0.05)
print(X.shape)

# é‡å°é¡åˆ¥å„ç•«ä¸€å€‹æ•£ä½ˆåœ–
colors = ["red", "blue"]
for k in range(2):
    X_0 = []
    X_1 = []
    for i in range(len(y)):
        if y[i] == k:
            X_0.append(X[i, 0])
            X_1.append(X[i, 1])
            plt.scatter(X_0, X_1, s=50, c=colors[k])

print("------------------------------")  # 30å€‹
plt.subplot(236)
plt.title("xxx")


plt.show()


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


"""
sklearn ä½¿ç”¨make_regressionç”Ÿæˆå›å½’æ ·æœ¬æ•°æ®åŠNumPyæ‹Ÿåˆ

1. ä»‹ç»
sklearnçš„make_regressionå‡½æ•°èƒ½ç”Ÿæˆå›å½’æ ·æœ¬æ•°æ®ã€‚

2. å‡½æ•°è¯­æ³•
make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, 
                effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)

3. å‚æ•°è¯´æ˜ï¼š
n_samplesï¼šæ ·æœ¬æ•°
n_featuresï¼šç‰¹å¾æ•°(è‡ªå˜é‡ä¸ªæ•°)
n_informativeï¼šå‚ä¸å»ºæ¨¡ç‰¹å¾æ•°
n_targetsï¼šå› å˜é‡ä¸ªæ•°
noiseï¼šå™ªéŸ³
biasï¼šåå·®(æˆªè·)
coefï¼šæ˜¯å¦è¾“å‡ºcoefæ ‡è¯†
random_stateï¼šéšæœºçŠ¶æ€è‹¥ä¸ºå›ºå®šå€¼åˆ™æ¯æ¬¡äº§ç”Ÿçš„æ•°æ®éƒ½ä¸€æ ·

"""

X, Y = make_regression(
    n_samples=10, n_features=1, n_targets=1, noise=1.5, random_state=9487
)
cc = X.shape, Y.shape
print(cc)


plt.scatter(
    X,  # xåæ ‡
    Y,  # yåæ ‡
)
plt.show()


# 5. ç”¨NumPyå®ç°æ‹Ÿåˆ
# Numpyæ‹ŸåˆåŸºäºæœ€å°äºŒä¹˜æ³•

plt.scatter(
    X,  # xåæ ‡
    Y,  # yåæ ‡
)

# ç”¨ä¸€æ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œç›¸å½“äºçº¿æ€§æ‹Ÿåˆ
z1 = np.polyfit(X.reshape(10), Y, 1)
p1 = np.poly1d(z1)
print(z1)
print(p1)

y = z1[0] * X + z1[1]
plt.plot(X, y, c="red")

plt.show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

"""
N = 50
X, y = make_regression(n_samples=N, n_features=3)
print(X.shape, y.shape)
print(X)
print(y)

y = y.reshape((-1, 1))
#print(y)

from sklearn.linear_model import LinearRegression

linear_regression = LinearRegression()
linear_regression.fit(X, y)

y_pred_sk = linear_regression.predict(X)
#print(y_pred_sk)

plt.figure(figsize=(9, 4))

plt.plot(y, color="r", linewidth=10)
plt.plot(y_pred_sk, color="g", linewidth=4)

#plt.legend()

plt.show()

print('------------------------------')	#30å€‹


def gd(X, y, theta, l_rate, iterations):
    cost_history = [0] * iterations

    m = X.shape[0]

    for epoch in range(iterations):
        y_hat = X.dot(theta)

        loss = y_hat - y

        gradient = X.T.dot(loss) / m

        theta = theta - l_rate * gradient

        cost = np.dot(loss.T, loss)

        cost_history[epoch] = cost[0, 0]

    return theta, cost_history


def sgd(X, y, theta, l_rate, iterations):
    cost_history = [0] * iterations

    for epoch in range(iterations):
        for i, row in enumerate(X):
            yhat = np.dot(row, theta)

            loss = yhat[0] - y[i]

            theta = theta - l_rate * loss * row.reshape((-1, 1))

            cost_history[epoch] += loss**2

    return theta, cost_history


def predict(X, theta):
    return np.dot(X, theta)


theta = np.random.rand(X.shape[1], 1)

iterations = 100

l_rate = 0.1

theta, cost_history = gd(X, y, theta, l_rate, iterations)

print(theta.T)

# array([[ 1.12259549, 64.22439151, 84.34968956]])

y_predict = predict(X, theta)

y_predict = predict(X, theta)

plt.figure(figsize=(9, 4))

plt.plot(y, color="r")
plt.plot(y, alpha=0.3, linewidth=5)
plt.plot(y_predict, color="g")
plt.plot(y_predict, linewidth=2)

plt.show()

print(linear_regression.coef_)
# array([[48.54597102, 82.31351886,  8.52184984]])

"""

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹

print("åˆ†é¡æ•ˆæœè©•ä¼°")
print("FP/FN/TP/TN")

y_pred = [0, 0, 0, 1, 1, 1, 0, 1, 0, 0]  # é æ¸¬å€¼
y_real = [0, 1, 1, 1, 1, 1, 0, 0, 0, 0]  # å¯¦éš›å€¼

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_real, y_pred)
tn, fp, fn, tp = cm.ravel()
print("tn", tn, "fp", fp, "fn", fn, "tp", tp)

print("æº–ç¢ºç‡")
from sklearn.metrics import accuracy_score

print(accuracy_score(y_real, y_pred))

print("å¬å›ç‡")
from sklearn.metrics import recall_score

print(recall_score(y_real, y_pred))

print("ç²¾åº¦")
from sklearn.metrics import precision_score

print(precision_score(y_real, y_pred))

print("Få€¼")

from sklearn.metrics import f1_score
from sklearn.metrics import fbeta_score

print(f1_score(y_real, y_pred))  # è¨ˆç®—f1
print(fbeta_score(y_real, y_pred, beta=2))  # è¨ˆç®—fn

print("Logloss")
from sklearn.metrics import log_loss

y_real = [0, 1, 1, 1, 1, 1, 0, 0, 0, 0]
y_score = [0.9, 0.75, 0.86, 0.47, 0.55, 0.56, 0.74, 0.22, 0.5, 0.26]
print(log_loss(y_real, y_score))

print("ROCæ›²ç·šå’ŒAUC")
from sklearn.metrics import roc_auc_score, roc_curve

print(roc_auc_score(y_real, y_score))  # AUCå€¼

fpr, tpr, thresholds = roc_curve(y_real, y_score)
plt.plot(fpr, tpr)  # ç¹ªåœ–

# plt.show()

# P-Ræ›²ç·š
from sklearn.metrics import precision_recall_curve

precision, recall, _ = precision_recall_curve(y_real, y_score)
plt.plot(recall, precision)

# plt.show()


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

print("å¤šæŒ‡æ¨™è©•åˆ†")

from sklearn.metrics import classification_report

y_real = [0, 1, 1, 1, 1, 1, 0, 0, 0, 0]
y_score = [0.9, 0.75, 0.86, 0.47, 0.55, 0.56, 0.74, 0.22, 0.5, 0.26]
y_pred = [round(i) for i in y_score]
print(classification_report(y_real, y_pred))

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.svm import SVC

a = np.array([[10, 2.7, 3.6], [-100, 5, -2], [120, 20, 40]], dtype=np.float64)
print(a)
print(preprocessing.scale(a))

X, y = make_classification(
    n_samples=300,
    n_features=2,
    n_redundant=0,
    n_informative=2,
    random_state=22,
    n_clusters_per_class=1,
    scale=100,
)
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.show()

X = preprocessing.scale(X)  # normalization step
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
clf = SVC()
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

"""
æœºå™¨å­¦ä¹ ç¬”è®°ï¼šå¸¸ç”¨æ•°æ®é›†ä¹‹scikit-learnç”Ÿæˆåˆ†ç±»å’Œèšç±»æ•°æ®é›†

æœ¬æ–‡ä»‹ç»åˆ†ç±»å’Œèšç±»æ•°æ®é›†çš„ç”Ÿæˆï¼ŒåŒ…æ‹¬ä»¥ä¸‹9ä¸ªæ¥å£å‡½æ•°ï¼Œå…¶ä¸­ï¼Œ

æœ‰å…­ä¸ªæ˜¯ç”¨äºå•æ ‡ç­¾ç±»æ•°æ®ç”Ÿæˆï¼š

(1) make_blobs()
(2) make_classification()
(3) make_gaussian_quantiles()
(4) make_hastie_10_2()
(5) make_circles()
(6) make_moons()

ä¸€ä¸ªç”¨äºå¤šæ ‡ç­¾ç±»æ•°æ®ç”Ÿæˆ:
(7) make_multilabel_classification()

è¿˜æœ‰ä¸¤ä¸ªç”¨äºåŒèšç±»æ•°æ®é›†ç”Ÿæˆï¼š
(8) make_biclusters
(9) make_checkerboard
"""

"""
2. make_classificationÂ¶

make_blobs()å’Œmake_classification()éƒ½ç”¨äºç”Ÿæˆå¤šç±»åˆ«çš„æ•°æ®é›†ï¼Œ
æ¯ä¸ªç±»åˆ«éƒ½æ˜¯ç”±ä¸€ä¸ªæˆ–è€…å¤šä¸ªæ­£æ€åˆ†å¸ƒç°‡(normally-distributed cluster)æ„æˆã€‚

make_blobså¯¹äºå„ç°‡çš„ä¸­å¿ƒå’Œæ ‡å‡†åå·®æä¾›äº†æ›´æ–¹ä¾¿çš„æ§åˆ¶é€‰é¡¹ï¼Œ
é€šå¸¸ç”¨äºèšç±»ç®—æ³•çš„æ¼”ç¤ºã€‚è€Œmake_classificationåˆ™æ›´åŠ ä¾§é‡äºé€šè¿‡å„ç§æ‰‹æ®µå¯¼å…¥å„ç§â€œå™ªå£°â€çš„å½±å“ï¼Œ
æ¯”å¦‚è¯´ï¼Œç›¸å…³çš„ã€å†—ä½™çš„ã€æ²¡æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾ï¼›æ¯ä¸ªç±»åˆ†æˆå¤šä¸ªæ­£æ€åˆ†å¸ƒç°‡ï¼›ç‰¹å¾ç©ºé—´çš„çº¿æ€§å˜æ¢ç­‰ç­‰ã€‚
"""

# make_classification()ç”ŸæˆäºŒåˆ†ç±»æ•°æ®é›†

X, y = make_classification(
    n_samples=1000,
    n_features=5,
    n_redundant=0,
    n_clusters_per_class=1,
    n_informative=1,
    n_classes=2,
    random_state=20,
)

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue"}
fig, ax = plt.subplots()
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

print(X.shape, y.shape)
plt.show()

"""
3. make_blobs
'blob'çš„æ„æ€å¯èƒ½è·Ÿclusterå·®ä¸å¤šï¼Œéƒ½æ˜¯ç°‡ã€å›¢ã€å—çš„æ„æ€ã€‚
ä»¥ä¸‹ç¬¬ä¸€ä¸ªä¾‹å­ç”Ÿæˆäº†3ä¸ªblobsï¼Œç¬¬äºŒä¸ªä¾‹å­ç”Ÿæˆäº†4ä¸ªblobsã€‚
æ³¨æ„ï¼Œåœ¨ç¬¬3ä¸ªä¾‹å­ä¸­ï¼Œæ˜¾å¼åœ°åˆ¶æŒ‡å®šäº†4ä¸ªblobsçš„ä¸­å¿ƒå„ç°‡çš„æ ·æœ¬æ•°ï¼Œä»¥åŠå„ç°‡çš„standard deviation.
"""

# make_blobs: Generate isotropic Gaussian blobs for clustering. Of course, can also be used for classfication problem.

X, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=10)

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue", 2: "y"}
fig, ax = plt.subplots()
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])


plt.show()


X, y = make_blobs(
    n_samples=[100, 300, 250, 400],
    n_features=2,
    centers=[[100, 120], [250, 300], [700, 150], [300, 500]],
    cluster_std=50,
    random_state=111,
)

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue", 2: "y", 3: "green"}
fig, ax = plt.subplots()
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

plt.show()


""" 
4. make_moons
        make_moons()å‡½æ•°ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜æ•°æ®é›†ï¼Œ
        å®ƒç”Ÿæˆä¸¤ä¸ªåŠæœˆå½¢å¯¹åº”ä¸¤ä¸ªåˆ†ç±»ã€‚å¯ä»¥é€šè¿‡noiseå‚æ•°æ¥æ§åˆ¶å™ªå£°é‡ã€‚
        é€‚åˆäºéçº¿æ€§åˆ†ç±»ç®—æ³•çš„æ¼”ç¤ºã€‚
"""

# make_moons: Generate isotropic Gaussian blobs for clustering.
# ç»å¸¸ç”¨äºéçº¿æ€§åˆ†ç±»ç¤ºä¾‹ã€‚

# generate 2d classification dataset
X, y = make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=10)
# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue"}
fig, ax = plt.subplots()
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

plt.show()

"""
5. make_circles
é¡¾åæ€ä¹‰ï¼Œæ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ„æˆä¸€ä¸ªåœ†å½¢ã€‚
"""
# make_circles: generates a binary classification problem with datasets that fall into concentric circles.
# Make a large circle containing a smaller circle in 2d.
# A simple toy dataset to visualize clustering and classification algorithms, suitable for algorithms that can learn complex non-linear manifolds.


# generate 2d classification dataset
X, y = make_circles(
    n_samples=1000, noise=0.05
)  # 'noise' is used to control the amount of noise in the shapes.
# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))
colors = {0: "red", 1: "blue"}
fig, ax = plt.subplots(figsize=[6, 6])
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

plt.show()

"""
6. make_gaussian_quantiles
make_gaussian_quantiles()é¦–å…ˆç”Ÿæˆä¸€ä¸ªå¤šç»´æ­£æ€åˆ†å¸ƒæ ·æœ¬é›†ï¼Œç„¶åï¼Œå°†è¿™æ ·æœ¬é›†åŸºäºåˆ†ä½ç‚¹(quantiles)åˆ†å‰²æˆå¤šä¸ª(n_classes=3 by default)åµŒå¥—çš„å¤šç»´åŒå¿ƒè¶…çƒï¼Œæ¯ä¸ªè¶…çƒå±äºä¸€ç±»ï¼Œå¹¶ä½¿å¾—å¤§è‡´å„ç±»çš„æ ·æœ¬åŸºæœ¬ç›¸ç­‰ã€‚
åŸºäºåˆ†ä½ç‚¹è¿›è¡Œåˆ†å‰²æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ
ä»¥ä¸€ç»´æ­£æ€åˆ†å¸ƒä¸ºä¾‹ï¼Œå¤§è‡´æ¥è¯´å°±æ˜¯è¿™æ ·åˆ†å‰²çš„ã€‚å‡è®¾n_classes = 3ï¼Œå› æ­¤å¯¹åº”çš„ä¸¤ä¸ªåˆ†å‰²ç”¨çš„åˆ†ä½ç‚¹å°±æ˜¯33%å’Œ66%ã€‚å–æ ·æœ¬ä¸­ä½äº[0, 33%]åˆ†ä½åŒºé—´çš„ä½œä¸ºç¬¬ä¸€ç±»ï¼Œä½äº[33%, 66%]åˆ†ä½åŒºé—´çš„ä½œä¸ºç¬¬äºŒç±»ï¼Œä½äº[66%, 100%]åˆ†ä½åŒºé—´çš„ä½œä¸ºç¬¬ä¸‰ç±»ã€‚å¯¹äºå¤šç»´æ•°æ®ï¼Œæ˜¯åŸºäºå¯¹åº”çš„ğœ’2åˆ†å¸ƒçš„åˆ†ä½æ•°æ¥è¿›è¡Œåˆ†ç±»ã€‚
"""
from sklearn.datasets import make_gaussian_quantiles

data, target = make_gaussian_quantiles(n_samples=1500, cov=1.0, n_classes=3)

# scatter plot, dots colored by class value
df = pd.DataFrame(dict(x=data[:, 0], y=data[:, 1], label=target))
colors = {0: "red", 1: "blue", 2: "k"}
fig, ax = plt.subplots(figsize=[6, 6])
grouped = df.groupby("label")
for key, group in grouped:
    group.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])

plt.show()

"""
7. make_hastie_10_2
è¿™ä¸ªå‡½æ•°æ˜¯ä¸“é—¨ç”¨äºä»¥ä¸‹Hastieçš„æœºå™¨å­¦ä¹ ç»å…¸æ•™æä¸­ä¾‹10.2æ‰€æåŠçš„æ•°æ®é›†çš„ç”Ÿæˆï¼Œç”¨äºäºŒåˆ†ç±»é—®é¢˜ã€‚ä¸ºä¸€æœ¬ä¹¦ä¸­çš„ä¸€ä¸ªä¾‹å­ä¸“é—¨åˆ—äº†ä¸€ä¸ªå‡½æ•°ï¼Œç¡®å®æ˜¯å¾ˆæ‹¼ã€‚å¯ä»¥çœ‹ä½œæ˜¯make_gaussian_quantilesçš„ä¸€ç§ç‰¹ä¾‹ï¼Œæˆ–è€…åè¿‡æ¥è¯´make_gaussian_quantilesæ˜¯make_hastie_10_2çš„æ¨å¹¿ã€‚
T. Hastie, R. Tibshirani and J. Friedman, â€œElements of Statistical Learning Ed. 2â€, Springer, 2009.ã€‹
è¯¥æ•°æ®é›†æœ‰10ä¸ªç‰¹å¾ï¼Œæ˜¯i.i.dï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒï¼‰çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œtarget yå®šä¹‰å¦‚ä¸‹ï¼š
y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1
"""
from sklearn.datasets import make_hastie_10_2

data, target = make_hastie_10_2(n_samples=1000, random_state=42)

# target[target==-1] = 0  # åŸæ•°æ®é›†ç”Ÿæˆçš„targetä¸º[1,-1],è¿™é‡Œå˜æ¢ä¸º[1,0]
# target = target.astype('int32') # å˜æ¢æˆæ•´æ•°

df = pd.DataFrame(data)
df["target"] = target

print(df)

"""
è¿™æ˜¯ä¸€ä¸ª10ç»´çš„æ•°æ®ï¼Œæ‰€ä»¥ä¸å®¹æ˜“ä»¥æ•£ç‚¹å›¾çš„å½¢å¼è¿›è¡Œå›¾ç¤ºåŒ–ã€‚ä»¥ä¸‹é€šè¿‡å›¾ç¤ºçš„æ–¹å¼çœ‹çœ‹å„ä¸ªç»´åº¦æ˜¯ä¸æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.dï¼‰çš„æ ‡å‡†é«˜æ–¯åˆ†å¸ƒã€‚
"""
from scipy.stats import norm

plt.figure(figsize=(20, 6))
for k in range(10):
    df[k].plot(kind="kde", secondary_y=True, label="feature#" + str(k))

x = np.linspace(-8, 8, 1000)
plt.plot(x, norm.pdf(x), "r-", lw=2, alpha=0.6, label="theoretic std norm pdf")

plt.legend()
plt.show()

# å¦‚ä¸Šå›¾å¯çŸ¥ï¼Œ10ä¸ªç‰¹å¾åˆ†é‡ç¡®å®åŸºæœ¬ä¸Šéƒ½æ˜¯ä¸æ ‡å‡†æ­£æ€åˆ†å¸ƒå»åˆçš„ã€‚


"""
8. å¤šæ ‡ç­¾æ•°æ®é›†ç”Ÿæˆ
å¤šæ ‡ç­¾æ•°æ®é›†ç”¨äºå½“å­˜åœ¨å¤šå„ç±»åˆ«ï¼Œè€Œå¾…åˆ†ç±»çš„æ•°æ®å¯èƒ½å±äºå…¶ä¸­çš„ä¸€ç±»æˆ–è€…åŒæ—¶å±äºå¤šä¸ªç±»åˆ«ï¼Œæˆ–è€…ç”šè‡³ä¸å±äºä»»ä½•ç±»åˆ«ã€‚æ¯”å¦‚è¯´ï¼Œå½“éœ€è¦è¯†åˆ«åœ¨ä¸€å¼ å›¾åƒä¸­æ‰€åŒ…å«çš„äº¤é€šä¿¡å·ç­‰çš„ç±»å‹ã€‚ä¸€å¼ å›¾ç‰‡å¯èƒ½ä¸åŒ…å«ä¿¡å·ç¯ï¼Œä¹Ÿå¯èƒ½åªåŒ…å«ä¸€ä¸ªçº¢ç¯æˆ–ç»¿ç¯æˆ–é»„ç¯ï¼Œä¹Ÿå¯èƒ½åŒæ—¶åŒ…å«ä¸€ä¸ªçº¢ç¯å’Œç»¿ç¯ï¼ˆå¦‚æœè¿™å¼ å›¾ç‰‡è¦†ç›–äº†ä¸€ä¸ªåå­—è·¯å£çš„ä¸¤ä¸ªæ–¹å‘çš„ä¿¡å·ç¯çš„è¯ï¼‰ã€‚
"""

from sklearn.datasets import make_multilabel_classification as make_ml_clf

x, y = make_ml_clf(n_samples=1000, n_features=10, n_classes=3, random_state=0)
print(x.shape, y.shape)
print(y[:10, :])

"""
å¯ä»¥çœ‹å‡ºï¼Œç”±äºæ˜¯å¤šåˆ†ç±»ï¼ˆæœ¬ä¾‹æ˜¯3åˆ†ç±»ï¼‰å¤šæ ‡ç­¾çš„ï¼Œæ‰€ä»¥target(label)é‡‡ç”¨äº†one-hotç¼–ç çš„å½¢å¼ï¼Œæ¯ä¸ªæ•°æ®æ ·æœ¬çš„labelä¸­å¯èƒ½æœ‰ä¸€ä¸ªæˆ–å¤šä¸ª1ï¼Œè¡¨ç¤ºå±äº1ä¸ªç±»åˆ«æˆ–è€…å¤šä¸ªç±»åˆ«ã€‚å½“ç„¶ï¼Œè™½ç„¶ä»¥ä¸Šæ²¡æœ‰æ˜¾ç¤ºå‡ºæ¥ï¼Œä¹Ÿå­˜åœ¨ä¸å±äºä»»ä½•ç±»åˆ«çš„æ ·æœ¬ï¼Œå³å…¶labelä¸ºå…¨é›¶å‘é‡ã€‚
"""

"""
9. make_biclusters
make_biclustersç”¨äºç”Ÿæˆå…·æœ‰æ’å®šå—å¯¹è§’çº¿ç»“æ„(constant block diagonal structure)çš„æ•°ç»„ä»¥è¿›è¡ŒåŒå‘èšç±»ã€‚æ‰€è°“â€œåŒå‘èšç±»â€ï¼Œæ˜¯æŒ‡å¯¹å˜é‡å’Œå®ä¾‹åŒæ—¶èšç±»ã€‚æœ¬æ•°æ®é›†å¯ä»¥ç”¨äºè°±åèšç±»(SpectralCoclustering)ç®—æ³•çš„ç¤ºä¾‹ã€‚
"""

from sklearn.datasets import make_biclusters

data, rows, columns = make_biclusters(
    shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=0
)

plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Original dataset")
plt.show()

"""
10. make_checkerboard
make_checkerboard()ç”¨äºç”Ÿæˆä¸€ä¸ªå…·æœ‰æ£‹ç›˜æ ¼ç»“æ„çš„æ•°ç»„ï¼Œä»¥è¿›è¡ŒåŒå‘èšç±»ã€‚
"""
from sklearn.datasets import make_checkerboard

data, rows, columns = make_checkerboard(
    shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=0
)

plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Original dataset")
plt.show()

print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹

print("------------------------------------------------------------")  # 60å€‹
print("ä½œæ¥­å®Œæˆ")
print("------------------------------------------------------------")  # 60å€‹

print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹
print("------------------------------------------------------------")  # 60å€‹


print("------------------------------------------------------------")  # 60å€‹

X, y = make_regression(n_samples=100, n_features=1, n_targets=1, noise=10)
plt.scatter(X, y)
plt.show()
